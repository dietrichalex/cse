{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "FROM_SCRATCH = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "9.1.1 Preparing the data.\n",
    "We’re using the wine-quality dataset, a numeric tabular dataset containing features that refer to the chemical composition of wines and quality ratings. To make this a simple classification task, we bucket all wines with ratings greater\n",
    "than five as good, and the rest we label bad. We also normalize all the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "store = pd.read_csv('input/store.csv')\n",
    "train = pd.read_csv('input/train.csv',parse_dates=[2])\n",
    "test = pd.read_csv('input/test.csv',parse_dates=[3])\n",
    "# fillna in store with 0 has better result than median()\n",
    "# Aufbereiten der daten\n",
    "store.fillna(0, inplace=True)\n",
    "# fill missing values in test with 1\n",
    "# Aufbereiten der Daten\n",
    "test.fillna(value = 1, inplace = True)\n",
    "# merge data with store\n",
    "# Alles in eine Tabelle\n",
    "train = pd.merge(train, store, on='Store')\n",
    "test = pd.merge(test, store, on='Store')\n",
    "# only use data of Sales>0 and Open is 1\n",
    "train = train[(train.Open != 0)&(train.Sales >0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# draw store 1 and store 10 sales distribution plot\n",
    "import matplotlib.pyplot as plt\n",
    "store_1 = train.loc[(train[\"Store\"]==1)&(train['Sales']>0), ['Date',\"Sales\"]]\n",
    "store_10 = train.loc[(train[\"Store\"]==10)&(train['Sales']>0), ['Date',\"Sales\"]]\n",
    "f = plt.figure(figsize=(18,10))\n",
    "ax1 = f.add_subplot(211)\n",
    "ax1.plot(store_1['Date'], store_1['Sales'], '-')\n",
    "ax1.set_xlabel('Time')\n",
    "ax1.set_ylabel('Sales')\n",
    "ax1.set_title('Store 1 Sales Distribution')\n",
    "\n",
    "ax2 = f.add_subplot(212)\n",
    "ax2.plot(store_10['Date'], store_10['Sales'], '-')\n",
    "ax2.set_xlabel('Time')\n",
    "ax2.set_ylabel('Sales')\n",
    "ax2.set_title('Store 10 Sales Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# check stores open distribution on days of week\n",
    "import seaborn as sns\n",
    "sns.countplot(x = 'DayOfWeek', hue = 'Open', data = test)\n",
    "plt.title('Store Daily Open Countplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Dichteverteilung der Verkaufszahlen. Einmal normal und einmal logarithmisch und normalisiert\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# check distribution of sales in train set\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "g1 = sns.distplot(train['Sales'],hist = True,label='skewness:{:.2f}'.format(train['Sales'].skew()),ax = ax1)\n",
    "g1.legend()\n",
    "g1.set(xlabel = 'Sales', ylabel = 'Density', title = 'Sales Distribution')\n",
    "g2 = sns.distplot(np.log1p(train['Sales']),hist = True,label='skewness:{:.2f}'.format(np.log1p(train['Sales']).skew()),ax=ax2)\n",
    "g2.legend()\n",
    "g2.set(xlabel = 'log(Sales+1)',ylabel = 'Density', title = 'log(Sales+1) Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nQ = 5\n",
    "train['Class'] = 0\n",
    "classes = np.arange(0,nQ+1)\n",
    "\n",
    "#  Quantile für alle Tage\n",
    "quantile = np.arange(1, nQ) / nQ\n",
    "store_quan = np.arange(0, nQ+2)\n",
    "store_quan[0] = -1\n",
    "store_quan[1] = 1\n",
    "for index, x in enumerate(quantile):\n",
    "    store_quan[index+2] = train['Sales'].quantile(x)\n",
    "store_quan[nQ+1] = train['Sales'].max()\n",
    "\n",
    "\"\"\"\n",
    "#  Quantil für verkaufsoffene Sonntage\n",
    "store_df_day = train.loc[(train['DayOfWeek']== 7) & (train['Open'] == 1)]\n",
    "quantile = np.arange(1, nQ) / nQ\n",
    "store_quan_Sun = np.arange(0, nQ+2)\n",
    "store_quan_Sun[0] = -1\n",
    "store_quan_Sun[1] = 1\n",
    "for index, x in enumerate(quantile):\n",
    "    store_quan_Sun[index+2] = store_df_day['Sales'].quantile(x)\n",
    "store_quan_Sun[nQ+1] = store_df_day['Sales'].max()\n",
    "\n",
    "# Spezifische Quantile für jeden Tag und jeden Store\n",
    "store_quan = np.zeros((train['Store'].max(), 7, nQ+2))\n",
    "for Id in train['Store'].unique():\n",
    "    store_df = train[train['Store'] == Id]\n",
    "    for DayOfWeek in  store_df['DayOfWeek'].unique():\n",
    "        if DayOfWeek != 7:\n",
    "            store_df_day = store_df.loc[(store_df['DayOfWeek']== DayOfWeek) & (store_df['Open'] == 1)]\n",
    "            quantile = np.arange(1, nQ) / nQ\n",
    "            store_quan[Id-1][DayOfWeek-1][0] = -1\n",
    "            store_quan[Id-1][DayOfWeek-1][1] = 1\n",
    "            for index, x in enumerate(quantile):\n",
    "                store_quan[Id-1][DayOfWeek-1][index+2] = store_df_day['Sales'].quantile(x)\n",
    "            store_quan[Id-1][DayOfWeek-1][nQ+1] = store_df_day['Sales'].max()\n",
    "        else:\n",
    "            store_quan[Id-1][DayOfWeek-1] = store_quan_Sun\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(store_quan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# process train and test\n",
    "# aufbereiten der daten, neue Spalten und manche werden entfernt\n",
    "def process(data, isTest = False):\n",
    "    # label encode some features\n",
    "    mappings = {'0':0, 'a':1, 'b':2, 'c':3, 'd':4}\n",
    "    # buchstaben zu zahlen\n",
    "    data.StoreType.replace(mappings, inplace=True)\n",
    "    data.Assortment.replace(mappings, inplace=True)\n",
    "    data.StateHoliday.replace(mappings, inplace=True)\n",
    "\n",
    "    # extract some features from date column\n",
    "    data['Month'] = data.Date.dt.month\n",
    "    data['Year'] = data.Date.dt.year\n",
    "    data['Day'] = data.Date.dt.day\n",
    "    data['WeekOfYear'] = data.Date.dt.weekofyear\n",
    "\n",
    "    # calculate competiter open time in months\n",
    "    data['CompetitionOpen'] = 12 * (data.Year - data.CompetitionOpenSinceYear) + \\\n",
    "        (data.Month - data.CompetitionOpenSinceMonth)\n",
    "    data['CompetitionOpen'] = data['CompetitionOpen'].apply(lambda x: x if x > 0 else 0)\n",
    "\n",
    "    # calculate promo2 open time in months\n",
    "    data['PromoOpen'] = 12 * (data.Year - data.Promo2SinceYear) + \\\n",
    "        (data.WeekOfYear - data.Promo2SinceWeek) / 4.0\n",
    "    data['PromoOpen'] = data['PromoOpen'].apply(lambda x: x if x > 0 else 0)\n",
    "\n",
    "    # Indicate whether the month is in promo interval\n",
    "    month2str = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', \\\n",
    "             7:'Jul', 8:'Aug', 9:'Sept', 10:'Oct', 11:'Nov', 12:'Dec'}\n",
    "    data['month_str'] = data.Month.map(month2str)\n",
    "\n",
    "    def check(row):\n",
    "        if isinstance(row['PromoInterval'],str) and row['month_str'] in row['PromoInterval']:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    data['IsPromoMonth'] =  data.apply(lambda row: check(row),axis=1)\n",
    "\n",
    "    # select the features we need\n",
    "    features = ['Store', 'DayOfWeek', 'Promo', 'StateHoliday', 'SchoolHoliday',\n",
    "       'StoreType', 'Assortment', 'CompetitionDistance',\n",
    "       'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2',\n",
    "       'Promo2SinceWeek', 'Promo2SinceYear', 'Year', 'Month', 'Day',\n",
    "       'WeekOfYear', 'CompetitionOpen', 'PromoOpen', 'IsPromoMonth']\n",
    "    if not isTest:\n",
    "        features.append('Sales')\n",
    "\n",
    "    data = data[features]\n",
    "    return data\n",
    "\n",
    "train = train.sort_values(['Date'],ascending = False)\n",
    "train = process(train)\n",
    "test = process(test,isTest = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "featuresR  = ['Store', 'DayOfWeek', 'Promo', 'StateHoliday', 'SchoolHoliday',\n",
    "       'StoreType', 'Assortment', 'CompetitionDistance',\n",
    "       'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2',\n",
    "       'Promo2SinceWeek', 'Promo2SinceYear', 'Year', 'Month', 'Day',\n",
    "       'WeekOfYear', 'CompetitionOpen', 'PromoOpen', 'IsPromoMonth']\n",
    "train_data = train[featuresR].to_numpy()\n",
    "test_data = test[featuresR].to_numpy()\n",
    "labels_train = train[['Sales']].to_numpy()\n",
    "labels_train = np.log1p(labels_train)\n",
    "\n",
    "X_trainR, X_testR, y_trainR, y_testR = train_test_split(train_data, labels_train, random_state=0)\n",
    "X_trainR, X_testR = X_trainR.astype('float32'), X_testR.astype('float32')\n",
    "y_train_labR, y_test_labR = y_trainR[:, 0], y_testR[:, 0]\n",
    "y_trainR, y_testR = y_trainR[:, 1:].astype('float32'), y_testR[:, 1:].astype('float32')\n",
    "scalerR = StandardScaler()\n",
    "scalerR.fit(X_trainR)\n",
    "category_map = {1: [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"], 2:[\"PromoNo\", \"PromoYes\"], 3: [\"NoStateHoliday\", \"PublicHoliday\", \"EasterHoliday\", \"ChristmasHoliday\"],\n",
    "                4:[\"SchoolHolidayNo\", \"SchoolHolidayYes\"], 5: [\"StoreTypeA\", \"StoreTypeB\", \"StoreTypeC\", \"StoreTypeD\", \"StoreTypeE\"], 6:[\"Basic\", \"Extra\", \"Extended\"], 8:[\"None\",\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"], 10: [\"NoPromo2\", \"Promo2\"], 14: [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"], 19: [\"NoPromoMonth\", \"PromoMonth\"] }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Select good wine instance\n",
    "We partition the dataset into good and bad portions and select an instance of interest. I’ve chosen it to be a good quality\n",
    "wine.\n",
    "Note that bad wines are class 1 and correspond to the second model output being high, whereas good wines are class\n",
    "0 and correspond to the first model output being high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# bad_days = np.array([a for a, b in zip(X_trainR, y_trainR) if b[1] == 1])\n",
    "# good_days = np.array([a for a, b in zip(X_trainR, y_trainR) if b[1] == 0])\n",
    "xR = np.array([[747,5,1,0,1,3,3,45740.0,8.0,2008.0,1,0.0,0.0,2015,7,31,31,83.0,24187.75,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "9.1.2 Training models\n",
    "Creating an Autoencoder\n",
    "For some of the explainers, we need an autoencoder to check whether example instances are close to the training data\n",
    "distribution or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Random Forest Model\n",
    "We need a tree-based model to get results for the tree SHAP explainer. Hence we train a random forest on the winequality dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define eval metrics\n",
    "# Mittleres Abweichungsquadrat\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def rmspe(y, yhat):\n",
    "    return np.sqrt(np.mean((yhat/y-1) ** 2))\n",
    "# expm1 ist umkehr von log1p\n",
    "def rmspe_xg(yhat, y):\n",
    "    y = np.expm1(y.get_label())\n",
    "    yhat = np.expm1(yhat)\n",
    "    return \"rmspe\", rmspe(y,yhat)\n",
    "def rmse(ytest, y):\n",
    "    return np.sqrt(mean_squared_error(ytest, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_train_xgb = y_train_labR\n",
    "y_test_xgb = y_test_labR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# hier zwei Zeilen auskommentiert, da es sonst bei mir immer einen Fehler raushaut\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def make_xgb_modelR():\n",
    "    params = {\"objective\": \"reg:linear\", # for linear regression\n",
    "              \"booster\" : \"gbtree\",   # use tree based models\n",
    "              \"eta\": 0.03,   # learning rate\n",
    "              \"max_depth\": 10,    # maximum depth of a tree\n",
    "              \"subsample\": 0.9,    # Subsample ratio of the training instances\n",
    "              \"colsample_bytree\": 0.7,   # Subsample ratio of columns when constructing each tree\n",
    "              \"silent\": 1,   # silent mode\n",
    "              \"seed\": 10,   # Random number seed\n",
    "              #\"gpu_id\": 0,\n",
    "              #\"tree_method\": \"gpu_hist\",\n",
    "              # \"eval_metric\": \"rmse\"\n",
    "              }\n",
    "    # anzahl trainingsrunden\n",
    "    num_boost_round = 5\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_trainR, y_train_xgb)\n",
    "    dtest = xgb.DMatrix(X_testR, y_test_xgb)\n",
    "    watchlist = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "    # train the xgboost model\n",
    "    model = xgb.train(params, dtrain, num_boost_round, evals=watchlist, \\\n",
    "      early_stopping_rounds= 1000, feval=rmspe_xg, verbose_eval=True)\n",
    "    y_predR = model.predict(xgb.DMatrix(X_testR))\n",
    "    print(y_predR)\n",
    "    print('accuracy_score:', rmse(y_predR, y_test_xgb))\n",
    "    # print('f1_score:', f1_score(y_predR, y_test_xgb))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Tensorflow Model\n",
    "Finally, we also train a TensorFlow model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Modell wird schon passend gespeichert @Bene :)\n",
    "import os.path\n",
    "\n",
    "if FROM_SCRATCH or not os.path.isfile('modelXGBR.json'):\n",
    "    modelXGBR = make_xgb_modelR()\n",
    "    modelXGBR.save_model('modelXGBR.json')\n",
    "else:\n",
    "    modelXGBR = xgb.Booster()\n",
    "    modelXGBR.load_model('modelXGBR.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#y_pred = np.rint(modelXGBR.predict(xgb.DMatrix(X_testR)))\n",
    "y_pred = modelXGBR.predict(xgb.DMatrix(X_testR))\n",
    "print(np.expm1(y_test_xgb))\n",
    "print(y_pred)\n",
    "error = rmse(np.expm1(y_test_xgb), np.expm1(y_pred))\n",
    "print('RMSE: {:.4f}'.format(error))\n",
    "error = rmspe(np.expm1(y_test_xgb), np.expm1(y_pred))\n",
    "print('RMSPE: {:.4f}'.format(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load/Make models\n",
    "We save and load the same models each time to ensure stable results. If they don’t exist we create new ones. If you\n",
    "want to generate new models on each notebook run, then set FROM_SCRATCH=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import shap\n",
    "\n",
    "dtrain = xgb.DMatrix(X_trainR, y_train_xgb)\n",
    "modelXGBR.set_param({\"predictor\": \"gpu_predictor\"})\n",
    "shap_values = modelXGBR.predict(dtrain, pred_contribs=True)\n",
    "shap_interaction_values = modelXGBR.predict(dtrain, pred_interactions=True)\n",
    "\n",
    "modelXGBR.set_param({\"predictor\": \"gpu_predictor\"})\n",
    "explainer = shap.TreeExplainer(modelXGBR)\n",
    "shap_values = explainer.shap_values(X_trainR)\n",
    "shap.summary_plot(shap_values, X_trainR,max_display= 10, title = 'SHAP', plot_type= 'bar')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "9.1.3 Util functions\n",
    "These are utility functions for exploring results. The first shows two instances of the data side by side and compares\n",
    "the difference. We’ll use this to see how the counterfactuals differ from their original instances. The second function\n",
    "plots the importance of each feature. This will be useful for visualizing the attribution methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def compare_instances(x, cf):\n",
    "\n",
    "    #Show the difference in values between two instances.\n",
    "\n",
    "    x = x.astype('float64')\n",
    "    cf = cf.astype('float64')\n",
    "    for f, v1, v2 in zip(features, x[0], cf[0]):\n",
    "        print(f'{f:<25} instance: {round(v1, 3):^10} counter factual: {round(v2, 3):^10} difference: {round(v2, 7):^5}')\n",
    "\n",
    "def plot_importance(feat_imp, feat_names, class_idx, **kwargs):\n",
    "\n",
    "    #Create a horizontal barchart of feature effects, sorted by their magnitude.\n",
    "\n",
    "    df = pd.DataFrame(data=feat_imp, columns=feat_names).sort_values(by=0, axis='columns')\n",
    "    feat_imp, feat_names = df.values[0], df.columns\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    y_pos = np.arange(len(feat_imp))\n",
    "    ax.barh(y_pos, feat_imp)\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(feat_names, fontsize=15)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xlabel(f'Feature effects for class {class_idx}', fontsize=15)\n",
    "    return ax, fig\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "9.1.5 Local Necessary Features\n",
    "Anchors\n",
    "Anchors tell us what features need to stay the same for a specific instance for the model to give the same classification.\n",
    "In the case of a trained image classification model, an anchor for a given instance would be a minimal subset of the\n",
    "image that the model uses to make its decision.\n",
    "Here we apply Anchors to the tensor flow model trained on the wine-quality dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "from alibi.explainers import AnchorTabular\n",
    "predict_fnR = lambda x: modelR.predict(scalerR.transform(x))\n",
    "explainerR = AnchorTabular(predict_fnR, featuresR, categorical_names=category_map)\n",
    "explainerR.fit(X_trainR, disc_perc=(25, 50, 75))\n",
    "resultR = explainerR.explain(xR, threshold=0.95)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "category_map = { 1: [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"], 2:[\"PromoNo\", \"PromoYes\"], 3: [\"NoStateHoliday\", \"PublicHoliday\", \"EasterHoliday\", \"ChristmasHoliday\"],\n",
    "                4:[\"SchoolHolidayNo\", \"SchoolHolidayYes\"], 5: [\"?\", \"StoreTypeA\", \"StoreTypeB\", \"StoreTypeC\", \"StoreTypeD\"], 6:[ \"?\", \"Basic\", \"Extra\", \"Extended\"], 8:[\"None\",\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"], 10: [\"NoPromo2\", \"Promo2\"], 14: [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"], 19: [\"NoPromoMonth\", \"PromoMonth\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_trainR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def bin(input_array, data):\n",
    "    output_array = np.zeros(input_array.shape)\n",
    "    for i in range(len(input_array)):\n",
    "        Id = data[i][0]\n",
    "        Day = data[i][1]\n",
    "        print(Day)\n",
    "        print(Id)\n",
    "        print(input_array[i])\n",
    "        print(store_quan[np.rint(Id)][np.rint(Day)])\n",
    "        output_array[i] = np.searchsorted(store_quan[np.rint(Id)][np.rint(Day)], input_array[i])\n",
    "    return output_array\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Array mit 21 boolean-Werten, die angibt, ob eine Spalte behalten werden soll oder nicht\n",
    "column_filter = np.array([False, False, True, True, True, True, True, True, False, False, True, False, False, False, False, False, False, True, True, True])\n",
    "\n",
    "# Die Spalten auswählen, die behalten werden sollen\n",
    "X_trainR = X_trainR[:, column_filter]\n",
    "\n",
    "featuresR = [ 'Promo', 'StateHoliday', 'SchoolHoliday',\n",
    "       'StoreType', 'Assortment', 'CompetitionDistance', 'Promo2',\n",
    "       'CompetitionOpen', 'PromoOpen', 'IsPromoMonth']\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def bin(input_array):\n",
    "    output_array = np.zeros(input_array.shape)\n",
    "    for i in range(len(input_array)):\n",
    "        output_array[i] = np.searchsorted(store_quan, input_array[i])\n",
    "    return output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from alibi.explainers import AnchorTabular\n",
    "predict_xgb = lambda x: bin(np.expm1(modelXGBR.predict(xgb.DMatrix(x))))\n",
    "explainerXGB = AnchorTabular(predict_xgb, featuresR, categorical_names=category_map)\n",
    "explainerXGB.fit(X_trainR, disc_perc=(25, 50, 75))\n",
    "resultXGB = explainerXGB.explain(xR, threshold=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Anchor =', resultXGB.data['anchor'])\n",
    "print('Precision = ', resultXGB.data['precision'])\n",
    "print('Coverage = ', resultXGB.data['coverage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "idx = 11\n",
    "print(explainerXGB.predictor(X_testR[idx].reshape(1, -1))[0])\n",
    "explanation = explainerXGB.explain(X_testR[idx], threshold=0.8)\n",
    "print('Anchor: %s' % (' AND '.join(explanation.anchor)))\n",
    "print('Precision: %.2f' % explanation.precision)\n",
    "print('Coverage: %.2f' % explanation.coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def filter_array(array):\n",
    "  filtered = []\n",
    "  substrings = ['Promo', 'StateHoliday', 'SchoolHoliday', 'StoreType', 'Assortment', 'CompetitionDistance', 'Promo2', 'CompetitionOpen', 'PromoOpen', 'IsPromoMonth']\n",
    "  for element in array:\n",
    "    for substring in substrings:\n",
    "      if substring in element:\n",
    "        filtered.append(element)\n",
    "        break\n",
    "  return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new = filter_array(explanation.anchor)\n",
    "print(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
