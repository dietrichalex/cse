{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "FROM_SCRATCH = True"
   ],
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 86,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "9.1.1 Preparing the data.\n",
    "We’re using the wine-quality dataset, a numeric tabular dataset containing features that refer to the chemical composition of wines and quality ratings. To make this a simple classification task, we bucket all wines with ratings greater\n",
    "than five as good, and the rest we label bad. We also normalize all the features."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    }
   ],
   "source": [
    "store = pd.read_csv('input/store.csv')\n",
    "train = pd.read_csv('input/train.csv',parse_dates=[2])\n",
    "test = pd.read_csv('input/test.csv',parse_dates=[3])\n",
    "# fillna in store with 0 has better result than median()\n",
    "# Aufbereiten der daten\n",
    "store.fillna(0, inplace=True)\n",
    "# fill missing values in test with 1\n",
    "# Aufbereiten der Daten\n",
    "test.fillna(value = 1, inplace = True)\n",
    "# merge data with store\n",
    "# Alles in eine Tabelle\n",
    "train = pd.merge(train, store, on='Store')\n",
    "test = pd.merge(test, store, on='Store')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "data": {
      "text/plain": "1115"
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "nQ = 5\n",
    "train['Class'] = 0\n",
    "classes = np.arange(0,nQ+1)\n",
    "\n",
    "#  Quantil für verkaufsoffene Sonntage\n",
    "store_df_day = train.loc[(train['DayOfWeek']== 7) & (train['Open'] == 1)]\n",
    "quantile = np.arange(1, nQ) / nQ\n",
    "store_quan_Sun = np.arange(0, nQ+2)\n",
    "store_quan_Sun[0] = -1\n",
    "store_quan_Sun[1] = 1\n",
    "for index, x in enumerate(quantile):\n",
    "    store_quan_Sun[index+2] = store_df_day['Sales'].quantile(x)\n",
    "store_quan_Sun[nQ+1] = store_df_day['Sales'].max()\n",
    "\n",
    "store_quan = np.zeros((train['Store'].max(), 7, nQ+2))\n",
    "\n",
    "for Id in train['Store'].unique():\n",
    "    store_df = train[train['Store'] == Id]\n",
    "    for DayOfWeek in  store_df['DayOfWeek'].unique():\n",
    "        if DayOfWeek != 7:\n",
    "            store_df_day = store_df.loc[(store_df['DayOfWeek']== DayOfWeek) & (store_df['Open'] == 1)]\n",
    "            quantile = np.arange(1, nQ) / nQ\n",
    "            store_quan[Id-1][DayOfWeek-1][0] = -1\n",
    "            store_quan[Id-1][DayOfWeek-1][1] = 1\n",
    "            for index, x in enumerate(quantile):\n",
    "                store_quan[Id-1][DayOfWeek-1][index+2] = store_df_day['Sales'].quantile(x)\n",
    "            store_quan[Id-1][DayOfWeek-1][nQ+1] = store_df_day['Sales'].max()\n",
    "        else:\n",
    "            store_quan[Id-1][DayOfWeek-1] = store_quan_Sun"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0000e+00  1.0000e+00  2.7340e+03  5.5190e+03  8.0940e+03  1.2561e+04\n",
      "  3.7376e+04]\n"
     ]
    }
   ],
   "source": [
    "print(store_quan[0][6])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christian Karg\\AppData\\Local\\Temp\\ipykernel_21472\\2713858278.py:15: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated. Please use Series.dt.isocalendar().week instead.\n",
      "  data['WeekOfYear'] = data.Date.dt.weekofyear\n",
      "C:\\Users\\Christian Karg\\AppData\\Local\\Temp\\ipykernel_21472\\2713858278.py:15: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated. Please use Series.dt.isocalendar().week instead.\n",
      "  data['WeekOfYear'] = data.Date.dt.weekofyear\n"
     ]
    }
   ],
   "source": [
    "# process train and test\n",
    "# aufbereiten der daten, neue Spalten und manche werden entfernt\n",
    "def process(data, isTest = False):\n",
    "    # label encode some features\n",
    "    mappings = {'0':0, 'a':1, 'b':2, 'c':3, 'd':4}\n",
    "    # buchstaben zu zahlen\n",
    "    data.StoreType.replace(mappings, inplace=True)\n",
    "    data.Assortment.replace(mappings, inplace=True)\n",
    "    data.StateHoliday.replace(mappings, inplace=True)\n",
    "\n",
    "    # extract some features from date column\n",
    "    data['Month'] = data.Date.dt.month\n",
    "    data['Year'] = data.Date.dt.year\n",
    "    data['Day'] = data.Date.dt.day\n",
    "    data['WeekOfYear'] = data.Date.dt.weekofyear\n",
    "\n",
    "    # calculate competiter open time in months\n",
    "    data['CompetitionOpen'] = 12 * (data.Year - data.CompetitionOpenSinceYear) + \\\n",
    "        (data.Month - data.CompetitionOpenSinceMonth)\n",
    "    data['CompetitionOpen'] = data['CompetitionOpen'].apply(lambda x: x if x > 0 else 0)\n",
    "\n",
    "    # calculate promo2 open time in months\n",
    "    data['PromoOpen'] = 12 * (data.Year - data.Promo2SinceYear) + \\\n",
    "        (data.WeekOfYear - data.Promo2SinceWeek) / 4.0\n",
    "    data['PromoOpen'] = data['PromoOpen'].apply(lambda x: x if x > 0 else 0)\n",
    "\n",
    "    # Indicate whether the month is in promo interval\n",
    "    month2str = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', \\\n",
    "             7:'Jul', 8:'Aug', 9:'Sept', 10:'Oct', 11:'Nov', 12:'Dec'}\n",
    "    data['month_str'] = data.Month.map(month2str)\n",
    "\n",
    "    def check(row):\n",
    "        if isinstance(row['PromoInterval'],str) and row['month_str'] in row['PromoInterval']:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    data['IsPromoMonth'] =  data.apply(lambda row: check(row),axis=1)\n",
    "\n",
    "    # select the features we need\n",
    "    features = ['Store', 'DayOfWeek', 'Promo', 'StateHoliday', 'SchoolHoliday',\n",
    "       'StoreType', 'Assortment', 'CompetitionDistance',\n",
    "       'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2',\n",
    "       'Promo2SinceWeek', 'Promo2SinceYear', 'Year', 'Month', 'Day',\n",
    "       'WeekOfYear', 'CompetitionOpen', 'PromoOpen', 'IsPromoMonth']\n",
    "    if not isTest:\n",
    "        features.append('Sales')\n",
    "\n",
    "    data = data[features]\n",
    "    return data\n",
    "\n",
    "train = train.sort_values(['Date'],ascending = False)\n",
    "train = process(train)\n",
    "test = process(test,isTest = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "   Store  DayOfWeek  Promo  StateHoliday  SchoolHoliday  StoreType  \\\n0      1          4      1             0              0          3   \n1      1          3      1             0              0          3   \n2      1          2      1             0              0          3   \n3      1          1      1             0              0          3   \n4      1          7      0             0              0          3   \n\n   Assortment  CompetitionDistance  CompetitionOpenSinceMonth  \\\n0           1               1270.0                        9.0   \n1           1               1270.0                        9.0   \n2           1               1270.0                        9.0   \n3           1               1270.0                        9.0   \n4           1               1270.0                        9.0   \n\n   CompetitionOpenSinceYear  Promo2  Promo2SinceWeek  Promo2SinceYear  Year  \\\n0                    2008.0       0              0.0              0.0  2015   \n1                    2008.0       0              0.0              0.0  2015   \n2                    2008.0       0              0.0              0.0  2015   \n3                    2008.0       0              0.0              0.0  2015   \n4                    2008.0       0              0.0              0.0  2015   \n\n   Month  Day  WeekOfYear  CompetitionOpen  PromoOpen  IsPromoMonth  \n0      9   17          38             84.0   24189.50             0  \n1      9   16          38             84.0   24189.50             0  \n2      9   15          38             84.0   24189.50             0  \n3      9   14          38             84.0   24189.50             0  \n4      9   13          37             84.0   24189.25             0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Store</th>\n      <th>DayOfWeek</th>\n      <th>Promo</th>\n      <th>StateHoliday</th>\n      <th>SchoolHoliday</th>\n      <th>StoreType</th>\n      <th>Assortment</th>\n      <th>CompetitionDistance</th>\n      <th>CompetitionOpenSinceMonth</th>\n      <th>CompetitionOpenSinceYear</th>\n      <th>Promo2</th>\n      <th>Promo2SinceWeek</th>\n      <th>Promo2SinceYear</th>\n      <th>Year</th>\n      <th>Month</th>\n      <th>Day</th>\n      <th>WeekOfYear</th>\n      <th>CompetitionOpen</th>\n      <th>PromoOpen</th>\n      <th>IsPromoMonth</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1270.0</td>\n      <td>9.0</td>\n      <td>2008.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2015</td>\n      <td>9</td>\n      <td>17</td>\n      <td>38</td>\n      <td>84.0</td>\n      <td>24189.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1270.0</td>\n      <td>9.0</td>\n      <td>2008.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2015</td>\n      <td>9</td>\n      <td>16</td>\n      <td>38</td>\n      <td>84.0</td>\n      <td>24189.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1270.0</td>\n      <td>9.0</td>\n      <td>2008.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2015</td>\n      <td>9</td>\n      <td>15</td>\n      <td>38</td>\n      <td>84.0</td>\n      <td>24189.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1270.0</td>\n      <td>9.0</td>\n      <td>2008.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2015</td>\n      <td>9</td>\n      <td>14</td>\n      <td>38</td>\n      <td>84.0</td>\n      <td>24189.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1270.0</td>\n      <td>9.0</td>\n      <td>2008.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2015</td>\n      <td>9</td>\n      <td>13</td>\n      <td>37</td>\n      <td>84.0</td>\n      <td>24189.25</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "        Store  DayOfWeek  Promo  StateHoliday  SchoolHoliday  StoreType  \\\n0           1          5      1             0              1          3   \n679364    747          5      1             0              1          3   \n702362    772          5      1             0              1          4   \n683890    752          5      1             0              1          1   \n17714      20          5      1             0              0          4   \n\n        Assortment  CompetitionDistance  CompetitionOpenSinceMonth  \\\n0                1               1270.0                        9.0   \n679364           3              45740.0                        8.0   \n702362           3               1850.0                        0.0   \n683890           1                970.0                        3.0   \n17714            1               2340.0                        5.0   \n\n        CompetitionOpenSinceYear  ...  Promo2SinceWeek  Promo2SinceYear  Year  \\\n0                         2008.0  ...              0.0              0.0  2015   \n679364                    2008.0  ...              0.0              0.0  2015   \n702362                       0.0  ...              0.0              0.0  2015   \n683890                    2013.0  ...             31.0           2013.0  2015   \n17714                     2009.0  ...             40.0           2014.0  2015   \n\n        Month  Day  WeekOfYear  CompetitionOpen  PromoOpen  IsPromoMonth  \\\n0           7   31          31             82.0   24187.75             0   \n679364      7   31          31             83.0   24187.75             0   \n702362      7   31          31          24187.0   24187.75             0   \n683890      7   31          31             28.0      24.00             0   \n17714       7   31          31             74.0       9.75             1   \n\n        Sales  \n0        5263  \n679364  10708  \n702362   5224  \n683890   7763  \n17714    9593  \n\n[5 rows x 21 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Store</th>\n      <th>DayOfWeek</th>\n      <th>Promo</th>\n      <th>StateHoliday</th>\n      <th>SchoolHoliday</th>\n      <th>StoreType</th>\n      <th>Assortment</th>\n      <th>CompetitionDistance</th>\n      <th>CompetitionOpenSinceMonth</th>\n      <th>CompetitionOpenSinceYear</th>\n      <th>...</th>\n      <th>Promo2SinceWeek</th>\n      <th>Promo2SinceYear</th>\n      <th>Year</th>\n      <th>Month</th>\n      <th>Day</th>\n      <th>WeekOfYear</th>\n      <th>CompetitionOpen</th>\n      <th>PromoOpen</th>\n      <th>IsPromoMonth</th>\n      <th>Sales</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1270.0</td>\n      <td>9.0</td>\n      <td>2008.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2015</td>\n      <td>7</td>\n      <td>31</td>\n      <td>31</td>\n      <td>82.0</td>\n      <td>24187.75</td>\n      <td>0</td>\n      <td>5263</td>\n    </tr>\n    <tr>\n      <th>679364</th>\n      <td>747</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>3</td>\n      <td>45740.0</td>\n      <td>8.0</td>\n      <td>2008.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2015</td>\n      <td>7</td>\n      <td>31</td>\n      <td>31</td>\n      <td>83.0</td>\n      <td>24187.75</td>\n      <td>0</td>\n      <td>10708</td>\n    </tr>\n    <tr>\n      <th>702362</th>\n      <td>772</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>4</td>\n      <td>3</td>\n      <td>1850.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2015</td>\n      <td>7</td>\n      <td>31</td>\n      <td>31</td>\n      <td>24187.0</td>\n      <td>24187.75</td>\n      <td>0</td>\n      <td>5224</td>\n    </tr>\n    <tr>\n      <th>683890</th>\n      <td>752</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>970.0</td>\n      <td>3.0</td>\n      <td>2013.0</td>\n      <td>...</td>\n      <td>31.0</td>\n      <td>2013.0</td>\n      <td>2015</td>\n      <td>7</td>\n      <td>31</td>\n      <td>31</td>\n      <td>28.0</td>\n      <td>24.00</td>\n      <td>0</td>\n      <td>7763</td>\n    </tr>\n    <tr>\n      <th>17714</th>\n      <td>20</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>2340.0</td>\n      <td>5.0</td>\n      <td>2009.0</td>\n      <td>...</td>\n      <td>40.0</td>\n      <td>2014.0</td>\n      <td>2015</td>\n      <td>7</td>\n      <td>31</td>\n      <td>31</td>\n      <td>74.0</td>\n      <td>9.75</td>\n      <td>1</td>\n      <td>9593</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 21 columns</p>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "featuresR  = ['Store', 'DayOfWeek', 'Promo', 'StateHoliday', 'SchoolHoliday',\n",
    "       'StoreType', 'Assortment', 'CompetitionDistance',\n",
    "       'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2',\n",
    "       'Promo2SinceWeek', 'Promo2SinceYear', 'Year', 'Month', 'Day',\n",
    "       'WeekOfYear', 'CompetitionOpen', 'PromoOpen', 'IsPromoMonth']\n",
    "train_data = train[featuresR].to_numpy()\n",
    "test_data = test[featuresR].to_numpy()\n",
    "labels_train = train[['Sales']].to_numpy()\n",
    "\n",
    "X_trainR, X_testR, y_trainR, y_testR = train_test_split(train_data, labels_train, random_state=0)\n",
    "X_trainR, X_testR = X_trainR.astype('float32'), X_testR.astype('float32')\n",
    "y_train_labR, y_test_labR = y_trainR[:, 0], y_testR[:, 0]\n",
    "y_trainR, y_testR = y_trainR[:, 1:].astype('float32'), y_testR[:, 1:].astype('float32')\n",
    "scalerR = StandardScaler()\n",
    "scalerR.fit(X_trainR)\n",
    "category_map = {1: [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"], 2:[\"PromoNo\", \"PromoYes\"], 3: [\"NoStateHoliday\", \"PublicHoliday\", \"EasterHoliday\", \"ChristmasHoliday\"],\n",
    "                4:[\"SchoolHolidayNo\", \"SchoolHolidayYes\"], 5: [\"StoreTypeA\", \"StoreTypeB\", \"StoreTypeC\", \"StoreTypeD\", \"StoreTypeE\"], 6:[\"Basic\", \"Extra\", \"Extended\"], 8:[\"None\",\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"], 10: [\"NoPromo2\", \"Promo2\"], 14: [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"], 19: [\"NoPromoMonth\", \"PromoMonth\"] }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Select good wine instance\n",
    "We partition the dataset into good and bad portions and select an instance of interest. I’ve chosen it to be a good quality\n",
    "wine.\n",
    "Note that bad wines are class 1 and correspond to the second model output being high, whereas good wines are class\n",
    "0 and correspond to the first model output being high."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# bad_days = np.array([a for a, b in zip(X_trainR, y_trainR) if b[1] == 1])\n",
    "# good_days = np.array([a for a, b in zip(X_trainR, y_trainR) if b[1] == 0])\n",
    "xR = np.array([[747,5,1,0,1,3,3,45740.0,8.0,2008.0,1,0.0,0.0,2015,7,31,31,83.0,24187.75,0]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "9.1.2 Training models\n",
    "Creating an Autoencoder\n",
    "For some of the explainers, we need an autoencoder to check whether example instances are close to the training data\n",
    "distribution or not."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Random Forest Model\n",
    "We need a tree-based model to get results for the tree SHAP explainer. Hence we train a random forest on the winequality dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "XGBoost Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "# define eval metrics\n",
    "# Mittleres Abweichungsquadrat\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def rmspe(y, yhat):\n",
    "    return np.sqrt(np.mean((yhat/y-1) ** 2))\n",
    "# expm1 ist umkehr von log1p\n",
    "def rmspe_xg(yhat, y):\n",
    "    yhat = yhat\n",
    "    y = y\n",
    "    y = np.expm1(y.get_label())\n",
    "    yhat = np.expm1(yhat)\n",
    "    return \"rmspe\", rmspe(y,yhat)\n",
    "def rmse(ytest, y):\n",
    "    return np.sqrt(mean_squared_error(ytest, y))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "y_train_xgb = y_train_labR\n",
    "y_test_xgb = y_test_labR"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def make_xgb_modelR():\n",
    "    params = {\"objective\": \"reg:linear\", # for linear regression\n",
    "              \"booster\" : \"gbtree\",   # use tree based models\n",
    "              \"eta\": 0.03,   # learning rate\n",
    "              \"max_depth\": 10,    # maximum depth of a tree\n",
    "              \"subsample\": 0.9,    # Subsample ratio of the training instances\n",
    "              \"colsample_bytree\": 0.7,   # Subsample ratio of columns when constructing each tree\n",
    "              \"silent\": 1,   # silent mode\n",
    "              \"seed\": 10,   # Random number seed\n",
    "              \"gpu_id\": 0,\n",
    "              \"tree_method\": \"gpu_hist\",\n",
    "              # \"eval_metric\": \"rmse\"\n",
    "              }\n",
    "    # anzahl trainingsrunden\n",
    "    num_boost_round = 1000\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_trainR, y_train_xgb)\n",
    "    dtest = xgb.DMatrix(X_testR, y_test_xgb)\n",
    "    watchlist = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "    # train the xgboost model\n",
    "    model = xgb.train(params, dtrain, num_boost_round, evals=watchlist, \\\n",
    "      early_stopping_rounds= 1000, verbose_eval=True)\n",
    "    y_predR = model.predict(xgb.DMatrix(X_testR))\n",
    "    print(y_predR)\n",
    "    print('accuracy_score:', accuracy_score(y_predR, y_test_xgb))\n",
    "    # print('f1_score:', f1_score(y_predR, y_test_xgb))\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tensorflow Model\n",
    "Finally, we also train a TensorFlow model.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:43:28] WARNING: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0082aa9edf5298699-1/xgboost/xgboost-ci-windows/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:43:28] WARNING: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0082aa9edf5298699-1/xgboost/xgboost-ci-windows/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[0]\ttrain-rmse:6783.71780\teval-rmse:6776.07166\n",
      "[1]\ttrain-rmse:6602.53123\teval-rmse:6594.85779\n",
      "[2]\ttrain-rmse:6430.81356\teval-rmse:6423.14357\n",
      "[3]\ttrain-rmse:6261.15897\teval-rmse:6253.53184\n",
      "[4]\ttrain-rmse:6111.68577\teval-rmse:6104.08319\n",
      "[5]\ttrain-rmse:5955.24076\teval-rmse:5947.62357\n",
      "[6]\ttrain-rmse:5804.55756\teval-rmse:5796.90853\n",
      "[7]\ttrain-rmse:5655.51778\teval-rmse:5647.99697\n",
      "[8]\ttrain-rmse:5512.68120\teval-rmse:5505.09666\n",
      "[9]\ttrain-rmse:5376.48564\teval-rmse:5368.93140\n",
      "[10]\ttrain-rmse:5244.67746\teval-rmse:5237.26217\n",
      "[11]\ttrain-rmse:5116.35312\teval-rmse:5109.05027\n",
      "[12]\ttrain-rmse:4990.45318\teval-rmse:4983.26423\n",
      "[13]\ttrain-rmse:4880.38357\teval-rmse:4873.43848\n",
      "[14]\ttrain-rmse:4765.83664\teval-rmse:4758.94962\n",
      "[15]\ttrain-rmse:4655.93718\teval-rmse:4649.02198\n",
      "[16]\ttrain-rmse:4549.28470\teval-rmse:4542.44135\n",
      "[17]\ttrain-rmse:4446.80478\teval-rmse:4440.01510\n",
      "[18]\ttrain-rmse:4346.83566\teval-rmse:4340.25912\n",
      "[19]\ttrain-rmse:4251.32388\teval-rmse:4244.76147\n",
      "[20]\ttrain-rmse:4160.83990\teval-rmse:4154.46413\n",
      "[21]\ttrain-rmse:4070.86650\teval-rmse:4064.62009\n",
      "[22]\ttrain-rmse:3984.55403\teval-rmse:3978.37572\n",
      "[23]\ttrain-rmse:3897.91190\teval-rmse:3892.03073\n",
      "[24]\ttrain-rmse:3822.07725\teval-rmse:3816.39507\n",
      "[25]\ttrain-rmse:3750.95500\teval-rmse:3745.52287\n",
      "[26]\ttrain-rmse:3678.80333\teval-rmse:3673.51941\n",
      "[27]\ttrain-rmse:3605.54691\teval-rmse:3600.44455\n",
      "[28]\ttrain-rmse:3538.41433\teval-rmse:3533.79880\n",
      "[29]\ttrain-rmse:3480.02642\teval-rmse:3475.57111\n",
      "[30]\ttrain-rmse:3413.50906\teval-rmse:3409.26969\n",
      "[31]\ttrain-rmse:3349.87543\teval-rmse:3345.88336\n",
      "[32]\ttrain-rmse:3291.59249\teval-rmse:3287.90706\n",
      "[33]\ttrain-rmse:3231.42196\teval-rmse:3227.97568\n",
      "[34]\ttrain-rmse:3173.85303\teval-rmse:3170.66946\n",
      "[35]\ttrain-rmse:3125.07028\teval-rmse:3122.14172\n",
      "[36]\ttrain-rmse:3072.67719\teval-rmse:3070.01767\n",
      "[37]\ttrain-rmse:3022.00437\teval-rmse:3019.64558\n",
      "[38]\ttrain-rmse:2969.23508\teval-rmse:2967.17418\n",
      "[39]\ttrain-rmse:2919.41152\teval-rmse:2917.66233\n",
      "[40]\ttrain-rmse:2875.44913\teval-rmse:2874.13279\n",
      "[41]\ttrain-rmse:2828.74947\teval-rmse:2827.86117\n",
      "[42]\ttrain-rmse:2786.18873\teval-rmse:2785.50222\n",
      "[43]\ttrain-rmse:2753.19831\teval-rmse:2752.73321\n",
      "[44]\ttrain-rmse:2714.41924\teval-rmse:2714.24018\n",
      "[45]\ttrain-rmse:2675.43592\teval-rmse:2675.52968\n",
      "[46]\ttrain-rmse:2641.11005\teval-rmse:2641.45427\n",
      "[47]\ttrain-rmse:2606.43058\teval-rmse:2607.05193\n",
      "[48]\ttrain-rmse:2573.55504\teval-rmse:2574.40068\n",
      "[49]\ttrain-rmse:2544.04915\teval-rmse:2545.18214\n",
      "[50]\ttrain-rmse:2511.37801\teval-rmse:2512.74009\n",
      "[51]\ttrain-rmse:2481.74056\teval-rmse:2483.64047\n",
      "[52]\ttrain-rmse:2450.42596\teval-rmse:2452.59676\n",
      "[53]\ttrain-rmse:2424.10283\teval-rmse:2426.54702\n",
      "[54]\ttrain-rmse:2397.53344\teval-rmse:2400.26390\n",
      "[55]\ttrain-rmse:2370.52434\teval-rmse:2373.53204\n",
      "[56]\ttrain-rmse:2345.84418\teval-rmse:2349.13389\n",
      "[57]\ttrain-rmse:2323.20661\teval-rmse:2326.72016\n",
      "[58]\ttrain-rmse:2301.16265\teval-rmse:2305.01933\n",
      "[59]\ttrain-rmse:2278.07641\teval-rmse:2282.26838\n",
      "[60]\ttrain-rmse:2259.22005\teval-rmse:2263.71563\n",
      "[61]\ttrain-rmse:2238.85670\teval-rmse:2243.60282\n",
      "[62]\ttrain-rmse:2220.54443\teval-rmse:2225.50983\n",
      "[63]\ttrain-rmse:2202.31244\teval-rmse:2207.44294\n",
      "[64]\ttrain-rmse:2184.83500\teval-rmse:2190.17279\n",
      "[65]\ttrain-rmse:2168.11162\teval-rmse:2173.73338\n",
      "[66]\ttrain-rmse:2150.30034\teval-rmse:2156.16511\n",
      "[67]\ttrain-rmse:2135.49131\teval-rmse:2141.66240\n",
      "[68]\ttrain-rmse:2113.03482\teval-rmse:2119.57293\n",
      "[69]\ttrain-rmse:2096.91716\teval-rmse:2103.82737\n",
      "[70]\ttrain-rmse:2084.63123\teval-rmse:2091.77560\n",
      "[71]\ttrain-rmse:2067.39802\teval-rmse:2074.77276\n",
      "[72]\ttrain-rmse:2055.01112\teval-rmse:2062.72402\n",
      "[73]\ttrain-rmse:2044.36879\teval-rmse:2052.39141\n",
      "[74]\ttrain-rmse:2033.91141\teval-rmse:2042.17599\n",
      "[75]\ttrain-rmse:2020.78976\teval-rmse:2029.30633\n",
      "[76]\ttrain-rmse:2006.89201\teval-rmse:2015.63876\n",
      "[77]\ttrain-rmse:1995.31676\teval-rmse:2004.38927\n",
      "[78]\ttrain-rmse:1985.23409\teval-rmse:1994.48083\n",
      "[79]\ttrain-rmse:1976.55201\teval-rmse:1986.00324\n",
      "[80]\ttrain-rmse:1963.84707\teval-rmse:1973.50665\n",
      "[81]\ttrain-rmse:1956.58127\teval-rmse:1966.51048\n",
      "[82]\ttrain-rmse:1944.11277\teval-rmse:1954.28965\n",
      "[83]\ttrain-rmse:1933.81812\teval-rmse:1944.23488\n",
      "[84]\ttrain-rmse:1926.93253\teval-rmse:1937.53157\n",
      "[85]\ttrain-rmse:1916.13596\teval-rmse:1926.94540\n",
      "[86]\ttrain-rmse:1909.18188\teval-rmse:1920.18473\n",
      "[87]\ttrain-rmse:1899.59761\teval-rmse:1910.80987\n",
      "[88]\ttrain-rmse:1891.48335\teval-rmse:1902.84304\n",
      "[89]\ttrain-rmse:1876.62986\teval-rmse:1888.22556\n",
      "[90]\ttrain-rmse:1871.21428\teval-rmse:1883.01354\n",
      "[91]\ttrain-rmse:1862.82232\teval-rmse:1874.90584\n",
      "[92]\ttrain-rmse:1858.21422\teval-rmse:1870.43970\n",
      "[93]\ttrain-rmse:1852.63532\teval-rmse:1865.02440\n",
      "[94]\ttrain-rmse:1845.56593\teval-rmse:1858.02072\n",
      "[95]\ttrain-rmse:1840.23270\teval-rmse:1852.97352\n",
      "[96]\ttrain-rmse:1832.59941\teval-rmse:1845.56159\n",
      "[97]\ttrain-rmse:1825.69418\teval-rmse:1838.82310\n",
      "[98]\ttrain-rmse:1820.22117\teval-rmse:1833.53304\n",
      "[99]\ttrain-rmse:1815.66219\teval-rmse:1829.15546\n",
      "[100]\ttrain-rmse:1809.28054\teval-rmse:1822.90266\n",
      "[101]\ttrain-rmse:1804.42561\teval-rmse:1818.24455\n",
      "[102]\ttrain-rmse:1797.22168\teval-rmse:1811.13563\n",
      "[103]\ttrain-rmse:1793.58424\teval-rmse:1807.62938\n",
      "[104]\ttrain-rmse:1790.26290\teval-rmse:1804.56007\n",
      "[105]\ttrain-rmse:1785.32037\teval-rmse:1799.60178\n",
      "[106]\ttrain-rmse:1781.35448\teval-rmse:1795.79482\n",
      "[107]\ttrain-rmse:1778.68299\teval-rmse:1793.27240\n",
      "[108]\ttrain-rmse:1770.46761\teval-rmse:1785.22060\n",
      "[109]\ttrain-rmse:1761.73672\teval-rmse:1776.57236\n",
      "[110]\ttrain-rmse:1758.62961\teval-rmse:1773.62927\n",
      "[111]\ttrain-rmse:1752.11857\teval-rmse:1767.06615\n",
      "[112]\ttrain-rmse:1749.72978\teval-rmse:1764.78795\n",
      "[113]\ttrain-rmse:1742.47329\teval-rmse:1757.82554\n",
      "[114]\ttrain-rmse:1739.70806\teval-rmse:1755.24654\n",
      "[115]\ttrain-rmse:1737.38804\teval-rmse:1753.05058\n",
      "[116]\ttrain-rmse:1730.32459\teval-rmse:1746.06906\n",
      "[117]\ttrain-rmse:1720.66778\teval-rmse:1736.67421\n",
      "[118]\ttrain-rmse:1712.61612\teval-rmse:1728.74201\n",
      "[119]\ttrain-rmse:1705.99752\teval-rmse:1722.16130\n",
      "[120]\ttrain-rmse:1696.30640\teval-rmse:1712.59511\n",
      "[121]\ttrain-rmse:1691.25952\teval-rmse:1707.74647\n",
      "[122]\ttrain-rmse:1681.88727\teval-rmse:1698.69406\n",
      "[123]\ttrain-rmse:1680.20630\teval-rmse:1697.09712\n",
      "[124]\ttrain-rmse:1678.30655\teval-rmse:1695.34605\n",
      "[125]\ttrain-rmse:1673.69816\teval-rmse:1690.84439\n",
      "[126]\ttrain-rmse:1670.41072\teval-rmse:1687.57740\n",
      "[127]\ttrain-rmse:1662.97365\teval-rmse:1680.34776\n",
      "[128]\ttrain-rmse:1653.64455\teval-rmse:1670.96530\n",
      "[129]\ttrain-rmse:1651.67130\teval-rmse:1669.12710\n",
      "[130]\ttrain-rmse:1650.22712\teval-rmse:1667.81550\n",
      "[131]\ttrain-rmse:1641.00234\teval-rmse:1658.82329\n",
      "[132]\ttrain-rmse:1639.00123\teval-rmse:1656.92341\n",
      "[133]\ttrain-rmse:1637.06910\teval-rmse:1655.16307\n",
      "[134]\ttrain-rmse:1634.16873\teval-rmse:1652.30821\n",
      "[135]\ttrain-rmse:1629.30806\teval-rmse:1647.53903\n",
      "[136]\ttrain-rmse:1626.14497\teval-rmse:1644.51851\n",
      "[137]\ttrain-rmse:1620.33900\teval-rmse:1638.89847\n",
      "[138]\ttrain-rmse:1616.26635\teval-rmse:1634.98440\n",
      "[139]\ttrain-rmse:1606.94870\teval-rmse:1625.89884\n",
      "[140]\ttrain-rmse:1605.21930\teval-rmse:1624.30829\n",
      "[141]\ttrain-rmse:1602.41486\teval-rmse:1621.69545\n",
      "[142]\ttrain-rmse:1599.76791\teval-rmse:1619.12158\n",
      "[143]\ttrain-rmse:1598.66531\teval-rmse:1618.12730\n",
      "[144]\ttrain-rmse:1593.00605\teval-rmse:1612.54492\n",
      "[145]\ttrain-rmse:1587.05193\teval-rmse:1606.70895\n",
      "[146]\ttrain-rmse:1577.34333\teval-rmse:1597.11596\n",
      "[147]\ttrain-rmse:1575.54534\teval-rmse:1595.35013\n",
      "[148]\ttrain-rmse:1574.06284\teval-rmse:1594.00318\n",
      "[149]\ttrain-rmse:1567.50863\teval-rmse:1587.54648\n",
      "[150]\ttrain-rmse:1566.30964\teval-rmse:1586.46848\n",
      "[151]\ttrain-rmse:1563.19366\teval-rmse:1583.40022\n",
      "[152]\ttrain-rmse:1557.87414\teval-rmse:1578.24659\n",
      "[153]\ttrain-rmse:1556.47808\teval-rmse:1576.93269\n",
      "[154]\ttrain-rmse:1554.18077\teval-rmse:1574.69736\n",
      "[155]\ttrain-rmse:1543.53686\teval-rmse:1564.29422\n",
      "[156]\ttrain-rmse:1539.53962\teval-rmse:1560.35684\n",
      "[157]\ttrain-rmse:1538.69452\teval-rmse:1559.60283\n",
      "[158]\ttrain-rmse:1537.43914\teval-rmse:1558.40391\n",
      "[159]\ttrain-rmse:1529.74339\teval-rmse:1550.88874\n",
      "[160]\ttrain-rmse:1521.08389\teval-rmse:1542.24478\n",
      "[161]\ttrain-rmse:1518.77852\teval-rmse:1539.99391\n",
      "[162]\ttrain-rmse:1515.28257\teval-rmse:1536.58508\n",
      "[163]\ttrain-rmse:1509.75591\teval-rmse:1531.23918\n",
      "[164]\ttrain-rmse:1508.76605\teval-rmse:1530.31587\n",
      "[165]\ttrain-rmse:1505.05751\teval-rmse:1526.61941\n",
      "[166]\ttrain-rmse:1503.63777\teval-rmse:1525.24802\n",
      "[167]\ttrain-rmse:1499.32985\teval-rmse:1521.02624\n",
      "[168]\ttrain-rmse:1498.55072\teval-rmse:1520.33784\n",
      "[169]\ttrain-rmse:1494.35456\teval-rmse:1516.27411\n",
      "[170]\ttrain-rmse:1493.01229\teval-rmse:1515.07360\n",
      "[171]\ttrain-rmse:1488.27730\teval-rmse:1510.29508\n",
      "[172]\ttrain-rmse:1486.02104\teval-rmse:1508.22251\n",
      "[173]\ttrain-rmse:1482.59841\teval-rmse:1504.88001\n",
      "[174]\ttrain-rmse:1480.82341\teval-rmse:1503.22515\n",
      "[175]\ttrain-rmse:1474.60144\teval-rmse:1497.14701\n",
      "[176]\ttrain-rmse:1471.98660\teval-rmse:1494.67630\n",
      "[177]\ttrain-rmse:1468.72231\teval-rmse:1491.55487\n",
      "[178]\ttrain-rmse:1467.89288\teval-rmse:1490.83761\n",
      "[179]\ttrain-rmse:1466.13887\teval-rmse:1489.17545\n",
      "[180]\ttrain-rmse:1462.00773\teval-rmse:1485.30321\n",
      "[181]\ttrain-rmse:1456.38582\teval-rmse:1479.81227\n",
      "[182]\ttrain-rmse:1454.27669\teval-rmse:1477.82908\n",
      "[183]\ttrain-rmse:1448.64542\teval-rmse:1472.28578\n",
      "[184]\ttrain-rmse:1447.40297\teval-rmse:1471.23044\n",
      "[185]\ttrain-rmse:1446.28230\teval-rmse:1470.19546\n",
      "[186]\ttrain-rmse:1440.34721\teval-rmse:1464.37024\n",
      "[187]\ttrain-rmse:1437.90841\teval-rmse:1462.07846\n",
      "[188]\ttrain-rmse:1433.74565\teval-rmse:1458.05146\n",
      "[189]\ttrain-rmse:1429.67377\teval-rmse:1454.26008\n",
      "[190]\ttrain-rmse:1423.66618\teval-rmse:1448.23270\n",
      "[191]\ttrain-rmse:1422.82664\teval-rmse:1447.55298\n",
      "[192]\ttrain-rmse:1420.32317\teval-rmse:1445.19686\n",
      "[193]\ttrain-rmse:1417.93610\teval-rmse:1442.82991\n",
      "[194]\ttrain-rmse:1413.37298\teval-rmse:1438.38298\n",
      "[195]\ttrain-rmse:1410.94937\teval-rmse:1436.19941\n",
      "[196]\ttrain-rmse:1409.18600\teval-rmse:1434.58647\n",
      "[197]\ttrain-rmse:1406.67460\teval-rmse:1432.22954\n",
      "[198]\ttrain-rmse:1399.23405\teval-rmse:1424.76554\n",
      "[199]\ttrain-rmse:1395.08344\teval-rmse:1420.59085\n",
      "[200]\ttrain-rmse:1394.26630\teval-rmse:1419.86866\n",
      "[201]\ttrain-rmse:1387.86870\teval-rmse:1413.44660\n",
      "[202]\ttrain-rmse:1382.56773\teval-rmse:1408.17901\n",
      "[203]\ttrain-rmse:1377.95118\teval-rmse:1403.73830\n",
      "[204]\ttrain-rmse:1374.30416\teval-rmse:1400.31758\n",
      "[205]\ttrain-rmse:1371.21471\teval-rmse:1397.30143\n",
      "[206]\ttrain-rmse:1363.90795\teval-rmse:1390.14165\n",
      "[207]\ttrain-rmse:1359.61710\teval-rmse:1385.92098\n",
      "[208]\ttrain-rmse:1355.10408\teval-rmse:1381.48945\n",
      "[209]\ttrain-rmse:1353.94076\teval-rmse:1380.46163\n",
      "[210]\ttrain-rmse:1350.78332\teval-rmse:1377.38849\n",
      "[211]\ttrain-rmse:1350.01565\teval-rmse:1376.73361\n",
      "[212]\ttrain-rmse:1348.11713\teval-rmse:1374.87892\n",
      "[213]\ttrain-rmse:1346.54019\teval-rmse:1373.41816\n",
      "[214]\ttrain-rmse:1339.92597\teval-rmse:1366.97769\n",
      "[215]\ttrain-rmse:1337.57710\teval-rmse:1364.73573\n",
      "[216]\ttrain-rmse:1336.70972\teval-rmse:1364.02088\n",
      "[217]\ttrain-rmse:1334.27563\teval-rmse:1361.61966\n",
      "[218]\ttrain-rmse:1331.95699\teval-rmse:1359.39508\n",
      "[219]\ttrain-rmse:1330.74536\teval-rmse:1358.26802\n",
      "[220]\ttrain-rmse:1327.00803\teval-rmse:1354.59626\n",
      "[221]\ttrain-rmse:1323.11106\teval-rmse:1350.77659\n",
      "[222]\ttrain-rmse:1319.71712\teval-rmse:1347.46067\n",
      "[223]\ttrain-rmse:1316.08551\teval-rmse:1343.90061\n",
      "[224]\ttrain-rmse:1314.01073\teval-rmse:1342.06565\n",
      "[225]\ttrain-rmse:1310.92344\teval-rmse:1339.04334\n",
      "[226]\ttrain-rmse:1308.38527\teval-rmse:1336.58230\n",
      "[227]\ttrain-rmse:1305.43323\teval-rmse:1333.77659\n",
      "[228]\ttrain-rmse:1299.41083\teval-rmse:1327.95652\n",
      "[229]\ttrain-rmse:1296.01786\teval-rmse:1324.55905\n",
      "[230]\ttrain-rmse:1292.48436\teval-rmse:1321.07153\n",
      "[231]\ttrain-rmse:1291.50659\teval-rmse:1320.28914\n",
      "[232]\ttrain-rmse:1289.92162\teval-rmse:1318.80378\n",
      "[233]\ttrain-rmse:1286.67049\teval-rmse:1315.55741\n",
      "[234]\ttrain-rmse:1284.62378\teval-rmse:1313.62367\n",
      "[235]\ttrain-rmse:1283.74049\teval-rmse:1312.82116\n",
      "[236]\ttrain-rmse:1278.59822\teval-rmse:1307.76994\n",
      "[237]\ttrain-rmse:1274.27145\teval-rmse:1303.51426\n",
      "[238]\ttrain-rmse:1271.62528\teval-rmse:1300.92673\n",
      "[239]\ttrain-rmse:1270.67433\teval-rmse:1300.03105\n",
      "[240]\ttrain-rmse:1268.49774\teval-rmse:1297.89616\n",
      "[241]\ttrain-rmse:1265.58068\teval-rmse:1295.02743\n",
      "[242]\ttrain-rmse:1260.12410\teval-rmse:1289.64065\n",
      "[243]\ttrain-rmse:1255.62771\teval-rmse:1285.19231\n",
      "[244]\ttrain-rmse:1251.56276\teval-rmse:1281.18052\n",
      "[245]\ttrain-rmse:1249.38442\teval-rmse:1279.10645\n",
      "[246]\ttrain-rmse:1246.02396\teval-rmse:1275.73609\n",
      "[247]\ttrain-rmse:1245.28961\teval-rmse:1275.08281\n",
      "[248]\ttrain-rmse:1241.71651\teval-rmse:1271.58011\n",
      "[249]\ttrain-rmse:1240.29391\teval-rmse:1270.27321\n",
      "[250]\ttrain-rmse:1235.47729\teval-rmse:1265.57692\n",
      "[251]\ttrain-rmse:1231.92212\teval-rmse:1262.22703\n",
      "[252]\ttrain-rmse:1226.65627\teval-rmse:1256.97570\n",
      "[253]\ttrain-rmse:1224.80872\teval-rmse:1255.12929\n",
      "[254]\ttrain-rmse:1221.15155\teval-rmse:1251.60072\n",
      "[255]\ttrain-rmse:1220.45771\teval-rmse:1250.99745\n",
      "[256]\ttrain-rmse:1219.62835\teval-rmse:1250.20572\n",
      "[257]\ttrain-rmse:1214.81586\teval-rmse:1245.53148\n",
      "[258]\ttrain-rmse:1210.19787\teval-rmse:1240.90376\n",
      "[259]\ttrain-rmse:1206.32619\teval-rmse:1237.19978\n",
      "[260]\ttrain-rmse:1204.20590\teval-rmse:1235.09418\n",
      "[261]\ttrain-rmse:1202.77510\teval-rmse:1233.84620\n",
      "[262]\ttrain-rmse:1199.21169\teval-rmse:1230.39223\n",
      "[263]\ttrain-rmse:1195.18417\teval-rmse:1226.45586\n",
      "[264]\ttrain-rmse:1193.39732\teval-rmse:1224.78413\n",
      "[265]\ttrain-rmse:1191.31884\teval-rmse:1222.70291\n",
      "[266]\ttrain-rmse:1189.37345\teval-rmse:1220.79552\n",
      "[267]\ttrain-rmse:1187.39335\teval-rmse:1218.88086\n",
      "[268]\ttrain-rmse:1183.88746\teval-rmse:1215.49660\n",
      "[269]\ttrain-rmse:1182.84815\teval-rmse:1214.55696\n",
      "[270]\ttrain-rmse:1181.62350\teval-rmse:1213.43609\n",
      "[271]\ttrain-rmse:1180.11051\teval-rmse:1212.10531\n",
      "[272]\ttrain-rmse:1179.45341\teval-rmse:1211.51327\n",
      "[273]\ttrain-rmse:1176.35759\teval-rmse:1208.41845\n",
      "[274]\ttrain-rmse:1173.69948\teval-rmse:1205.83858\n",
      "[275]\ttrain-rmse:1170.42381\teval-rmse:1202.67867\n",
      "[276]\ttrain-rmse:1169.07617\teval-rmse:1201.51169\n",
      "[277]\ttrain-rmse:1166.64198\teval-rmse:1199.10320\n",
      "[278]\ttrain-rmse:1165.69002\teval-rmse:1198.21387\n",
      "[279]\ttrain-rmse:1164.48983\teval-rmse:1197.21979\n",
      "[280]\ttrain-rmse:1163.04371\teval-rmse:1195.85334\n",
      "[281]\ttrain-rmse:1160.58193\teval-rmse:1193.42087\n",
      "[282]\ttrain-rmse:1158.76650\teval-rmse:1191.69770\n",
      "[283]\ttrain-rmse:1158.02320\teval-rmse:1191.02101\n",
      "[284]\ttrain-rmse:1155.59789\teval-rmse:1188.60070\n",
      "[285]\ttrain-rmse:1153.16809\teval-rmse:1186.18301\n",
      "[286]\ttrain-rmse:1151.42768\teval-rmse:1184.47191\n",
      "[287]\ttrain-rmse:1150.53168\teval-rmse:1183.65675\n",
      "[288]\ttrain-rmse:1147.90576\teval-rmse:1181.07429\n",
      "[289]\ttrain-rmse:1145.89986\teval-rmse:1179.14592\n",
      "[290]\ttrain-rmse:1143.80999\teval-rmse:1177.12325\n",
      "[291]\ttrain-rmse:1140.53637\teval-rmse:1174.04521\n",
      "[292]\ttrain-rmse:1137.83948\teval-rmse:1171.45175\n",
      "[293]\ttrain-rmse:1135.72166\teval-rmse:1169.43553\n",
      "[294]\ttrain-rmse:1131.98860\teval-rmse:1165.75391\n",
      "[295]\ttrain-rmse:1128.81279\teval-rmse:1162.72003\n",
      "[296]\ttrain-rmse:1126.58925\teval-rmse:1160.51691\n",
      "[297]\ttrain-rmse:1125.35201\teval-rmse:1159.40768\n",
      "[298]\ttrain-rmse:1122.56108\teval-rmse:1156.78230\n",
      "[299]\ttrain-rmse:1121.18223\teval-rmse:1155.46261\n",
      "[300]\ttrain-rmse:1119.83986\teval-rmse:1154.17849\n",
      "[301]\ttrain-rmse:1117.49706\teval-rmse:1151.97683\n",
      "[302]\ttrain-rmse:1116.25236\teval-rmse:1150.79923\n",
      "[303]\ttrain-rmse:1114.17654\teval-rmse:1148.80812\n",
      "[304]\ttrain-rmse:1111.23671\teval-rmse:1145.93574\n",
      "[305]\ttrain-rmse:1110.42485\teval-rmse:1145.23671\n",
      "[306]\ttrain-rmse:1109.33542\teval-rmse:1144.24473\n",
      "[307]\ttrain-rmse:1107.33161\teval-rmse:1142.23559\n",
      "[308]\ttrain-rmse:1104.79662\teval-rmse:1139.80954\n",
      "[309]\ttrain-rmse:1102.69767\teval-rmse:1137.72503\n",
      "[310]\ttrain-rmse:1100.96375\teval-rmse:1136.00889\n",
      "[311]\ttrain-rmse:1099.75767\teval-rmse:1134.83739\n",
      "[312]\ttrain-rmse:1096.51445\teval-rmse:1131.73218\n",
      "[313]\ttrain-rmse:1094.81739\teval-rmse:1130.11922\n",
      "[314]\ttrain-rmse:1093.04549\teval-rmse:1128.35340\n",
      "[315]\ttrain-rmse:1090.38762\teval-rmse:1125.80294\n",
      "[316]\ttrain-rmse:1086.74939\teval-rmse:1122.26924\n",
      "[317]\ttrain-rmse:1085.21731\teval-rmse:1120.79243\n",
      "[318]\ttrain-rmse:1084.19657\teval-rmse:1119.85300\n",
      "[319]\ttrain-rmse:1083.32336\teval-rmse:1119.13813\n",
      "[320]\ttrain-rmse:1082.16750\teval-rmse:1118.06427\n",
      "[321]\ttrain-rmse:1079.91813\teval-rmse:1115.87809\n",
      "[322]\ttrain-rmse:1076.26716\teval-rmse:1112.30810\n",
      "[323]\ttrain-rmse:1074.46991\teval-rmse:1110.66637\n",
      "[324]\ttrain-rmse:1073.93634\teval-rmse:1110.19595\n",
      "[325]\ttrain-rmse:1073.16897\teval-rmse:1109.47402\n",
      "[326]\ttrain-rmse:1069.49059\teval-rmse:1105.88688\n",
      "[327]\ttrain-rmse:1068.00789\teval-rmse:1104.49294\n",
      "[328]\ttrain-rmse:1066.84379\teval-rmse:1103.50604\n",
      "[329]\ttrain-rmse:1065.51768\teval-rmse:1102.26998\n",
      "[330]\ttrain-rmse:1064.72592\teval-rmse:1101.62939\n",
      "[331]\ttrain-rmse:1062.20746\teval-rmse:1099.22408\n",
      "[332]\ttrain-rmse:1060.91281\teval-rmse:1098.05802\n",
      "[333]\ttrain-rmse:1059.29941\teval-rmse:1096.46592\n",
      "[334]\ttrain-rmse:1058.54436\teval-rmse:1095.73317\n",
      "[335]\ttrain-rmse:1057.67204\teval-rmse:1094.89593\n",
      "[336]\ttrain-rmse:1056.78583\teval-rmse:1094.10782\n",
      "[337]\ttrain-rmse:1056.06642\teval-rmse:1093.50177\n",
      "[338]\ttrain-rmse:1054.99478\teval-rmse:1092.46851\n",
      "[339]\ttrain-rmse:1052.13113\teval-rmse:1089.70312\n",
      "[340]\ttrain-rmse:1051.22218\teval-rmse:1088.94849\n",
      "[341]\ttrain-rmse:1049.63096\teval-rmse:1087.40782\n",
      "[342]\ttrain-rmse:1048.41793\teval-rmse:1086.33282\n",
      "[343]\ttrain-rmse:1046.12995\teval-rmse:1084.17012\n",
      "[344]\ttrain-rmse:1044.93057\teval-rmse:1083.06165\n",
      "[345]\ttrain-rmse:1041.13300\teval-rmse:1079.31789\n",
      "[346]\ttrain-rmse:1040.63660\teval-rmse:1078.87351\n",
      "[347]\ttrain-rmse:1040.04783\teval-rmse:1078.39718\n",
      "[348]\ttrain-rmse:1039.38019\teval-rmse:1077.88038\n",
      "[349]\ttrain-rmse:1038.15677\teval-rmse:1076.72787\n",
      "[350]\ttrain-rmse:1037.10647\teval-rmse:1075.68968\n",
      "[351]\ttrain-rmse:1036.62761\teval-rmse:1075.27781\n",
      "[352]\ttrain-rmse:1034.28983\teval-rmse:1073.02686\n",
      "[353]\ttrain-rmse:1033.11428\teval-rmse:1071.90211\n",
      "[354]\ttrain-rmse:1031.11978\teval-rmse:1069.98965\n",
      "[355]\ttrain-rmse:1029.67056\teval-rmse:1068.59853\n",
      "[356]\ttrain-rmse:1028.89047\teval-rmse:1067.96729\n",
      "[357]\ttrain-rmse:1028.00564\teval-rmse:1067.22975\n",
      "[358]\ttrain-rmse:1026.53145\teval-rmse:1065.79139\n",
      "[359]\ttrain-rmse:1023.15125\teval-rmse:1062.46394\n",
      "[360]\ttrain-rmse:1021.47064\teval-rmse:1060.96159\n",
      "[361]\ttrain-rmse:1020.41222\teval-rmse:1059.98957\n",
      "[362]\ttrain-rmse:1017.68680\teval-rmse:1057.30402\n",
      "[363]\ttrain-rmse:1017.10337\teval-rmse:1056.84935\n",
      "[364]\ttrain-rmse:1016.45356\teval-rmse:1056.33366\n",
      "[365]\ttrain-rmse:1014.59346\teval-rmse:1054.63232\n",
      "[366]\ttrain-rmse:1013.38366\teval-rmse:1053.47601\n",
      "[367]\ttrain-rmse:1012.29679\teval-rmse:1052.43573\n",
      "[368]\ttrain-rmse:1011.15421\teval-rmse:1051.33431\n",
      "[369]\ttrain-rmse:1010.63126\teval-rmse:1050.83172\n",
      "[370]\ttrain-rmse:1008.72346\teval-rmse:1049.03589\n",
      "[371]\ttrain-rmse:1007.66114\teval-rmse:1048.02325\n",
      "[372]\ttrain-rmse:1006.59489\teval-rmse:1046.99692\n",
      "[373]\ttrain-rmse:1004.92380\teval-rmse:1045.39124\n",
      "[374]\ttrain-rmse:1004.16829\teval-rmse:1044.70882\n",
      "[375]\ttrain-rmse:1003.54628\teval-rmse:1044.11716\n",
      "[376]\ttrain-rmse:1002.25765\teval-rmse:1042.91499\n",
      "[377]\ttrain-rmse:998.97935\teval-rmse:1039.73872\n",
      "[378]\ttrain-rmse:998.04470\teval-rmse:1038.85903\n",
      "[379]\ttrain-rmse:996.84202\teval-rmse:1037.74241\n",
      "[380]\ttrain-rmse:995.79585\teval-rmse:1036.75497\n",
      "[381]\ttrain-rmse:994.38007\teval-rmse:1035.48222\n",
      "[382]\ttrain-rmse:992.97675\teval-rmse:1034.18494\n",
      "[383]\ttrain-rmse:991.99613\teval-rmse:1033.30407\n",
      "[384]\ttrain-rmse:990.02494\teval-rmse:1031.45748\n",
      "[385]\ttrain-rmse:989.05559\teval-rmse:1030.56584\n",
      "[386]\ttrain-rmse:988.54822\teval-rmse:1030.18290\n",
      "[387]\ttrain-rmse:987.34445\teval-rmse:1029.04864\n",
      "[388]\ttrain-rmse:986.06480\teval-rmse:1027.82653\n",
      "[389]\ttrain-rmse:985.24769\teval-rmse:1027.12986\n",
      "[390]\ttrain-rmse:984.46212\teval-rmse:1026.46180\n",
      "[391]\ttrain-rmse:983.83462\teval-rmse:1025.91305\n",
      "[392]\ttrain-rmse:982.60771\teval-rmse:1024.74086\n",
      "[393]\ttrain-rmse:981.71131\teval-rmse:1023.84064\n",
      "[394]\ttrain-rmse:981.07381\teval-rmse:1023.26204\n",
      "[395]\ttrain-rmse:980.22332\teval-rmse:1022.48169\n",
      "[396]\ttrain-rmse:979.49341\teval-rmse:1021.89643\n",
      "[397]\ttrain-rmse:977.68978\teval-rmse:1020.13950\n",
      "[398]\ttrain-rmse:976.96171\teval-rmse:1019.49297\n",
      "[399]\ttrain-rmse:976.00948\teval-rmse:1018.64518\n",
      "[400]\ttrain-rmse:973.73525\teval-rmse:1016.42967\n",
      "[401]\ttrain-rmse:973.03845\teval-rmse:1015.87092\n",
      "[402]\ttrain-rmse:970.54680\teval-rmse:1013.54030\n",
      "[403]\ttrain-rmse:969.10423\teval-rmse:1012.18710\n",
      "[404]\ttrain-rmse:968.44468\teval-rmse:1011.61026\n",
      "[405]\ttrain-rmse:967.69224\teval-rmse:1010.87294\n",
      "[406]\ttrain-rmse:967.01636\teval-rmse:1010.29927\n",
      "[407]\ttrain-rmse:966.10412\teval-rmse:1009.39466\n",
      "[408]\ttrain-rmse:964.77366\teval-rmse:1008.15329\n",
      "[409]\ttrain-rmse:964.07522\teval-rmse:1007.50514\n",
      "[410]\ttrain-rmse:963.30235\teval-rmse:1006.77558\n",
      "[411]\ttrain-rmse:962.59438\teval-rmse:1006.08848\n",
      "[412]\ttrain-rmse:962.07146\teval-rmse:1005.61992\n",
      "[413]\ttrain-rmse:961.58206\teval-rmse:1005.22895\n",
      "[414]\ttrain-rmse:960.89380\teval-rmse:1004.62279\n",
      "[415]\ttrain-rmse:960.06381\teval-rmse:1003.82080\n",
      "[416]\ttrain-rmse:959.04263\teval-rmse:1002.82636\n",
      "[417]\ttrain-rmse:958.08104\teval-rmse:1001.92292\n",
      "[418]\ttrain-rmse:956.89098\teval-rmse:1000.77462\n",
      "[419]\ttrain-rmse:954.96838\teval-rmse:998.86769\n",
      "[420]\ttrain-rmse:954.33421\teval-rmse:998.28582\n",
      "[421]\ttrain-rmse:953.21237\teval-rmse:997.24588\n",
      "[422]\ttrain-rmse:951.02181\teval-rmse:995.14523\n",
      "[423]\ttrain-rmse:950.33446\teval-rmse:994.48994\n",
      "[424]\ttrain-rmse:949.72911\teval-rmse:993.97477\n",
      "[425]\ttrain-rmse:948.59208\teval-rmse:992.96901\n",
      "[426]\ttrain-rmse:947.22484\teval-rmse:991.68124\n",
      "[427]\ttrain-rmse:946.35202\teval-rmse:990.86182\n",
      "[428]\ttrain-rmse:945.27608\teval-rmse:989.90257\n",
      "[429]\ttrain-rmse:943.98075\teval-rmse:988.67885\n",
      "[430]\ttrain-rmse:942.95393\teval-rmse:987.72334\n",
      "[431]\ttrain-rmse:942.07458\teval-rmse:986.88855\n",
      "[432]\ttrain-rmse:941.36341\teval-rmse:986.25629\n",
      "[433]\ttrain-rmse:938.95335\teval-rmse:983.89503\n",
      "[434]\ttrain-rmse:938.42861\teval-rmse:983.46048\n",
      "[435]\ttrain-rmse:937.18709\teval-rmse:982.27716\n",
      "[436]\ttrain-rmse:933.82113\teval-rmse:978.91874\n",
      "[437]\ttrain-rmse:933.19151\teval-rmse:978.33140\n",
      "[438]\ttrain-rmse:932.30365\teval-rmse:977.50059\n",
      "[439]\ttrain-rmse:931.84117\teval-rmse:977.15323\n",
      "[440]\ttrain-rmse:930.99004\teval-rmse:976.34767\n",
      "[441]\ttrain-rmse:928.37178\teval-rmse:973.78486\n",
      "[442]\ttrain-rmse:927.22405\teval-rmse:972.74838\n",
      "[443]\ttrain-rmse:926.89285\teval-rmse:972.52519\n",
      "[444]\ttrain-rmse:926.31155\teval-rmse:972.01818\n",
      "[445]\ttrain-rmse:925.80605\teval-rmse:971.62255\n",
      "[446]\ttrain-rmse:924.98945\teval-rmse:970.90488\n",
      "[447]\ttrain-rmse:923.87831\teval-rmse:969.95089\n",
      "[448]\ttrain-rmse:922.38251\teval-rmse:968.51119\n",
      "[449]\ttrain-rmse:921.55460\teval-rmse:967.71477\n",
      "[450]\ttrain-rmse:920.93558\teval-rmse:967.14603\n",
      "[451]\ttrain-rmse:920.43847\teval-rmse:966.66204\n",
      "[452]\ttrain-rmse:919.27528\teval-rmse:965.57059\n",
      "[453]\ttrain-rmse:918.64326\teval-rmse:964.96829\n",
      "[454]\ttrain-rmse:917.87879\teval-rmse:964.24493\n",
      "[455]\ttrain-rmse:917.23766\teval-rmse:963.62479\n",
      "[456]\ttrain-rmse:915.16026\teval-rmse:961.63115\n",
      "[457]\ttrain-rmse:913.28967\teval-rmse:959.88944\n",
      "[458]\ttrain-rmse:910.76681\teval-rmse:957.47791\n",
      "[459]\ttrain-rmse:909.70225\teval-rmse:956.52393\n",
      "[460]\ttrain-rmse:909.30689\teval-rmse:956.15792\n",
      "[461]\ttrain-rmse:908.89789\teval-rmse:955.84286\n",
      "[462]\ttrain-rmse:908.33875\teval-rmse:955.32967\n",
      "[463]\ttrain-rmse:907.58868\teval-rmse:954.62894\n",
      "[464]\ttrain-rmse:907.12577\teval-rmse:954.23879\n",
      "[465]\ttrain-rmse:906.44463\teval-rmse:953.61909\n",
      "[466]\ttrain-rmse:904.89077\teval-rmse:952.13626\n",
      "[467]\ttrain-rmse:903.92524\teval-rmse:951.28163\n",
      "[468]\ttrain-rmse:903.28493\teval-rmse:950.65872\n",
      "[469]\ttrain-rmse:902.60028\teval-rmse:950.03522\n",
      "[470]\ttrain-rmse:902.05865\teval-rmse:949.51266\n",
      "[471]\ttrain-rmse:900.84884\teval-rmse:948.36121\n",
      "[472]\ttrain-rmse:899.45423\teval-rmse:947.02960\n",
      "[473]\ttrain-rmse:897.56283\teval-rmse:945.17827\n",
      "[474]\ttrain-rmse:896.34679\teval-rmse:944.05394\n",
      "[475]\ttrain-rmse:895.36878\teval-rmse:943.16929\n",
      "[476]\ttrain-rmse:894.31447\teval-rmse:942.20444\n",
      "[477]\ttrain-rmse:892.46514\teval-rmse:940.47113\n",
      "[478]\ttrain-rmse:891.78823\teval-rmse:939.90065\n",
      "[479]\ttrain-rmse:890.79485\teval-rmse:938.98110\n",
      "[480]\ttrain-rmse:890.21123\teval-rmse:938.48841\n",
      "[481]\ttrain-rmse:889.24184\teval-rmse:937.59265\n",
      "[482]\ttrain-rmse:888.27399\teval-rmse:936.71181\n",
      "[483]\ttrain-rmse:887.18958\teval-rmse:935.74042\n",
      "[484]\ttrain-rmse:886.32373\teval-rmse:934.98294\n",
      "[485]\ttrain-rmse:885.05482\teval-rmse:933.86482\n",
      "[486]\ttrain-rmse:884.50463\teval-rmse:933.39282\n",
      "[487]\ttrain-rmse:883.64150\teval-rmse:932.61956\n",
      "[488]\ttrain-rmse:882.38974\teval-rmse:931.53538\n",
      "[489]\ttrain-rmse:881.76952\teval-rmse:931.02014\n",
      "[490]\ttrain-rmse:881.04960\teval-rmse:930.31860\n",
      "[491]\ttrain-rmse:880.52726\teval-rmse:929.88997\n",
      "[492]\ttrain-rmse:879.74189\teval-rmse:929.16222\n",
      "[493]\ttrain-rmse:878.81117\teval-rmse:928.27440\n",
      "[494]\ttrain-rmse:877.76853\teval-rmse:927.33495\n",
      "[495]\ttrain-rmse:876.76866\teval-rmse:926.46456\n",
      "[496]\ttrain-rmse:875.48264\teval-rmse:925.25466\n",
      "[497]\ttrain-rmse:873.73302\teval-rmse:923.59831\n",
      "[498]\ttrain-rmse:873.34569\teval-rmse:923.24667\n",
      "[499]\ttrain-rmse:872.46265\teval-rmse:922.45143\n",
      "[500]\ttrain-rmse:871.76990\teval-rmse:921.83753\n",
      "[501]\ttrain-rmse:870.21468\teval-rmse:920.35099\n",
      "[502]\ttrain-rmse:869.23753\teval-rmse:919.42878\n",
      "[503]\ttrain-rmse:868.14509\teval-rmse:918.42154\n",
      "[504]\ttrain-rmse:867.23522\teval-rmse:917.57302\n",
      "[505]\ttrain-rmse:866.94083\teval-rmse:917.34907\n",
      "[506]\ttrain-rmse:866.30002\teval-rmse:916.77125\n",
      "[507]\ttrain-rmse:865.83577\teval-rmse:916.37107\n",
      "[508]\ttrain-rmse:865.55992\teval-rmse:916.19359\n",
      "[509]\ttrain-rmse:864.98123\teval-rmse:915.70977\n",
      "[510]\ttrain-rmse:863.82936\teval-rmse:914.65158\n",
      "[511]\ttrain-rmse:863.24077\teval-rmse:914.09405\n",
      "[512]\ttrain-rmse:862.68359\teval-rmse:913.67042\n",
      "[513]\ttrain-rmse:862.32077\teval-rmse:913.40457\n",
      "[514]\ttrain-rmse:861.63033\teval-rmse:912.76403\n",
      "[515]\ttrain-rmse:859.29333\teval-rmse:910.54150\n",
      "[516]\ttrain-rmse:858.50730\teval-rmse:909.82532\n",
      "[517]\ttrain-rmse:858.18459\teval-rmse:909.55441\n",
      "[518]\ttrain-rmse:857.48105\teval-rmse:908.89758\n",
      "[519]\ttrain-rmse:856.97240\teval-rmse:908.43682\n",
      "[520]\ttrain-rmse:856.34814\teval-rmse:907.85987\n",
      "[521]\ttrain-rmse:855.87483\teval-rmse:907.44717\n",
      "[522]\ttrain-rmse:854.44086\teval-rmse:906.09118\n",
      "[523]\ttrain-rmse:852.85125\teval-rmse:904.65960\n",
      "[524]\ttrain-rmse:851.62570\teval-rmse:903.51299\n",
      "[525]\ttrain-rmse:850.64685\teval-rmse:902.59093\n",
      "[526]\ttrain-rmse:849.67893\teval-rmse:901.68122\n",
      "[527]\ttrain-rmse:849.34314\teval-rmse:901.42124\n",
      "[528]\ttrain-rmse:848.31141\teval-rmse:900.46030\n",
      "[529]\ttrain-rmse:847.90249\teval-rmse:900.07882\n",
      "[530]\ttrain-rmse:847.53120\teval-rmse:899.76493\n",
      "[531]\ttrain-rmse:846.99083\teval-rmse:899.29057\n",
      "[532]\ttrain-rmse:845.89736\teval-rmse:898.29334\n",
      "[533]\ttrain-rmse:845.35626\teval-rmse:897.85934\n",
      "[534]\ttrain-rmse:844.75856\teval-rmse:897.38929\n",
      "[535]\ttrain-rmse:844.26947\teval-rmse:896.94100\n",
      "[536]\ttrain-rmse:843.79147\teval-rmse:896.51097\n",
      "[537]\ttrain-rmse:843.12217\teval-rmse:895.91159\n",
      "[538]\ttrain-rmse:842.00713\teval-rmse:894.87779\n",
      "[539]\ttrain-rmse:841.30979\teval-rmse:894.23348\n",
      "[540]\ttrain-rmse:840.16713\teval-rmse:893.18379\n",
      "[541]\ttrain-rmse:839.88155\teval-rmse:892.91278\n",
      "[542]\ttrain-rmse:838.41043\teval-rmse:891.48045\n",
      "[543]\ttrain-rmse:838.11328\teval-rmse:891.24787\n",
      "[544]\ttrain-rmse:837.57554\teval-rmse:890.78947\n",
      "[545]\ttrain-rmse:837.11315\teval-rmse:890.43194\n",
      "[546]\ttrain-rmse:836.79470\teval-rmse:890.18150\n",
      "[547]\ttrain-rmse:835.58210\teval-rmse:888.97215\n",
      "[548]\ttrain-rmse:835.30187\teval-rmse:888.79895\n",
      "[549]\ttrain-rmse:834.60369\teval-rmse:888.17708\n",
      "[550]\ttrain-rmse:833.70357\teval-rmse:887.33910\n",
      "[551]\ttrain-rmse:833.19351\teval-rmse:886.94317\n",
      "[552]\ttrain-rmse:832.75920\teval-rmse:886.57820\n",
      "[553]\ttrain-rmse:832.39729\teval-rmse:886.31477\n",
      "[554]\ttrain-rmse:831.91246\teval-rmse:885.94062\n",
      "[555]\ttrain-rmse:831.14967\teval-rmse:885.29081\n",
      "[556]\ttrain-rmse:830.49481\teval-rmse:884.75024\n",
      "[557]\ttrain-rmse:830.23291\teval-rmse:884.58738\n",
      "[558]\ttrain-rmse:829.85728\teval-rmse:884.24970\n",
      "[559]\ttrain-rmse:829.19959\teval-rmse:883.65647\n",
      "[560]\ttrain-rmse:828.73696\teval-rmse:883.23421\n",
      "[561]\ttrain-rmse:827.91529\teval-rmse:882.53897\n",
      "[562]\ttrain-rmse:826.93008\teval-rmse:881.61877\n",
      "[563]\ttrain-rmse:826.39048\teval-rmse:881.15765\n",
      "[564]\ttrain-rmse:826.10201\teval-rmse:880.95242\n",
      "[565]\ttrain-rmse:825.12866\teval-rmse:880.04139\n",
      "[566]\ttrain-rmse:824.57407\teval-rmse:879.54767\n",
      "[567]\ttrain-rmse:823.78453\teval-rmse:878.85911\n",
      "[568]\ttrain-rmse:823.52536\teval-rmse:878.66981\n",
      "[569]\ttrain-rmse:822.75763\teval-rmse:877.99815\n",
      "[570]\ttrain-rmse:821.42829\teval-rmse:876.72042\n",
      "[571]\ttrain-rmse:820.84771\teval-rmse:876.21974\n",
      "[572]\ttrain-rmse:820.40230\teval-rmse:875.87621\n",
      "[573]\ttrain-rmse:818.63427\teval-rmse:874.18276\n",
      "[574]\ttrain-rmse:818.20450\teval-rmse:873.80107\n",
      "[575]\ttrain-rmse:816.41753\teval-rmse:872.05618\n",
      "[576]\ttrain-rmse:815.84782\teval-rmse:871.57334\n",
      "[577]\ttrain-rmse:814.79378\teval-rmse:870.54713\n",
      "[578]\ttrain-rmse:814.35510\teval-rmse:870.16874\n",
      "[579]\ttrain-rmse:813.83337\teval-rmse:869.69343\n",
      "[580]\ttrain-rmse:813.27817\teval-rmse:869.20584\n",
      "[581]\ttrain-rmse:812.17297\teval-rmse:868.15883\n",
      "[582]\ttrain-rmse:811.44272\teval-rmse:867.46216\n",
      "[583]\ttrain-rmse:810.21558\teval-rmse:866.26515\n",
      "[584]\ttrain-rmse:809.44616\teval-rmse:865.51376\n",
      "[585]\ttrain-rmse:809.13829\teval-rmse:865.30128\n",
      "[586]\ttrain-rmse:808.73921\teval-rmse:864.97750\n",
      "[587]\ttrain-rmse:808.04028\teval-rmse:864.37504\n",
      "[588]\ttrain-rmse:807.58820\teval-rmse:863.95617\n",
      "[589]\ttrain-rmse:806.90171\teval-rmse:863.28460\n",
      "[590]\ttrain-rmse:806.08075\teval-rmse:862.51385\n",
      "[591]\ttrain-rmse:805.08764\teval-rmse:861.64607\n",
      "[592]\ttrain-rmse:803.55596\teval-rmse:860.18610\n",
      "[593]\ttrain-rmse:802.80227\teval-rmse:859.47035\n",
      "[594]\ttrain-rmse:802.58059\teval-rmse:859.29101\n",
      "[595]\ttrain-rmse:802.08148\teval-rmse:858.90181\n",
      "[596]\ttrain-rmse:801.41336\teval-rmse:858.31381\n",
      "[597]\ttrain-rmse:801.16032\teval-rmse:858.11830\n",
      "[598]\ttrain-rmse:800.72605\teval-rmse:857.71740\n",
      "[599]\ttrain-rmse:800.11009\teval-rmse:857.15075\n",
      "[600]\ttrain-rmse:799.52164\teval-rmse:856.66583\n",
      "[601]\ttrain-rmse:799.03359\teval-rmse:856.24801\n",
      "[602]\ttrain-rmse:798.55793\teval-rmse:855.92012\n",
      "[603]\ttrain-rmse:798.20320\teval-rmse:855.59717\n",
      "[604]\ttrain-rmse:798.03893\teval-rmse:855.46866\n",
      "[605]\ttrain-rmse:797.66220\teval-rmse:855.16294\n",
      "[606]\ttrain-rmse:797.22170\teval-rmse:854.78492\n",
      "[607]\ttrain-rmse:796.93178\teval-rmse:854.55227\n",
      "[608]\ttrain-rmse:796.42963\teval-rmse:854.09003\n",
      "[609]\ttrain-rmse:796.21412\teval-rmse:853.94347\n",
      "[610]\ttrain-rmse:795.46548\teval-rmse:853.30381\n",
      "[611]\ttrain-rmse:794.81776\teval-rmse:852.77680\n",
      "[612]\ttrain-rmse:793.48337\teval-rmse:851.52316\n",
      "[613]\ttrain-rmse:792.84077\teval-rmse:851.01298\n",
      "[614]\ttrain-rmse:792.36657\teval-rmse:850.58314\n",
      "[615]\ttrain-rmse:791.02171\teval-rmse:849.34683\n",
      "[616]\ttrain-rmse:790.36030\teval-rmse:848.72699\n",
      "[617]\ttrain-rmse:789.95633\teval-rmse:848.38805\n",
      "[618]\ttrain-rmse:788.95491\teval-rmse:847.51387\n",
      "[619]\ttrain-rmse:788.16920\teval-rmse:846.81809\n",
      "[620]\ttrain-rmse:787.08632\teval-rmse:845.82748\n",
      "[621]\ttrain-rmse:786.70740\teval-rmse:845.54734\n",
      "[622]\ttrain-rmse:785.73223\teval-rmse:844.63835\n",
      "[623]\ttrain-rmse:785.42538\teval-rmse:844.38063\n",
      "[624]\ttrain-rmse:784.98638\teval-rmse:843.96303\n",
      "[625]\ttrain-rmse:784.49116\teval-rmse:843.53604\n",
      "[626]\ttrain-rmse:783.93665\teval-rmse:843.03594\n",
      "[627]\ttrain-rmse:783.56592\teval-rmse:842.75733\n",
      "[628]\ttrain-rmse:783.15986\teval-rmse:842.37883\n",
      "[629]\ttrain-rmse:782.62339\teval-rmse:841.91032\n",
      "[630]\ttrain-rmse:782.42518\teval-rmse:841.78330\n",
      "[631]\ttrain-rmse:781.75566\teval-rmse:841.15487\n",
      "[632]\ttrain-rmse:781.02987\teval-rmse:840.49731\n",
      "[633]\ttrain-rmse:780.78853\teval-rmse:840.34431\n",
      "[634]\ttrain-rmse:780.36497\teval-rmse:840.02197\n",
      "[635]\ttrain-rmse:779.91578\teval-rmse:839.62924\n",
      "[636]\ttrain-rmse:779.44733\teval-rmse:839.29754\n",
      "[637]\ttrain-rmse:779.13193\teval-rmse:839.06708\n",
      "[638]\ttrain-rmse:778.67548\teval-rmse:838.71302\n",
      "[639]\ttrain-rmse:778.08635\teval-rmse:838.19006\n",
      "[640]\ttrain-rmse:777.75502\teval-rmse:837.99893\n",
      "[641]\ttrain-rmse:777.37679\teval-rmse:837.69861\n",
      "[642]\ttrain-rmse:776.95793\teval-rmse:837.29635\n",
      "[643]\ttrain-rmse:776.76404\teval-rmse:837.19374\n",
      "[644]\ttrain-rmse:776.22693\teval-rmse:836.79039\n",
      "[645]\ttrain-rmse:776.03616\teval-rmse:836.63878\n",
      "[646]\ttrain-rmse:775.57343\teval-rmse:836.28628\n",
      "[647]\ttrain-rmse:774.93549\teval-rmse:835.68380\n",
      "[648]\ttrain-rmse:774.36222\teval-rmse:835.13344\n",
      "[649]\ttrain-rmse:774.19023\teval-rmse:835.02564\n",
      "[650]\ttrain-rmse:772.91883\teval-rmse:833.80929\n",
      "[651]\ttrain-rmse:772.75132\teval-rmse:833.68731\n",
      "[652]\ttrain-rmse:771.92936\teval-rmse:832.93186\n",
      "[653]\ttrain-rmse:771.46531\teval-rmse:832.50733\n",
      "[654]\ttrain-rmse:771.34450\teval-rmse:832.43183\n",
      "[655]\ttrain-rmse:770.70502\teval-rmse:831.83220\n",
      "[656]\ttrain-rmse:770.30270\teval-rmse:831.48819\n",
      "[657]\ttrain-rmse:769.82533\teval-rmse:831.07223\n",
      "[658]\ttrain-rmse:769.21566\teval-rmse:830.54737\n",
      "[659]\ttrain-rmse:768.67522\teval-rmse:830.08245\n",
      "[660]\ttrain-rmse:768.11666\teval-rmse:829.57673\n",
      "[661]\ttrain-rmse:767.21974\teval-rmse:828.78118\n",
      "[662]\ttrain-rmse:766.93288\teval-rmse:828.57578\n",
      "[663]\ttrain-rmse:766.70534\teval-rmse:828.39171\n",
      "[664]\ttrain-rmse:766.39144\teval-rmse:828.12302\n",
      "[665]\ttrain-rmse:766.12196\teval-rmse:827.90695\n",
      "[666]\ttrain-rmse:765.92830\teval-rmse:827.75897\n",
      "[667]\ttrain-rmse:765.62458\teval-rmse:827.54211\n",
      "[668]\ttrain-rmse:765.04193\teval-rmse:827.01515\n",
      "[669]\ttrain-rmse:764.75611\teval-rmse:826.79633\n",
      "[670]\ttrain-rmse:764.25962\teval-rmse:826.42952\n",
      "[671]\ttrain-rmse:764.10606\teval-rmse:826.34165\n",
      "[672]\ttrain-rmse:763.44113\teval-rmse:825.75047\n",
      "[673]\ttrain-rmse:763.11426\teval-rmse:825.50966\n",
      "[674]\ttrain-rmse:762.24626\teval-rmse:824.65561\n",
      "[675]\ttrain-rmse:761.94173\teval-rmse:824.39922\n",
      "[676]\ttrain-rmse:761.44933\teval-rmse:823.94425\n",
      "[677]\ttrain-rmse:760.92552\teval-rmse:823.44127\n",
      "[678]\ttrain-rmse:760.20892\teval-rmse:822.77535\n",
      "[679]\ttrain-rmse:759.85102\teval-rmse:822.48988\n",
      "[680]\ttrain-rmse:759.67841\teval-rmse:822.34795\n",
      "[681]\ttrain-rmse:759.35135\teval-rmse:822.03897\n",
      "[682]\ttrain-rmse:759.03157\teval-rmse:821.79542\n",
      "[683]\ttrain-rmse:758.66398\teval-rmse:821.46775\n",
      "[684]\ttrain-rmse:758.34085\teval-rmse:821.16264\n",
      "[685]\ttrain-rmse:758.11832\teval-rmse:821.01623\n",
      "[686]\ttrain-rmse:757.58258\teval-rmse:820.57392\n",
      "[687]\ttrain-rmse:757.22651\teval-rmse:820.27383\n",
      "[688]\ttrain-rmse:756.78491\teval-rmse:819.88162\n",
      "[689]\ttrain-rmse:756.35311\teval-rmse:819.54662\n",
      "[690]\ttrain-rmse:755.97154\teval-rmse:819.25496\n",
      "[691]\ttrain-rmse:755.68789\teval-rmse:819.05599\n",
      "[692]\ttrain-rmse:755.17658\teval-rmse:818.60523\n",
      "[693]\ttrain-rmse:754.93926\teval-rmse:818.45443\n",
      "[694]\ttrain-rmse:754.15775\teval-rmse:817.76892\n",
      "[695]\ttrain-rmse:753.84760\teval-rmse:817.55596\n",
      "[696]\ttrain-rmse:753.41240\teval-rmse:817.14414\n",
      "[697]\ttrain-rmse:752.79533\teval-rmse:816.58466\n",
      "[698]\ttrain-rmse:752.40630\teval-rmse:816.31612\n",
      "[699]\ttrain-rmse:751.84618\teval-rmse:815.86577\n",
      "[700]\ttrain-rmse:751.44156\teval-rmse:815.49972\n",
      "[701]\ttrain-rmse:750.87010\teval-rmse:814.98268\n",
      "[702]\ttrain-rmse:750.48974\teval-rmse:814.74680\n",
      "[703]\ttrain-rmse:749.95540\teval-rmse:814.27481\n",
      "[704]\ttrain-rmse:749.40519\teval-rmse:813.74126\n",
      "[705]\ttrain-rmse:749.27961\teval-rmse:813.68958\n",
      "[706]\ttrain-rmse:748.89176\teval-rmse:813.38592\n",
      "[707]\ttrain-rmse:748.23159\teval-rmse:812.78757\n",
      "[708]\ttrain-rmse:747.77859\teval-rmse:812.41213\n",
      "[709]\ttrain-rmse:747.27471\teval-rmse:812.00910\n",
      "[710]\ttrain-rmse:746.51828\teval-rmse:811.31373\n",
      "[711]\ttrain-rmse:746.28337\teval-rmse:811.17331\n",
      "[712]\ttrain-rmse:746.02087\teval-rmse:811.00884\n",
      "[713]\ttrain-rmse:745.58409\teval-rmse:810.64417\n",
      "[714]\ttrain-rmse:745.03036\teval-rmse:810.16700\n",
      "[715]\ttrain-rmse:744.87077\teval-rmse:810.05774\n",
      "[716]\ttrain-rmse:744.59878\teval-rmse:809.79439\n",
      "[717]\ttrain-rmse:744.28926\teval-rmse:809.49021\n",
      "[718]\ttrain-rmse:743.93449\teval-rmse:809.21967\n",
      "[719]\ttrain-rmse:743.79028\teval-rmse:809.12024\n",
      "[720]\ttrain-rmse:743.19193\teval-rmse:808.59502\n",
      "[721]\ttrain-rmse:742.64036\teval-rmse:808.10615\n",
      "[722]\ttrain-rmse:742.30037\teval-rmse:807.81642\n",
      "[723]\ttrain-rmse:742.01699\teval-rmse:807.61223\n",
      "[724]\ttrain-rmse:741.35424\teval-rmse:806.99837\n",
      "[725]\ttrain-rmse:741.06960\teval-rmse:806.82441\n",
      "[726]\ttrain-rmse:740.80894\teval-rmse:806.61404\n",
      "[727]\ttrain-rmse:740.28960\teval-rmse:806.19540\n",
      "[728]\ttrain-rmse:739.93742\teval-rmse:805.87573\n",
      "[729]\ttrain-rmse:739.65645\teval-rmse:805.65795\n",
      "[730]\ttrain-rmse:739.32466\teval-rmse:805.37127\n",
      "[731]\ttrain-rmse:738.95057\teval-rmse:805.13069\n",
      "[732]\ttrain-rmse:738.34222\teval-rmse:804.55177\n",
      "[733]\ttrain-rmse:737.94107\teval-rmse:804.18191\n",
      "[734]\ttrain-rmse:737.49070\teval-rmse:803.72592\n",
      "[735]\ttrain-rmse:737.15379\teval-rmse:803.51524\n",
      "[736]\ttrain-rmse:736.58340\teval-rmse:802.99983\n",
      "[737]\ttrain-rmse:735.92589\teval-rmse:802.40283\n",
      "[738]\ttrain-rmse:735.29787\teval-rmse:801.83810\n",
      "[739]\ttrain-rmse:734.67243\teval-rmse:801.26865\n",
      "[740]\ttrain-rmse:734.28534\teval-rmse:800.98612\n",
      "[741]\ttrain-rmse:733.98176\teval-rmse:800.72101\n",
      "[742]\ttrain-rmse:733.80350\teval-rmse:800.63633\n",
      "[743]\ttrain-rmse:733.58703\teval-rmse:800.41982\n",
      "[744]\ttrain-rmse:733.38294\teval-rmse:800.24393\n",
      "[745]\ttrain-rmse:733.23784\teval-rmse:800.16561\n",
      "[746]\ttrain-rmse:732.65321\teval-rmse:799.64178\n",
      "[747]\ttrain-rmse:731.93393\teval-rmse:798.99803\n",
      "[748]\ttrain-rmse:731.62681\teval-rmse:798.71819\n",
      "[749]\ttrain-rmse:730.97618\teval-rmse:798.17957\n",
      "[750]\ttrain-rmse:730.39154\teval-rmse:797.64895\n",
      "[751]\ttrain-rmse:730.19740\teval-rmse:797.49977\n",
      "[752]\ttrain-rmse:729.94117\teval-rmse:797.30504\n",
      "[753]\ttrain-rmse:729.63968\teval-rmse:797.11040\n",
      "[754]\ttrain-rmse:729.30442\teval-rmse:796.81524\n",
      "[755]\ttrain-rmse:728.82361\teval-rmse:796.40762\n",
      "[756]\ttrain-rmse:728.59594\teval-rmse:796.24287\n",
      "[757]\ttrain-rmse:728.26566\teval-rmse:796.02709\n",
      "[758]\ttrain-rmse:727.76234\teval-rmse:795.54924\n",
      "[759]\ttrain-rmse:727.27088\teval-rmse:795.16089\n",
      "[760]\ttrain-rmse:727.04426\teval-rmse:795.01683\n",
      "[761]\ttrain-rmse:726.69316\teval-rmse:794.68789\n",
      "[762]\ttrain-rmse:726.34418\teval-rmse:794.41557\n",
      "[763]\ttrain-rmse:725.99391\teval-rmse:794.10410\n",
      "[764]\ttrain-rmse:725.77943\teval-rmse:793.95713\n",
      "[765]\ttrain-rmse:725.43258\teval-rmse:793.64913\n",
      "[766]\ttrain-rmse:725.04155\teval-rmse:793.31913\n",
      "[767]\ttrain-rmse:724.60028\teval-rmse:793.00882\n",
      "[768]\ttrain-rmse:724.30847\teval-rmse:792.80765\n",
      "[769]\ttrain-rmse:723.88618\teval-rmse:792.37975\n",
      "[770]\ttrain-rmse:723.65971\teval-rmse:792.24178\n",
      "[771]\ttrain-rmse:723.36501\teval-rmse:791.98791\n",
      "[772]\ttrain-rmse:722.97193\teval-rmse:791.66000\n",
      "[773]\ttrain-rmse:722.49941\teval-rmse:791.26431\n",
      "[774]\ttrain-rmse:722.12787\teval-rmse:791.00367\n",
      "[775]\ttrain-rmse:721.91558\teval-rmse:790.85706\n",
      "[776]\ttrain-rmse:721.60160\teval-rmse:790.60672\n",
      "[777]\ttrain-rmse:721.19028\teval-rmse:790.26368\n",
      "[778]\ttrain-rmse:720.62329\teval-rmse:789.76956\n",
      "[779]\ttrain-rmse:720.25386\teval-rmse:789.42578\n",
      "[780]\ttrain-rmse:720.00352\teval-rmse:789.20634\n",
      "[781]\ttrain-rmse:719.87700\teval-rmse:789.12045\n",
      "[782]\ttrain-rmse:719.48691\teval-rmse:788.77505\n",
      "[783]\ttrain-rmse:719.01751\teval-rmse:788.33521\n",
      "[784]\ttrain-rmse:718.76003\teval-rmse:788.13598\n",
      "[785]\ttrain-rmse:718.58681\teval-rmse:788.01969\n",
      "[786]\ttrain-rmse:718.21506\teval-rmse:787.70023\n",
      "[787]\ttrain-rmse:717.98236\teval-rmse:787.51743\n",
      "[788]\ttrain-rmse:717.60344\teval-rmse:787.21388\n",
      "[789]\ttrain-rmse:717.27180\teval-rmse:787.01456\n",
      "[790]\ttrain-rmse:716.85163\teval-rmse:786.63481\n",
      "[791]\ttrain-rmse:716.57154\teval-rmse:786.39426\n",
      "[792]\ttrain-rmse:716.08397\teval-rmse:786.02581\n",
      "[793]\ttrain-rmse:715.88790\teval-rmse:785.89994\n",
      "[794]\ttrain-rmse:715.54377\teval-rmse:785.66002\n",
      "[795]\ttrain-rmse:715.05004\teval-rmse:785.22360\n",
      "[796]\ttrain-rmse:714.82292\teval-rmse:785.09657\n",
      "[797]\ttrain-rmse:714.49865\teval-rmse:784.82868\n",
      "[798]\ttrain-rmse:714.12783\teval-rmse:784.52268\n",
      "[799]\ttrain-rmse:713.79279\teval-rmse:784.19971\n",
      "[800]\ttrain-rmse:713.49334\teval-rmse:783.91564\n",
      "[801]\ttrain-rmse:713.24771\teval-rmse:783.72205\n",
      "[802]\ttrain-rmse:712.80546\teval-rmse:783.33483\n",
      "[803]\ttrain-rmse:712.64483\teval-rmse:783.22677\n",
      "[804]\ttrain-rmse:712.33964\teval-rmse:782.98922\n",
      "[805]\ttrain-rmse:712.07314\teval-rmse:782.78514\n",
      "[806]\ttrain-rmse:711.61977\teval-rmse:782.38787\n",
      "[807]\ttrain-rmse:711.48209\teval-rmse:782.29488\n",
      "[808]\ttrain-rmse:711.31271\teval-rmse:782.17877\n",
      "[809]\ttrain-rmse:710.94156\teval-rmse:781.92046\n",
      "[810]\ttrain-rmse:710.36859\teval-rmse:781.43499\n",
      "[811]\ttrain-rmse:710.08632\teval-rmse:781.25581\n",
      "[812]\ttrain-rmse:709.65853\teval-rmse:780.86477\n",
      "[813]\ttrain-rmse:709.40232\teval-rmse:780.65205\n",
      "[814]\ttrain-rmse:709.11440\teval-rmse:780.43152\n",
      "[815]\ttrain-rmse:708.90847\teval-rmse:780.25998\n",
      "[816]\ttrain-rmse:708.55054\teval-rmse:779.96302\n",
      "[817]\ttrain-rmse:708.22565\teval-rmse:779.67810\n",
      "[818]\ttrain-rmse:707.98298\teval-rmse:779.46924\n",
      "[819]\ttrain-rmse:707.58989\teval-rmse:779.16189\n",
      "[820]\ttrain-rmse:707.28575\teval-rmse:778.91624\n",
      "[821]\ttrain-rmse:707.12201\teval-rmse:778.81599\n",
      "[822]\ttrain-rmse:706.79998\teval-rmse:778.57255\n",
      "[823]\ttrain-rmse:706.61804\teval-rmse:778.42932\n",
      "[824]\ttrain-rmse:706.10155\teval-rmse:777.99345\n",
      "[825]\ttrain-rmse:705.85648\teval-rmse:777.77587\n",
      "[826]\ttrain-rmse:705.73507\teval-rmse:777.70075\n",
      "[827]\ttrain-rmse:705.07123\teval-rmse:777.10525\n",
      "[828]\ttrain-rmse:704.87451\teval-rmse:776.97120\n",
      "[829]\ttrain-rmse:704.72049\teval-rmse:776.84946\n",
      "[830]\ttrain-rmse:704.56281\teval-rmse:776.71618\n",
      "[831]\ttrain-rmse:704.37459\teval-rmse:776.57315\n",
      "[832]\ttrain-rmse:704.31643\teval-rmse:776.53343\n",
      "[833]\ttrain-rmse:704.04770\teval-rmse:776.35204\n",
      "[834]\ttrain-rmse:703.65045\teval-rmse:776.02581\n",
      "[835]\ttrain-rmse:703.45123\teval-rmse:775.88073\n",
      "[836]\ttrain-rmse:703.29547\teval-rmse:775.77910\n",
      "[837]\ttrain-rmse:703.08436\teval-rmse:775.62233\n",
      "[838]\ttrain-rmse:702.82253\teval-rmse:775.45513\n",
      "[839]\ttrain-rmse:702.59557\teval-rmse:775.26187\n",
      "[840]\ttrain-rmse:702.29328\teval-rmse:775.01440\n",
      "[841]\ttrain-rmse:701.87816\teval-rmse:774.71010\n",
      "[842]\ttrain-rmse:701.47273\teval-rmse:774.34664\n",
      "[843]\ttrain-rmse:701.25724\teval-rmse:774.17953\n",
      "[844]\ttrain-rmse:700.93727\teval-rmse:773.93697\n",
      "[845]\ttrain-rmse:700.60050\teval-rmse:773.68350\n",
      "[846]\ttrain-rmse:700.06202\teval-rmse:773.17149\n",
      "[847]\ttrain-rmse:699.82002\teval-rmse:773.04647\n",
      "[848]\ttrain-rmse:699.69764\teval-rmse:772.97966\n",
      "[849]\ttrain-rmse:699.41808\teval-rmse:772.73802\n",
      "[850]\ttrain-rmse:699.05374\teval-rmse:772.47344\n",
      "[851]\ttrain-rmse:698.65709\teval-rmse:772.08809\n",
      "[852]\ttrain-rmse:698.15855\teval-rmse:771.66112\n",
      "[853]\ttrain-rmse:697.90155\teval-rmse:771.45963\n",
      "[854]\ttrain-rmse:697.43505\teval-rmse:771.06028\n",
      "[855]\ttrain-rmse:697.29222\teval-rmse:770.98087\n",
      "[856]\ttrain-rmse:697.11178\teval-rmse:770.85406\n",
      "[857]\ttrain-rmse:696.79650\teval-rmse:770.68030\n",
      "[858]\ttrain-rmse:696.56338\teval-rmse:770.54213\n",
      "[859]\ttrain-rmse:695.99183\teval-rmse:770.03011\n",
      "[860]\ttrain-rmse:695.72600\teval-rmse:769.86415\n",
      "[861]\ttrain-rmse:695.45443\teval-rmse:769.72063\n",
      "[862]\ttrain-rmse:695.27899\teval-rmse:769.57864\n",
      "[863]\ttrain-rmse:695.00199\teval-rmse:769.37969\n",
      "[864]\ttrain-rmse:694.71810\teval-rmse:769.16148\n",
      "[865]\ttrain-rmse:694.06583\teval-rmse:768.61340\n",
      "[866]\ttrain-rmse:693.75674\teval-rmse:768.39757\n",
      "[867]\ttrain-rmse:693.52333\teval-rmse:768.18347\n",
      "[868]\ttrain-rmse:692.95504\teval-rmse:767.68000\n",
      "[869]\ttrain-rmse:692.85607\teval-rmse:767.64155\n",
      "[870]\ttrain-rmse:692.57015\teval-rmse:767.47613\n",
      "[871]\ttrain-rmse:692.03694\teval-rmse:766.98100\n",
      "[872]\ttrain-rmse:691.84634\teval-rmse:766.83800\n",
      "[873]\ttrain-rmse:691.69408\teval-rmse:766.71338\n",
      "[874]\ttrain-rmse:691.38563\teval-rmse:766.43244\n",
      "[875]\ttrain-rmse:690.93974\teval-rmse:766.04010\n",
      "[876]\ttrain-rmse:690.77586\teval-rmse:765.92230\n",
      "[877]\ttrain-rmse:690.31580\teval-rmse:765.49087\n",
      "[878]\ttrain-rmse:689.96437\teval-rmse:765.21117\n",
      "[879]\ttrain-rmse:689.74093\teval-rmse:765.05203\n",
      "[880]\ttrain-rmse:689.50112\teval-rmse:764.86123\n",
      "[881]\ttrain-rmse:689.06815\teval-rmse:764.49463\n",
      "[882]\ttrain-rmse:688.95283\teval-rmse:764.42601\n",
      "[883]\ttrain-rmse:688.62619\teval-rmse:764.18232\n",
      "[884]\ttrain-rmse:688.32121\teval-rmse:763.93509\n",
      "[885]\ttrain-rmse:688.07154\teval-rmse:763.74775\n",
      "[886]\ttrain-rmse:687.91171\teval-rmse:763.64230\n",
      "[887]\ttrain-rmse:687.69611\teval-rmse:763.52710\n",
      "[888]\ttrain-rmse:687.40889\teval-rmse:763.34897\n",
      "[889]\ttrain-rmse:687.18144\teval-rmse:763.16893\n",
      "[890]\ttrain-rmse:686.99827\teval-rmse:763.06404\n",
      "[891]\ttrain-rmse:686.86190\teval-rmse:762.98824\n",
      "[892]\ttrain-rmse:686.55915\teval-rmse:762.75442\n",
      "[893]\ttrain-rmse:686.14904\teval-rmse:762.40593\n",
      "[894]\ttrain-rmse:685.79987\teval-rmse:762.16250\n",
      "[895]\ttrain-rmse:685.68223\teval-rmse:762.12939\n",
      "[896]\ttrain-rmse:685.49153\teval-rmse:761.98339\n",
      "[897]\ttrain-rmse:685.25855\teval-rmse:761.82094\n",
      "[898]\ttrain-rmse:685.01532\teval-rmse:761.68717\n",
      "[899]\ttrain-rmse:684.89213\teval-rmse:761.59478\n",
      "[900]\ttrain-rmse:684.69755\teval-rmse:761.47559\n",
      "[901]\ttrain-rmse:684.38157\teval-rmse:761.21932\n",
      "[902]\ttrain-rmse:683.99521\teval-rmse:760.97483\n",
      "[903]\ttrain-rmse:683.59695\teval-rmse:760.62919\n",
      "[904]\ttrain-rmse:683.30812\teval-rmse:760.39398\n",
      "[905]\ttrain-rmse:682.92245\teval-rmse:760.12159\n",
      "[906]\ttrain-rmse:682.42227\teval-rmse:759.70285\n",
      "[907]\ttrain-rmse:682.29538\teval-rmse:759.63089\n",
      "[908]\ttrain-rmse:682.12797\teval-rmse:759.50893\n",
      "[909]\ttrain-rmse:681.86631\teval-rmse:759.35479\n",
      "[910]\ttrain-rmse:681.68503\teval-rmse:759.23135\n",
      "[911]\ttrain-rmse:681.27529\teval-rmse:758.93024\n",
      "[912]\ttrain-rmse:680.91651\teval-rmse:758.64492\n",
      "[913]\ttrain-rmse:680.63697\teval-rmse:758.38449\n",
      "[914]\ttrain-rmse:680.45878\teval-rmse:758.29279\n",
      "[915]\ttrain-rmse:680.28817\teval-rmse:758.17174\n",
      "[916]\ttrain-rmse:680.12047\teval-rmse:758.06535\n",
      "[917]\ttrain-rmse:680.01717\teval-rmse:758.01176\n",
      "[918]\ttrain-rmse:679.90415\teval-rmse:757.92829\n",
      "[919]\ttrain-rmse:679.50960\teval-rmse:757.61122\n",
      "[920]\ttrain-rmse:679.33072\teval-rmse:757.49375\n",
      "[921]\ttrain-rmse:679.22863\teval-rmse:757.42448\n",
      "[922]\ttrain-rmse:679.10252\teval-rmse:757.35295\n",
      "[923]\ttrain-rmse:678.92893\teval-rmse:757.20846\n",
      "[924]\ttrain-rmse:678.65692\teval-rmse:756.99164\n",
      "[925]\ttrain-rmse:678.27383\teval-rmse:756.64011\n",
      "[926]\ttrain-rmse:677.94883\teval-rmse:756.38169\n",
      "[927]\ttrain-rmse:677.72200\teval-rmse:756.21725\n",
      "[928]\ttrain-rmse:677.49754\teval-rmse:756.01195\n",
      "[929]\ttrain-rmse:677.18441\teval-rmse:755.79160\n",
      "[930]\ttrain-rmse:676.68270\teval-rmse:755.29056\n",
      "[931]\ttrain-rmse:676.48870\teval-rmse:755.17173\n",
      "[932]\ttrain-rmse:676.35996\teval-rmse:755.07943\n",
      "[933]\ttrain-rmse:676.08787\teval-rmse:754.85742\n",
      "[934]\ttrain-rmse:675.83042\teval-rmse:754.67288\n",
      "[935]\ttrain-rmse:675.57116\teval-rmse:754.49066\n",
      "[936]\ttrain-rmse:675.26873\teval-rmse:754.18665\n",
      "[937]\ttrain-rmse:675.02483\teval-rmse:753.96055\n",
      "[938]\ttrain-rmse:674.59956\teval-rmse:753.62963\n",
      "[939]\ttrain-rmse:674.48676\teval-rmse:753.56429\n",
      "[940]\ttrain-rmse:674.13858\teval-rmse:753.27700\n",
      "[941]\ttrain-rmse:673.62121\teval-rmse:752.84668\n",
      "[942]\ttrain-rmse:673.22929\teval-rmse:752.51004\n",
      "[943]\ttrain-rmse:672.90379\teval-rmse:752.26008\n",
      "[944]\ttrain-rmse:672.68840\teval-rmse:752.12037\n",
      "[945]\ttrain-rmse:672.37322\teval-rmse:751.93335\n",
      "[946]\ttrain-rmse:672.12034\teval-rmse:751.78136\n",
      "[947]\ttrain-rmse:671.83035\teval-rmse:751.60157\n",
      "[948]\ttrain-rmse:671.61995\teval-rmse:751.47554\n",
      "[949]\ttrain-rmse:671.41569\teval-rmse:751.31168\n",
      "[950]\ttrain-rmse:671.20415\teval-rmse:751.12854\n",
      "[951]\ttrain-rmse:671.02325\teval-rmse:751.00866\n",
      "[952]\ttrain-rmse:670.81664\teval-rmse:750.88627\n",
      "[953]\ttrain-rmse:670.63994\teval-rmse:750.75798\n",
      "[954]\ttrain-rmse:670.35205\teval-rmse:750.52770\n",
      "[955]\ttrain-rmse:669.92512\teval-rmse:750.19910\n",
      "[956]\ttrain-rmse:669.73455\teval-rmse:750.10378\n",
      "[957]\ttrain-rmse:669.66326\teval-rmse:750.05878\n",
      "[958]\ttrain-rmse:669.50809\teval-rmse:749.92989\n",
      "[959]\ttrain-rmse:669.26173\teval-rmse:749.74359\n",
      "[960]\ttrain-rmse:669.12982\teval-rmse:749.64851\n",
      "[961]\ttrain-rmse:669.03680\teval-rmse:749.60266\n",
      "[962]\ttrain-rmse:668.55358\teval-rmse:749.16786\n",
      "[963]\ttrain-rmse:668.25111\teval-rmse:748.94406\n",
      "[964]\ttrain-rmse:668.08529\teval-rmse:748.85096\n",
      "[965]\ttrain-rmse:667.87844\teval-rmse:748.71619\n",
      "[966]\ttrain-rmse:667.59376\teval-rmse:748.49105\n",
      "[967]\ttrain-rmse:667.21706\teval-rmse:748.16829\n",
      "[968]\ttrain-rmse:667.09542\teval-rmse:748.08671\n",
      "[969]\ttrain-rmse:666.91208\teval-rmse:747.95434\n",
      "[970]\ttrain-rmse:666.76697\teval-rmse:747.86342\n",
      "[971]\ttrain-rmse:666.37909\teval-rmse:747.56824\n",
      "[972]\ttrain-rmse:666.06272\teval-rmse:747.29726\n",
      "[973]\ttrain-rmse:665.59441\teval-rmse:746.85927\n",
      "[974]\ttrain-rmse:665.23055\teval-rmse:746.56019\n",
      "[975]\ttrain-rmse:664.94178\teval-rmse:746.40914\n",
      "[976]\ttrain-rmse:664.70763\teval-rmse:746.27891\n",
      "[977]\ttrain-rmse:664.44090\teval-rmse:746.08024\n",
      "[978]\ttrain-rmse:664.14519\teval-rmse:745.84030\n",
      "[979]\ttrain-rmse:663.82694\teval-rmse:745.57948\n",
      "[980]\ttrain-rmse:663.67992\teval-rmse:745.47933\n",
      "[981]\ttrain-rmse:663.53159\teval-rmse:745.36825\n",
      "[982]\ttrain-rmse:663.30015\teval-rmse:745.19671\n",
      "[983]\ttrain-rmse:663.07539\teval-rmse:745.03551\n",
      "[984]\ttrain-rmse:662.69071\teval-rmse:744.69532\n",
      "[985]\ttrain-rmse:662.26187\teval-rmse:744.36289\n",
      "[986]\ttrain-rmse:662.04207\teval-rmse:744.22682\n",
      "[987]\ttrain-rmse:661.69647\teval-rmse:743.92587\n",
      "[988]\ttrain-rmse:661.59504\teval-rmse:743.87150\n",
      "[989]\ttrain-rmse:661.09293\teval-rmse:743.43551\n",
      "[990]\ttrain-rmse:660.92774\teval-rmse:743.31634\n",
      "[991]\ttrain-rmse:660.79939\teval-rmse:743.24674\n",
      "[992]\ttrain-rmse:660.50335\teval-rmse:743.00026\n",
      "[993]\ttrain-rmse:660.32508\teval-rmse:742.91983\n",
      "[994]\ttrain-rmse:660.10100\teval-rmse:742.76097\n",
      "[995]\ttrain-rmse:659.96151\teval-rmse:742.65032\n",
      "[996]\ttrain-rmse:659.44455\teval-rmse:742.16116\n",
      "[997]\ttrain-rmse:659.05717\teval-rmse:741.81138\n",
      "[998]\ttrain-rmse:658.91416\teval-rmse:741.71493\n",
      "[999]\ttrain-rmse:658.68984\teval-rmse:741.58506\n",
      "[6084.5874    4809.613     3605.294     ... 7596.7144    8439.332\n",
      "  -10.6845255]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of continuous and multiclass targets",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_21472\\2049000583.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mFROM_SCRATCH\u001B[0m \u001B[1;32mor\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0misfile\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'modelXGBR.json'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m     \u001B[0mmodelXGBR\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmake_xgb_modelR\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m     \u001B[0mmodelXGBR\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msave_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'modelXGBR.json'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_21472\\3753505068.py\u001B[0m in \u001B[0;36mmake_xgb_modelR\u001B[1;34m()\u001B[0m\n\u001B[0;32m     26\u001B[0m     \u001B[0my_predR\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mxgb\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDMatrix\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX_testR\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     27\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_predR\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 28\u001B[1;33m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'accuracy_score:'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maccuracy_score\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_predR\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_test_xgb\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     29\u001B[0m     \u001B[1;31m# print('f1_score:', f1_score(y_predR, y_test_xgb))\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001B[0m in \u001B[0;36maccuracy_score\u001B[1;34m(y_true, y_pred, normalize, sample_weight)\u001B[0m\n\u001B[0;32m    209\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    210\u001B[0m     \u001B[1;31m# Compute accuracy for each possible representation\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 211\u001B[1;33m     \u001B[0my_type\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_true\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_pred\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_check_targets\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_true\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_pred\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    212\u001B[0m     \u001B[0mcheck_consistent_length\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_true\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_pred\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msample_weight\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    213\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0my_type\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstartswith\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"multilabel\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001B[0m in \u001B[0;36m_check_targets\u001B[1;34m(y_true, y_pred)\u001B[0m\n\u001B[0;32m     91\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     92\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_type\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 93\u001B[1;33m         raise ValueError(\n\u001B[0m\u001B[0;32m     94\u001B[0m             \"Classification metrics can't handle a mix of {0} and {1} targets\".format(\n\u001B[0;32m     95\u001B[0m                 \u001B[0mtype_true\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtype_pred\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: Classification metrics can't handle a mix of continuous and multiclass targets"
     ]
    }
   ],
   "source": [
    "# Modell wird schon passend gespeichert @Bene :)\n",
    "import os.path\n",
    "\n",
    "if FROM_SCRATCH or not os.path.isfile('modelXGBR.json'):\n",
    "    modelXGBR = make_xgb_modelR()\n",
    "    modelXGBR.save_model('modelXGBR.json')\n",
    "else:\n",
    "    modelXGBR = xgb.Booster()\n",
    "    modelXGBR.load_model('modelXGBR.json')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#modelXGBR = make_xgb_modelR()\n",
    "#modelXGBR.save_model(\"modelXGBR.json\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.0300e+02 5.0000e+00 0.0000e+00 ... 7.7000e+01 2.1750e+01 0.0000e+00]\n",
      " [7.2800e+02 3.0000e+00 1.0000e+00 ... 2.4165e+04 3.0250e+01 0.0000e+00]\n",
      " [3.0000e+01 1.0000e+00 0.0000e+00 ... 1.3000e+01 1.2750e+01 1.0000e+00]\n",
      " ...\n",
      " [2.2800e+02 5.0000e+00 0.0000e+00 ... 2.4174e+04 3.7500e+01 0.0000e+00]\n",
      " [8.6600e+02 1.0000e+00 1.0000e+00 ... 2.4167e+04 1.0500e+01 1.0000e+00]\n",
      " [4.6000e+01 7.0000e+00 0.0000e+00 ... 1.0000e+02 3.2750e+01 1.0000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(X_testR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6331 4247 3536 ... 7728 8646    0]\n",
      "[5728.75     6066.821    4752.0376   ... 5808.47     7078.5015\n",
      "   31.974087]\n",
      "RMSE: 1829.1555\n",
      "RMSPE: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christian Karg\\AppData\\Local\\Temp\\ipykernel_21472\\3285679698.py:7: RuntimeWarning: overflow encountered in expm1\n",
      "  error = rmspe(np.expm1(y_test_xgb), np.expm1(y_pred))\n",
      "C:\\Users\\Christian Karg\\AppData\\Local\\Temp\\ipykernel_21472\\2248524130.py:5: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return np.sqrt(np.mean((yhat/y-1) ** 2))\n",
      "C:\\Users\\Christian Karg\\AppData\\Local\\Temp\\ipykernel_21472\\2248524130.py:5: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.sqrt(np.mean((yhat/y-1) ** 2))\n"
     ]
    }
   ],
   "source": [
    "#y_pred = np.rint(modelXGBR.predict(xgb.DMatrix(X_testR)))\n",
    "y_pred = modelXGBR.predict(xgb.DMatrix(X_testR))\n",
    "print(y_test_xgb)\n",
    "print(y_pred)\n",
    "error = rmse(y_test_xgb, y_pred)\n",
    "print('RMSE: {:.4f}'.format(error))\n",
    "error = rmspe(np.expm1(y_test_xgb), np.expm1(y_pred))\n",
    "print('RMSPE: {:.4f}'.format(error))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load/Make models\n",
    "We save and load the same models each time to ensure stable results. If they don’t exist we create new ones. If you\n",
    "want to generate new models on each notebook run, then set FROM_SCRATCH=True."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nimport shap\\n\\ndtrain = xgb.DMatrix(X_trainR, y_train_xgb)\\nmodelXGBR.set_param({\"predictor\": \"gpu_predictor\"})\\nshap_values = modelXGBR.predict(dtrain, pred_contribs=True)\\nshap_interaction_values = modelXGBR.predict(dtrain, pred_interactions=True)\\n\\nmodelXGBR.set_param({\"predictor\": \"gpu_predictor\"})\\nexplainer = shap.TreeExplainer(modelXGBR)\\nshap_values = explainer.shap_values(X_trainR)\\nshap.summary_plot(shap_values, X_trainR,max_display= 10, title = \\'SHAP\\', plot_type= \\'bar\\')\\n'"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import shap\n",
    "\n",
    "dtrain = xgb.DMatrix(X_trainR, y_train_xgb)\n",
    "modelXGBR.set_param({\"predictor\": \"gpu_predictor\"})\n",
    "shap_values = modelXGBR.predict(dtrain, pred_contribs=True)\n",
    "shap_interaction_values = modelXGBR.predict(dtrain, pred_interactions=True)\n",
    "\n",
    "modelXGBR.set_param({\"predictor\": \"gpu_predictor\"})\n",
    "explainer = shap.TreeExplainer(modelXGBR)\n",
    "shap_values = explainer.shap_values(X_trainR)\n",
    "shap.summary_plot(shap_values, X_trainR,max_display= 10, title = 'SHAP', plot_type= 'bar')\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "9.1.3 Util functions\n",
    "These are utility functions for exploring results. The first shows two instances of the data side by side and compares\n",
    "the difference. We’ll use this to see how the counterfactuals differ from their original instances. The second function\n",
    "plots the importance of each feature. This will be useful for visualizing the attribution methods"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "\"\\ndef compare_instances(x, cf):\\n\\n    #Show the difference in values between two instances.\\n\\n    x = x.astype('float64')\\n    cf = cf.astype('float64')\\n    for f, v1, v2 in zip(features, x[0], cf[0]):\\n        print(f'{f:<25} instance: {round(v1, 3):^10} counter factual: {round(v2, 3):^10} difference: {round(v2, 7):^5}')\\n\\ndef plot_importance(feat_imp, feat_names, class_idx, **kwargs):\\n\\n    #Create a horizontal barchart of feature effects, sorted by their magnitude.\\n\\n    df = pd.DataFrame(data=feat_imp, columns=feat_names).sort_values(by=0, axis='columns')\\n    feat_imp, feat_names = df.values[0], df.columns\\n    fig, ax = plt.subplots(figsize=(10, 5))\\n    y_pos = np.arange(len(feat_imp))\\n    ax.barh(y_pos, feat_imp)\\n    ax.set_yticks(y_pos)\\n    ax.set_yticklabels(feat_names, fontsize=15)\\n    ax.invert_yaxis()\\n    ax.set_xlabel(f'Feature effects for class {class_idx}', fontsize=15)\\n    return ax, fig\\n\""
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def compare_instances(x, cf):\n",
    "\n",
    "    #Show the difference in values between two instances.\n",
    "\n",
    "    x = x.astype('float64')\n",
    "    cf = cf.astype('float64')\n",
    "    for f, v1, v2 in zip(features, x[0], cf[0]):\n",
    "        print(f'{f:<25} instance: {round(v1, 3):^10} counter factual: {round(v2, 3):^10} difference: {round(v2, 7):^5}')\n",
    "\n",
    "def plot_importance(feat_imp, feat_names, class_idx, **kwargs):\n",
    "\n",
    "    #Create a horizontal barchart of feature effects, sorted by their magnitude.\n",
    "\n",
    "    df = pd.DataFrame(data=feat_imp, columns=feat_names).sort_values(by=0, axis='columns')\n",
    "    feat_imp, feat_names = df.values[0], df.columns\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    y_pos = np.arange(len(feat_imp))\n",
    "    ax.barh(y_pos, feat_imp)\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(feat_names, fontsize=15)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xlabel(f'Feature effects for class {class_idx}', fontsize=15)\n",
    "    return ax, fig\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "9.1.5 Local Necessary Features\n",
    "Anchors\n",
    "Anchors tell us what features need to stay the same for a specific instance for the model to give the same classification.\n",
    "In the case of a trained image classification model, an anchor for a given instance would be a minimal subset of the\n",
    "image that the model uses to make its decision.\n",
    "Here we apply Anchors to the tensor flow model trained on the wine-quality dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "'\"\\nfrom alibi.explainers import AnchorTabular\\npredict_fnR = lambda x: modelR.predict(scalerR.transform(x))\\nexplainerR = AnchorTabular(predict_fnR, featuresR, categorical_names=category_map)\\nexplainerR.fit(X_trainR, disc_perc=(25, 50, 75))\\nresultR = explainerR.explain(xR, threshold=0.95)\\n'"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "from alibi.explainers import AnchorTabular\n",
    "predict_fnR = lambda x: modelR.predict(scalerR.transform(x))\n",
    "explainerR = AnchorTabular(predict_fnR, featuresR, categorical_names=category_map)\n",
    "explainerR.fit(X_trainR, disc_perc=(25, 50, 75))\n",
    "resultR = explainerR.explain(xR, threshold=0.95)\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "category_map = {1: [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"], 2:[\"PromoNo\", \"PromoYes\"], 3: [\"NoStateHoliday\", \"PublicHoliday\", \"EasterHoliday\", \"ChristmasHoliday\"],\n",
    "                4:[\"SchoolHolidayNo\", \"SchoolHolidayYes\"], 5: [\"StoreTypeA\", \"StoreTypeB\", \"StoreTypeC\", \"StoreTypeD\"], 6:[ \"?\", \"Basic\", \"Extra\", \"Extended\"], 8:[\"None\",\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"], 10: [\"NoPromo2\", \"Promo2\"], 14: [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"], 19: [\"NoPromoMonth\", \"PromoMonth\"] }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "category_map = { 1: [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"], 2:[\"PromoNo\", \"PromoYes\"], 3: [\"NoStateHoliday\", \"PublicHoliday\", \"EasterHoliday\", \"ChristmasHoliday\"],\n",
    "                4:[\"SchoolHolidayNo\", \"SchoolHolidayYes\"], 5: [\"?\", \"StoreTypeA\", \"StoreTypeB\", \"StoreTypeC\", \"StoreTypeD\"], 6:[ \"?\", \"Basic\", \"Extra\", \"Extended\"], 8:[\"None\",\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"], 10: [\"NoPromo2\", \"Promo2\"], 14: [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"], 19: [\"NoPromoMonth\", \"PromoMonth\"]}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[8.320000e+02, 5.000000e+00, 1.000000e+00, ..., 2.416800e+04,\n        4.900000e+01, 0.000000e+00],\n       [4.070000e+02, 1.000000e+00, 0.000000e+00, ..., 1.180000e+02,\n        2.900000e+01, 1.000000e+00],\n       [3.980000e+02, 7.000000e+00, 0.000000e+00, ..., 2.417300e+04,\n        2.875000e+01, 0.000000e+00],\n       ...,\n       [5.850000e+02, 5.000000e+00, 1.000000e+00, ..., 2.000000e+00,\n        2.417375e+04, 0.000000e+00],\n       [7.420000e+02, 6.000000e+00, 0.000000e+00, ..., 2.417800e+04,\n        2.417850e+04, 0.000000e+00],\n       [1.043000e+03, 2.000000e+00, 0.000000e+00, ..., 8.200000e+01,\n        2.415725e+04, 0.000000e+00]], dtype=float32)"
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trainR"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "index = np.searchsorted(store_quan[0][0], 4892)\n",
    "print(index)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [],
   "source": [
    "def bin(input_array, data):\n",
    "    output_array = np.zeros(input_array.shape)\n",
    "    for i in range(len(input_array)):\n",
    "        Id = data[i][0]\n",
    "        Day = data[i][1]\n",
    "        print(Day)\n",
    "        print(Id)\n",
    "        print(input_array[i])\n",
    "        print(store_quan[np.rint(Id)][np.rint(Day)])\n",
    "        output_array[i] = np.searchsorted(store_quan[np.rint(Id)][np.rint(Day)], input_array[i])\n",
    "    return output_array"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "4892.5444\n"
     ]
    },
    {
     "ename": "PredictorCallError",
     "evalue": "Predictor failed to be called on <class 'numpy.ndarray'> of shape (1, 20) and dtype float32. Check that the parameter `feature_names` is correctly specified.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\alibi\\explainers\\anchors\\anchor_tabular.py\u001B[0m in \u001B[0;36m_transform_predictor\u001B[1;34m(self, predictor)\u001B[0m\n\u001B[0;32m   1006\u001B[0m             \u001B[1;31m# if needed adjust predictor so it returns the predicted class\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1007\u001B[1;33m             \u001B[0mprediction\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpredictor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1008\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_21472\\3309464326.py\u001B[0m in \u001B[0;36m<lambda>\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0malibi\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexplainers\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mAnchorTabular\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mpredict_xgb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mbin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodelXGBR\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mxgb\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDMatrix\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0mexplainerXGB\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mAnchorTabular\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpredict_xgb\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfeaturesR\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcategorical_names\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcategory_map\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_21472\\212018249.py\u001B[0m in \u001B[0;36mbin\u001B[1;34m(input_array, data)\u001B[0m\n\u001B[0;32m      8\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput_array\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 9\u001B[1;33m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstore_quan\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mId\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mDay\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     10\u001B[0m         \u001B[0moutput_array\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msearchsorted\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstore_quan\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mId\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mDay\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput_array\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mIndexError\u001B[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mPredictorCallError\u001B[0m                        Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_21472\\3309464326.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0malibi\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexplainers\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mAnchorTabular\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[0mpredict_xgb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mbin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodelXGBR\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mxgb\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDMatrix\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[0mexplainerXGB\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mAnchorTabular\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpredict_xgb\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfeaturesR\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcategorical_names\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcategory_map\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m \u001B[0mexplainerXGB\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX_trainR\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdisc_perc\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m25\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m50\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m75\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mresultXGB\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mexplainerXGB\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexplain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mxR\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mthreshold\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.95\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\alibi\\explainers\\anchors\\anchor_tabular.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, predictor, feature_names, categorical_names, dtype, ohe, seed)\u001B[0m\n\u001B[0;32m    640\u001B[0m         \u001B[1;31m# defines self._predictor which expect label categorical features, and if ohe == True,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    641\u001B[0m         \u001B[1;31m# it defines self._ohe_predictor which expects one-hot encoded categorical features\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 642\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpredictor\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpredictor\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    643\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    644\u001B[0m         \u001B[1;31m# define column indices of categorical and numerical (aka continuous) features\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\alibi\\explainers\\anchors\\anchor_tabular.py\u001B[0m in \u001B[0;36mpredictor\u001B[1;34m(self, predictor)\u001B[0m\n\u001B[0;32m    996\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    997\u001B[0m                 \u001B[1;31m# set the predictor\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 998\u001B[1;33m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_predictor\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_transform_predictor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpredictor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    999\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1000\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_transform_predictor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpredictor\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mCallable\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mCallable\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\alibi\\explainers\\anchors\\anchor_tabular.py\u001B[0m in \u001B[0;36m_transform_predictor\u001B[1;34m(self, predictor)\u001B[0m\n\u001B[0;32m   1009\u001B[0m             \u001B[0mmsg\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34mf\"Predictor failed to be called on {type(x)} of shape {x.shape} and dtype {x.dtype}. \"\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1010\u001B[0m                   \u001B[1;34mf\"Check that the parameter `feature_names` is correctly specified.\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1011\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0mPredictorCallError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmsg\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1012\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1013\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mprediction\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mndarray\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mPredictorCallError\u001B[0m: Predictor failed to be called on <class 'numpy.ndarray'> of shape (1, 20) and dtype float32. Check that the parameter `feature_names` is correctly specified."
     ]
    }
   ],
   "source": [
    "from alibi.explainers import AnchorTabular\n",
    "predict_xgb = lambda x: bin(modelXGBR.predict(xgb.DMatrix(x)), x)\n",
    "explainerXGB = AnchorTabular(predict_xgb, featuresR, categorical_names=category_map)\n",
    "explainerXGB.fit(X_trainR, disc_perc=(25, 50, 75))\n",
    "resultXGB = explainerXGB.explain(xR, threshold=0.95)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "print('Anchor =', resultR.data['anchor'])\n",
    "print('Precision = ', resultR.data['precision'])\n",
    "print('Coverage = ', resultR.data['coverage'])\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor = ['Promo = PromoYes', 'PromoOpen > 24171.75', 'Day > 23.00', 'Assortment = Extended', 'SchoolHoliday = SchoolHolidayYes', 'Promo2SinceYear <= 2009.00', 'StateHoliday = NoStateHoliday', 'DayOfWeek = Saturday', 'Month = Aug', 'CompetitionDistance > 6880.00']\n",
      "Precision =  0.9638989169675091\n",
      "Coverage =  9.306520069313913e-05\n"
     ]
    }
   ],
   "source": [
    "print('Anchor =', resultXGB.data['anchor'])\n",
    "print('Precision = ', resultXGB.data['precision'])\n",
    "print('Coverage = ', resultXGB.data['coverage'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "\"\\nidx  = 11\\nprint(explainerR.predictor(X_testR[idx].reshape(1, -1))[0])\\nexplanation = explainerR.explain(X_testR[idx], threshold=0.95)\\nprint('Anchor: %s' % (' AND '.join(explanation.anchor)))\\nprint('Precision: %.2f' % explanation.precision)\\nprint('Coverage: %.2f' % explanation.coverage)\\n\""
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "idx  = 11\n",
    "print(explainerR.predictor(X_testR[idx].reshape(1, -1))[0])\n",
    "explanation = explainerR.explain(X_testR[idx], threshold=0.95)\n",
    "print('Anchor: %s' % (' AND '.join(explanation.anchor)))\n",
    "print('Precision: %.2f' % explanation.precision)\n",
    "print('Coverage: %.2f' % explanation.coverage)\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor = ['Promo = PromoYes', 'PromoOpen > 24171.75', 'Day > 23.00', 'Assortment = Extended', 'SchoolHoliday = SchoolHolidayYes', 'Promo2SinceYear <= 2009.00', 'StateHoliday = NoStateHoliday', 'DayOfWeek = Saturday', 'Month = Aug', 'CompetitionDistance > 6880.00']\n",
      "Precision =  0.9638989169675091\n",
      "Coverage =  9.306520069313913e-05\n"
     ]
    }
   ],
   "source": [
    "print('Anchor =', resultXGB.data['anchor'])\n",
    "print('Precision = ', resultXGB.data['precision'])\n",
    "print('Coverage = ', resultXGB.data['coverage'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "\"\\nidx  = 11\\nprint(explainerR.predictor(X_testR[idx].reshape(1, -1))[0])\\nexplanation = explainerR.explain(X_testR[idx], threshold=0.95)\\nprint('Anchor: %s' % (' AND '.join(explanation.anchor)))\\nprint('Precision: %.2f' % explanation.precision)\\nprint('Coverage: %.2f' % explanation.coverage)\\n\""
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "idx  = 11\n",
    "print(explainerR.predictor(X_testR[idx].reshape(1, -1))[0])\n",
    "explanation = explainerR.explain(X_testR[idx], threshold=0.95)\n",
    "print('Anchor: %s' % (' AND '.join(explanation.anchor)))\n",
    "print('Precision: %.2f' % explanation.precision)\n",
    "print('Coverage: %.2f' % explanation.coverage)\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0\n",
      "Anchor: DayOfWeek = Wednesday AND Promo = PromoNo AND StoreType = StoreTypeD\n",
      "Precision: 0.96\n",
      "Coverage: 0.02\n"
     ]
    }
   ],
   "source": [
    "idx = 11\n",
    "print(explainerXGB.predictor(X_testR[idx].reshape(1, -1))[0])\n",
    "explanation = explainerXGB.explain(X_testR[idx], threshold=0.95)\n",
    "print('Anchor: %s' % (' AND '.join(explanation.anchor)))\n",
    "print('Precision: %.2f' % explanation.precision)\n",
    "print('Coverage: %.2f' % explanation.coverage)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "idx = 11\n",
    "print(explainerXGB.predictor(X_testR[idx].reshape(1, -1))[0])\n",
    "explanation = explainerXGB.explain(X_testR[idx], threshold=0.95)\n",
    "print('Anchor: %s' % (' AND '.join(explanation.anchor)))\n",
    "print('Precision: %.2f' % explanation.precision)\n",
    "print('Coverage: %.2f' % explanation.coverage)\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}