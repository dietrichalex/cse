{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "FROM_SCRATCH = False\n",
    "TF_MODEL_FNAMER = 'tf-clf-sales'\n",
    "RFC_FNAMER = 'rfc-sales'\n",
    "ENC_FNAMER = 'sales_encoderR'\n",
    "DEC_FNAMER = 'sales_decoderR'"
   ],
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "9.1.1 Preparing the data.\n",
    "We’re using the wine-quality dataset, a numeric tabular dataset containing features that refer to the chemical composition of wines and quality ratings. To make this a simple classification task, we bucket all wines with ratings greater\n",
    "than five as good, and the rest we label bad. We also normalize all the features."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christian Karg\\AppData\\Local\\Temp\\ipykernel_19692\\2235936024.py:2: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train = pd.read_csv('input/train.csv',parse_dates=[2])\n"
     ]
    }
   ],
   "source": [
    "store = pd.read_csv('input/store.csv')\n",
    "train = pd.read_csv('input/train.csv',parse_dates=[2])\n",
    "test = pd.read_csv('input/test.csv',parse_dates=[3])\n",
    "# fillna in store with 0 has better result than median()\n",
    "# Aufbereiten der daten\n",
    "store.fillna(0, inplace=True)\n",
    "# fill missing values in test with 1\n",
    "# Aufbereiten der Daten\n",
    "test.fillna(value = 1, inplace = True)\n",
    "# merge data with store\n",
    "# Alles in eine Tabelle\n",
    "train = pd.merge(train, store, on='Store')\n",
    "test = pd.merge(test, store, on='Store')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "   Store  DayOfWeek       Date  Sales  Customers  Open  Promo StateHoliday  \\\n0      1          5 2015-07-31   5263        555     1      1            0   \n1      1          4 2015-07-30   5020        546     1      1            0   \n2      1          3 2015-07-29   4782        523     1      1            0   \n3      1          2 2015-07-28   5011        560     1      1            0   \n4      1          1 2015-07-27   6102        612     1      1            0   \n\n   SchoolHoliday StoreType Assortment  CompetitionDistance  \\\n0              1         c          a               1270.0   \n1              1         c          a               1270.0   \n2              1         c          a               1270.0   \n3              1         c          a               1270.0   \n4              1         c          a               1270.0   \n\n   CompetitionOpenSinceMonth  CompetitionOpenSinceYear  Promo2  \\\n0                        9.0                    2008.0       0   \n1                        9.0                    2008.0       0   \n2                        9.0                    2008.0       0   \n3                        9.0                    2008.0       0   \n4                        9.0                    2008.0       0   \n\n   Promo2SinceWeek  Promo2SinceYear PromoInterval  \n0              0.0              0.0             0  \n1              0.0              0.0             0  \n2              0.0              0.0             0  \n3              0.0              0.0             0  \n4              0.0              0.0             0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Store</th>\n      <th>DayOfWeek</th>\n      <th>Date</th>\n      <th>Sales</th>\n      <th>Customers</th>\n      <th>Open</th>\n      <th>Promo</th>\n      <th>StateHoliday</th>\n      <th>SchoolHoliday</th>\n      <th>StoreType</th>\n      <th>Assortment</th>\n      <th>CompetitionDistance</th>\n      <th>CompetitionOpenSinceMonth</th>\n      <th>CompetitionOpenSinceYear</th>\n      <th>Promo2</th>\n      <th>Promo2SinceWeek</th>\n      <th>Promo2SinceYear</th>\n      <th>PromoInterval</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>5</td>\n      <td>2015-07-31</td>\n      <td>5263</td>\n      <td>555</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>c</td>\n      <td>a</td>\n      <td>1270.0</td>\n      <td>9.0</td>\n      <td>2008.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>4</td>\n      <td>2015-07-30</td>\n      <td>5020</td>\n      <td>546</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>c</td>\n      <td>a</td>\n      <td>1270.0</td>\n      <td>9.0</td>\n      <td>2008.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>3</td>\n      <td>2015-07-29</td>\n      <td>4782</td>\n      <td>523</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>c</td>\n      <td>a</td>\n      <td>1270.0</td>\n      <td>9.0</td>\n      <td>2008.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>2</td>\n      <td>2015-07-28</td>\n      <td>5011</td>\n      <td>560</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>c</td>\n      <td>a</td>\n      <td>1270.0</td>\n      <td>9.0</td>\n      <td>2008.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>1</td>\n      <td>2015-07-27</td>\n      <td>6102</td>\n      <td>612</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>c</td>\n      <td>a</td>\n      <td>1270.0</td>\n      <td>9.0</td>\n      <td>2008.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "     Id  Store  DayOfWeek       Date  Open  Promo StateHoliday  SchoolHoliday  \\\n0     1      1          4 2015-09-17   1.0      1            0              0   \n1   857      1          3 2015-09-16   1.0      1            0              0   \n2  1713      1          2 2015-09-15   1.0      1            0              0   \n3  2569      1          1 2015-09-14   1.0      1            0              0   \n4  3425      1          7 2015-09-13   0.0      0            0              0   \n\n  StoreType Assortment  CompetitionDistance  CompetitionOpenSinceMonth  \\\n0         c          a               1270.0                        9.0   \n1         c          a               1270.0                        9.0   \n2         c          a               1270.0                        9.0   \n3         c          a               1270.0                        9.0   \n4         c          a               1270.0                        9.0   \n\n   CompetitionOpenSinceYear  Promo2  Promo2SinceWeek  Promo2SinceYear  \\\n0                    2008.0       0              0.0              0.0   \n1                    2008.0       0              0.0              0.0   \n2                    2008.0       0              0.0              0.0   \n3                    2008.0       0              0.0              0.0   \n4                    2008.0       0              0.0              0.0   \n\n  PromoInterval  \n0             0  \n1             0  \n2             0  \n3             0  \n4             0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Store</th>\n      <th>DayOfWeek</th>\n      <th>Date</th>\n      <th>Open</th>\n      <th>Promo</th>\n      <th>StateHoliday</th>\n      <th>SchoolHoliday</th>\n      <th>StoreType</th>\n      <th>Assortment</th>\n      <th>CompetitionDistance</th>\n      <th>CompetitionOpenSinceMonth</th>\n      <th>CompetitionOpenSinceYear</th>\n      <th>Promo2</th>\n      <th>Promo2SinceWeek</th>\n      <th>Promo2SinceYear</th>\n      <th>PromoInterval</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>2015-09-17</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>c</td>\n      <td>a</td>\n      <td>1270.0</td>\n      <td>9.0</td>\n      <td>2008.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>857</td>\n      <td>1</td>\n      <td>3</td>\n      <td>2015-09-16</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>c</td>\n      <td>a</td>\n      <td>1270.0</td>\n      <td>9.0</td>\n      <td>2008.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1713</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2015-09-15</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>c</td>\n      <td>a</td>\n      <td>1270.0</td>\n      <td>9.0</td>\n      <td>2008.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2569</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2015-09-14</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>c</td>\n      <td>a</td>\n      <td>1270.0</td>\n      <td>9.0</td>\n      <td>2008.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3425</td>\n      <td>1</td>\n      <td>7</td>\n      <td>2015-09-13</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>c</td>\n      <td>a</td>\n      <td>1270.0</td>\n      <td>9.0</td>\n      <td>2008.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christian Karg\\AppData\\Local\\Temp\\ipykernel_19692\\2713858278.py:15: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated. Please use Series.dt.isocalendar().week instead.\n",
      "  data['WeekOfYear'] = data.Date.dt.weekofyear\n",
      "C:\\Users\\Christian Karg\\AppData\\Local\\Temp\\ipykernel_19692\\2713858278.py:15: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated. Please use Series.dt.isocalendar().week instead.\n",
      "  data['WeekOfYear'] = data.Date.dt.weekofyear\n"
     ]
    }
   ],
   "source": [
    "# process train and test\n",
    "# aufbereiten der daten, neue Spalten und manche werden entfernt\n",
    "def process(data, isTest = False):\n",
    "    # label encode some features\n",
    "    mappings = {'0':0, 'a':1, 'b':2, 'c':3, 'd':4}\n",
    "    # buchstaben zu zahlen\n",
    "    data.StoreType.replace(mappings, inplace=True)\n",
    "    data.Assortment.replace(mappings, inplace=True)\n",
    "    data.StateHoliday.replace(mappings, inplace=True)\n",
    "\n",
    "    # extract some features from date column\n",
    "    data['Month'] = data.Date.dt.month\n",
    "    data['Year'] = data.Date.dt.year\n",
    "    data['Day'] = data.Date.dt.day\n",
    "    data['WeekOfYear'] = data.Date.dt.weekofyear\n",
    "\n",
    "    # calculate competiter open time in months\n",
    "    data['CompetitionOpen'] = 12 * (data.Year - data.CompetitionOpenSinceYear) + \\\n",
    "        (data.Month - data.CompetitionOpenSinceMonth)\n",
    "    data['CompetitionOpen'] = data['CompetitionOpen'].apply(lambda x: x if x > 0 else 0)\n",
    "\n",
    "    # calculate promo2 open time in months\n",
    "    data['PromoOpen'] = 12 * (data.Year - data.Promo2SinceYear) + \\\n",
    "        (data.WeekOfYear - data.Promo2SinceWeek) / 4.0\n",
    "    data['PromoOpen'] = data['PromoOpen'].apply(lambda x: x if x > 0 else 0)\n",
    "\n",
    "    # Indicate whether the month is in promo interval\n",
    "    month2str = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', \\\n",
    "             7:'Jul', 8:'Aug', 9:'Sept', 10:'Oct', 11:'Nov', 12:'Dec'}\n",
    "    data['month_str'] = data.Month.map(month2str)\n",
    "\n",
    "    def check(row):\n",
    "        if isinstance(row['PromoInterval'],str) and row['month_str'] in row['PromoInterval']:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    data['IsPromoMonth'] =  data.apply(lambda row: check(row),axis=1)\n",
    "\n",
    "    # select the features we need\n",
    "    features = ['Store', 'DayOfWeek', 'Promo', 'StateHoliday', 'SchoolHoliday',\n",
    "       'StoreType', 'Assortment', 'CompetitionDistance',\n",
    "       'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2',\n",
    "       'Promo2SinceWeek', 'Promo2SinceYear', 'Year', 'Month', 'Day',\n",
    "       'WeekOfYear', 'CompetitionOpen', 'PromoOpen', 'IsPromoMonth']\n",
    "    if not isTest:\n",
    "        features.append('Sales')\n",
    "\n",
    "    data = data[features]\n",
    "    return data\n",
    "\n",
    "train = train.sort_values(['Date'],ascending = False)\n",
    "train = process(train)\n",
    "test = process(test,isTest = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "   Store  DayOfWeek  Promo  StateHoliday  SchoolHoliday  StoreType  \\\n0      1          4      1             0              0          3   \n1      1          3      1             0              0          3   \n2      1          2      1             0              0          3   \n3      1          1      1             0              0          3   \n4      1          7      0             0              0          3   \n\n   Assortment  CompetitionDistance  CompetitionOpenSinceMonth  \\\n0           1               1270.0                        9.0   \n1           1               1270.0                        9.0   \n2           1               1270.0                        9.0   \n3           1               1270.0                        9.0   \n4           1               1270.0                        9.0   \n\n   CompetitionOpenSinceYear  Promo2  Promo2SinceWeek  Promo2SinceYear  Year  \\\n0                    2008.0       0              0.0              0.0  2015   \n1                    2008.0       0              0.0              0.0  2015   \n2                    2008.0       0              0.0              0.0  2015   \n3                    2008.0       0              0.0              0.0  2015   \n4                    2008.0       0              0.0              0.0  2015   \n\n   Month  Day  WeekOfYear  CompetitionOpen  PromoOpen  IsPromoMonth  \n0      9   17          38             84.0   24189.50             0  \n1      9   16          38             84.0   24189.50             0  \n2      9   15          38             84.0   24189.50             0  \n3      9   14          38             84.0   24189.50             0  \n4      9   13          37             84.0   24189.25             0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Store</th>\n      <th>DayOfWeek</th>\n      <th>Promo</th>\n      <th>StateHoliday</th>\n      <th>SchoolHoliday</th>\n      <th>StoreType</th>\n      <th>Assortment</th>\n      <th>CompetitionDistance</th>\n      <th>CompetitionOpenSinceMonth</th>\n      <th>CompetitionOpenSinceYear</th>\n      <th>Promo2</th>\n      <th>Promo2SinceWeek</th>\n      <th>Promo2SinceYear</th>\n      <th>Year</th>\n      <th>Month</th>\n      <th>Day</th>\n      <th>WeekOfYear</th>\n      <th>CompetitionOpen</th>\n      <th>PromoOpen</th>\n      <th>IsPromoMonth</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1270.0</td>\n      <td>9.0</td>\n      <td>2008.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2015</td>\n      <td>9</td>\n      <td>17</td>\n      <td>38</td>\n      <td>84.0</td>\n      <td>24189.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1270.0</td>\n      <td>9.0</td>\n      <td>2008.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2015</td>\n      <td>9</td>\n      <td>16</td>\n      <td>38</td>\n      <td>84.0</td>\n      <td>24189.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1270.0</td>\n      <td>9.0</td>\n      <td>2008.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2015</td>\n      <td>9</td>\n      <td>15</td>\n      <td>38</td>\n      <td>84.0</td>\n      <td>24189.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1270.0</td>\n      <td>9.0</td>\n      <td>2008.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2015</td>\n      <td>9</td>\n      <td>14</td>\n      <td>38</td>\n      <td>84.0</td>\n      <td>24189.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1270.0</td>\n      <td>9.0</td>\n      <td>2008.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2015</td>\n      <td>9</td>\n      <td>13</td>\n      <td>37</td>\n      <td>84.0</td>\n      <td>24189.25</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "        Store  DayOfWeek  Promo  StateHoliday  SchoolHoliday  StoreType  \\\n0           1          5      1             0              1          3   \n679364    747          5      1             0              1          3   \n702362    772          5      1             0              1          4   \n683890    752          5      1             0              1          1   \n17714      20          5      1             0              0          4   \n\n        Assortment  CompetitionDistance  CompetitionOpenSinceMonth  \\\n0                1               1270.0                        9.0   \n679364           3              45740.0                        8.0   \n702362           3               1850.0                        0.0   \n683890           1                970.0                        3.0   \n17714            1               2340.0                        5.0   \n\n        CompetitionOpenSinceYear  ...  Promo2SinceWeek  Promo2SinceYear  Year  \\\n0                         2008.0  ...              0.0              0.0  2015   \n679364                    2008.0  ...              0.0              0.0  2015   \n702362                       0.0  ...              0.0              0.0  2015   \n683890                    2013.0  ...             31.0           2013.0  2015   \n17714                     2009.0  ...             40.0           2014.0  2015   \n\n        Month  Day  WeekOfYear  CompetitionOpen  PromoOpen  IsPromoMonth  \\\n0           7   31          31             82.0   24187.75             0   \n679364      7   31          31             83.0   24187.75             0   \n702362      7   31          31          24187.0   24187.75             0   \n683890      7   31          31             28.0      24.00             0   \n17714       7   31          31             74.0       9.75             1   \n\n        Sales  \n0        5263  \n679364  10708  \n702362   5224  \n683890   7763  \n17714    9593  \n\n[5 rows x 21 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Store</th>\n      <th>DayOfWeek</th>\n      <th>Promo</th>\n      <th>StateHoliday</th>\n      <th>SchoolHoliday</th>\n      <th>StoreType</th>\n      <th>Assortment</th>\n      <th>CompetitionDistance</th>\n      <th>CompetitionOpenSinceMonth</th>\n      <th>CompetitionOpenSinceYear</th>\n      <th>...</th>\n      <th>Promo2SinceWeek</th>\n      <th>Promo2SinceYear</th>\n      <th>Year</th>\n      <th>Month</th>\n      <th>Day</th>\n      <th>WeekOfYear</th>\n      <th>CompetitionOpen</th>\n      <th>PromoOpen</th>\n      <th>IsPromoMonth</th>\n      <th>Sales</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1270.0</td>\n      <td>9.0</td>\n      <td>2008.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2015</td>\n      <td>7</td>\n      <td>31</td>\n      <td>31</td>\n      <td>82.0</td>\n      <td>24187.75</td>\n      <td>0</td>\n      <td>5263</td>\n    </tr>\n    <tr>\n      <th>679364</th>\n      <td>747</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>3</td>\n      <td>45740.0</td>\n      <td>8.0</td>\n      <td>2008.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2015</td>\n      <td>7</td>\n      <td>31</td>\n      <td>31</td>\n      <td>83.0</td>\n      <td>24187.75</td>\n      <td>0</td>\n      <td>10708</td>\n    </tr>\n    <tr>\n      <th>702362</th>\n      <td>772</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>4</td>\n      <td>3</td>\n      <td>1850.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2015</td>\n      <td>7</td>\n      <td>31</td>\n      <td>31</td>\n      <td>24187.0</td>\n      <td>24187.75</td>\n      <td>0</td>\n      <td>5224</td>\n    </tr>\n    <tr>\n      <th>683890</th>\n      <td>752</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>970.0</td>\n      <td>3.0</td>\n      <td>2013.0</td>\n      <td>...</td>\n      <td>31.0</td>\n      <td>2013.0</td>\n      <td>2015</td>\n      <td>7</td>\n      <td>31</td>\n      <td>31</td>\n      <td>28.0</td>\n      <td>24.00</td>\n      <td>0</td>\n      <td>7763</td>\n    </tr>\n    <tr>\n      <th>17714</th>\n      <td>20</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>2340.0</td>\n      <td>5.0</td>\n      <td>2009.0</td>\n      <td>...</td>\n      <td>40.0</td>\n      <td>2014.0</td>\n      <td>2015</td>\n      <td>7</td>\n      <td>31</td>\n      <td>31</td>\n      <td>74.0</td>\n      <td>9.75</td>\n      <td>1</td>\n      <td>9593</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 21 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "train['class'] = 'bad'\n",
    "train.loc[(train['Sales'] > 6300), 'class'] = 'good'\n",
    "featuresR  = ['Store', 'DayOfWeek', 'Promo', 'StateHoliday', 'SchoolHoliday',\n",
    "       'StoreType', 'Assortment', 'CompetitionDistance',\n",
    "       'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2',\n",
    "       'Promo2SinceWeek', 'Promo2SinceYear', 'Year', 'Month', 'Day',\n",
    "       'WeekOfYear', 'CompetitionOpen', 'PromoOpen', 'IsPromoMonth']\n",
    "train['good'] = 0\n",
    "train['bad'] = 0\n",
    "train.loc[train['class'] == 'good', 'good'] = 1\n",
    "train.loc[train['class'] == 'bad', 'bad'] = 1\n",
    "train_data = train[featuresR].to_numpy()\n",
    "labels_train = train[['class','good', 'bad']].to_numpy()\n",
    "\n",
    "X_trainR, X_testR, y_trainR, y_testR = train_test_split(train_data, labels_train, random_state=0)\n",
    "X_trainR, X_testR = X_trainR.astype('float32'), X_testR.astype('float32')\n",
    "y_train_labR, y_test_labR = y_trainR[:, 0], y_testR[:, 0]\n",
    "y_trainR, y_testR = y_trainR[:, 1:].astype('float32'), y_testR[:, 1:].astype('float32')\n",
    "scalerR = StandardScaler()\n",
    "scalerR.fit(X_trainR)\n",
    "category_map = {1: ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], 2:['PromoNo', 'PromoYes'], 3: ['NoStateHoliday', 'PublicHoliday', 'EasterHoliday', 'ChristmasHoliday'],\n",
    "                4:['SchoolHolidayNo', 'SchoolHolidayYes'], 5: ['StoreTypeA', 'StoreTypeB', 'StoreTypeC', 'StoreTypeD'], 6:['Basic', 'Extra', 'Extended'], 8:['None','Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], 10: ['NoPromo2', 'Promo2'], 14: ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], 15: ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], 19: ['NoPromoMonth', 'PromoMonth'] }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "        Store  DayOfWeek  Promo  StateHoliday  SchoolHoliday  StoreType  \\\n0           1          5      1             0              1          3   \n679364    747          5      1             0              1          3   \n702362    772          5      1             0              1          4   \n683890    752          5      1             0              1          1   \n17714      20          5      1             0              0          4   \n\n        Assortment  CompetitionDistance  CompetitionOpenSinceMonth  \\\n0                1               1270.0                        9.0   \n679364           3              45740.0                        8.0   \n702362           3               1850.0                        0.0   \n683890           1                970.0                        3.0   \n17714            1               2340.0                        5.0   \n\n        CompetitionOpenSinceYear  ...  Month  Day  WeekOfYear  \\\n0                         2008.0  ...      7   31          31   \n679364                    2008.0  ...      7   31          31   \n702362                       0.0  ...      7   31          31   \n683890                    2013.0  ...      7   31          31   \n17714                     2009.0  ...      7   31          31   \n\n        CompetitionOpen  PromoOpen  IsPromoMonth  Sales  class  good  bad  \n0                  82.0   24187.75             0   5263    bad     0    1  \n679364             83.0   24187.75             0  10708   good     1    0  \n702362          24187.0   24187.75             0   5224    bad     0    1  \n683890             28.0      24.00             0   7763   good     1    0  \n17714              74.0       9.75             1   9593   good     1    0  \n\n[5 rows x 24 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Store</th>\n      <th>DayOfWeek</th>\n      <th>Promo</th>\n      <th>StateHoliday</th>\n      <th>SchoolHoliday</th>\n      <th>StoreType</th>\n      <th>Assortment</th>\n      <th>CompetitionDistance</th>\n      <th>CompetitionOpenSinceMonth</th>\n      <th>CompetitionOpenSinceYear</th>\n      <th>...</th>\n      <th>Month</th>\n      <th>Day</th>\n      <th>WeekOfYear</th>\n      <th>CompetitionOpen</th>\n      <th>PromoOpen</th>\n      <th>IsPromoMonth</th>\n      <th>Sales</th>\n      <th>class</th>\n      <th>good</th>\n      <th>bad</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1270.0</td>\n      <td>9.0</td>\n      <td>2008.0</td>\n      <td>...</td>\n      <td>7</td>\n      <td>31</td>\n      <td>31</td>\n      <td>82.0</td>\n      <td>24187.75</td>\n      <td>0</td>\n      <td>5263</td>\n      <td>bad</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>679364</th>\n      <td>747</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>3</td>\n      <td>45740.0</td>\n      <td>8.0</td>\n      <td>2008.0</td>\n      <td>...</td>\n      <td>7</td>\n      <td>31</td>\n      <td>31</td>\n      <td>83.0</td>\n      <td>24187.75</td>\n      <td>0</td>\n      <td>10708</td>\n      <td>good</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>702362</th>\n      <td>772</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>4</td>\n      <td>3</td>\n      <td>1850.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>7</td>\n      <td>31</td>\n      <td>31</td>\n      <td>24187.0</td>\n      <td>24187.75</td>\n      <td>0</td>\n      <td>5224</td>\n      <td>bad</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>683890</th>\n      <td>752</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>970.0</td>\n      <td>3.0</td>\n      <td>2013.0</td>\n      <td>...</td>\n      <td>7</td>\n      <td>31</td>\n      <td>31</td>\n      <td>28.0</td>\n      <td>24.00</td>\n      <td>0</td>\n      <td>7763</td>\n      <td>good</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17714</th>\n      <td>20</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>2340.0</td>\n      <td>5.0</td>\n      <td>2009.0</td>\n      <td>...</td>\n      <td>7</td>\n      <td>31</td>\n      <td>31</td>\n      <td>74.0</td>\n      <td>9.75</td>\n      <td>1</td>\n      <td>9593</td>\n      <td>good</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 24 columns</p>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Select good wine instance\n",
    "We partition the dataset into good and bad portions and select an instance of interest. I’ve chosen it to be a good quality\n",
    "wine.\n",
    "Note that bad wines are class 1 and correspond to the second model output being high, whereas good wines are class\n",
    "0 and correspond to the first model output being high."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "bad_days = np.array([a for a, b in zip(X_trainR, y_trainR) if b[1] == 1])\n",
    "good_days = np.array([a for a, b in zip(X_trainR, y_trainR) if b[1] == 0])\n",
    "xR = np.array([[747,5,1,0,1,3,3,45740.0,8.0,2008.0,1,0.0,0.0,2015,7,31,31,83.0,24187.75,0]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "9.1.2 Training models\n",
    "Creating an Autoencoder\n",
    "For some of the explainers, we need an autoencoder to check whether example instances are close to the training data\n",
    "distribution or not."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow import keras\n",
    "ENCODING_DIM = 3 #7\n",
    "BATCH_SIZE = 32 #64\n",
    "EPOCHS = 50 #100\n",
    "class AER(keras.Model):\n",
    "    def __init__(self, encoder: keras.Model, decoder: keras.Model, **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    def call(self, x: tf.Tensor, **kwargs):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "def make_aeR():\n",
    "    len_input_output = X_trainR.shape[-1]\n",
    "\n",
    "    encoder = keras.Sequential()\n",
    "    encoder.add(Dense(units=ENCODING_DIM*2, activation=\"relu\", input_shape=(len_input_output)))\n",
    "    encoder.add(Dense(units=ENCODING_DIM, activation=\"relu\"))\n",
    "    decoder = keras.Sequential()\n",
    "    decoder.add(Dense(units=ENCODING_DIM*2, activation=\"relu\", input_shape=(ENCODING_DIM)))\n",
    "    decoder.add(Dense(units=len_input_output, activation=\"linear\"))\n",
    "    ae = AER(encoder=encoder, decoder=decoder)\n",
    "    ae.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    history = ae.fit(\n",
    "        scalerR.transform(X_trainR),\n",
    "        scalerR.transform(X_trainR),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        verbose=False,)\n",
    "    # loss = history.history['loss']\n",
    "    # plt.plot(loss)\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.ylabel('MSE-Loss')\n",
    "    ae.encoder.save(f'{ENC_FNAMER}.h5')\n",
    "    ae.decoder.save(f'{DEC_FNAMER}.h5')\n",
    "    return ae\n",
    "\n",
    "def load_ae_modelR():\n",
    "    encoder = load_model(f'{ENC_FNAMER}.h5')\n",
    "    decoder = load_model(f'{DEC_FNAMER}.h5')\n",
    "    return AER(encoder=encoder, decoder=decoder)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Random Forest Model\n",
    "We need a tree-based model to get results for the tree SHAP explainer. Hence we train a random forest on the winequality dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def make_rfcR():\n",
    "    rfc = RandomForestClassifier(n_estimators=25)\n",
    "    rfc.fit(scalerR.transform(X_trainR), y_train_labR)\n",
    "    y_predR = rfc.predict(scalerR.transform(X_testR))\n",
    "    print('accuracy_score:', accuracy_score(y_predR, y_test_labR))\n",
    "    print('f1_score:', f1_score(y_test_labR, y_predR, average=None))\n",
    "    joblib.dump(rfc, f\"{RFC_FNAMER}.joblib\")\n",
    "    return rfc\n",
    "\n",
    "def load_rfc_modelR():\n",
    "    return joblib.load(f\"{RFC_FNAMER}.joblib\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "XGBoost Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# define eval metrics\n",
    "# Mittleres Abweichungsquadrat\n",
    "def rmspe(y, yhat):\n",
    "    return np.sqrt(np.mean((yhat/y-1) ** 2))\n",
    "# expm1 ist umkehr von log1p\n",
    "def rmspe_xg(yhat, y):\n",
    "    y = np.expm1(y.get_label())\n",
    "    yhat = np.expm1(yhat)\n",
    "    return \"rmspe\", rmspe(y,yhat)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.320000e+02 5.000000e+00 1.000000e+00 ... 2.416800e+04 4.900000e+01\n",
      "  0.000000e+00]\n",
      " [4.070000e+02 1.000000e+00 0.000000e+00 ... 1.180000e+02 2.900000e+01\n",
      "  1.000000e+00]\n",
      " [3.980000e+02 7.000000e+00 0.000000e+00 ... 2.417300e+04 2.875000e+01\n",
      "  0.000000e+00]\n",
      " ...\n",
      " [5.850000e+02 5.000000e+00 1.000000e+00 ... 2.000000e+00 2.417375e+04\n",
      "  0.000000e+00]\n",
      " [7.420000e+02 6.000000e+00 0.000000e+00 ... 2.417800e+04 2.417850e+04\n",
      "  0.000000e+00]\n",
      " [1.043000e+03 2.000000e+00 0.000000e+00 ... 8.200000e+01 2.415725e+04\n",
      "  0.000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(X_trainR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_trainR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "y_train_xgb = y_train_labR\n",
    "y_train_xgb = [1 if x == 'good' else 0 for x in y_train_xgb]\n",
    "y_test_xgb = y_test_labR\n",
    "y_test_xgb = [1 if x == 'good' else 0 for x in y_test_xgb]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "def make_xgb_modelR():\n",
    "    params = {\"objective\": \"reg:linear\", # for linear regression\n",
    "              \"booster\" : \"gbtree\",   # use tree based models\n",
    "              \"eta\": 0.03,   # learning rate\n",
    "              \"max_depth\": 10,    # maximum depth of a tree\n",
    "              \"subsample\": 0.9,    # Subsample ratio of the training instances\n",
    "              \"colsample_bytree\": 0.7,   # Subsample ratio of columns when constructing each tree\n",
    "              \"silent\": 1,   # silent mode\n",
    "              \"seed\": 10   # Random number seed\n",
    "              }\n",
    "    # anzahl trainingsrunden\n",
    "    num_boost_round = 500\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_trainR, y_train_xgb)\n",
    "    dtest = xgb.DMatrix(X_testR, y_test_xgb)\n",
    "    watchlist = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "    # train the xgboost model\n",
    "    model = xgb.train(params, dtrain, num_boost_round, evals=watchlist, \\\n",
    "      early_stopping_rounds= 500, feval=rmspe_xg, verbose_eval=True)\n",
    "    y_predR = np.rint(model.predict(xgb.DMatrix(X_testR)))\n",
    "    print(y_predR)\n",
    "    print('accuracy_score:', accuracy_score(y_predR, y_test_xgb))\n",
    "    print('f1_score:', f1_score(y_predR, y_test_xgb))\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tensorflow Model\n",
    "Finally, we also train a TensorFlow model.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def make_tf_modelR():\n",
    "    inputs = keras.Input(shape=X_trainR.shape[1])\n",
    "    x = layers.Dense(6, activation=\"relu\")(inputs)\n",
    "    outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "    history = model.fit(\n",
    "        scalerR.transform(X_trainR),\n",
    "        y_trainR,\n",
    "        epochs=15,\n",
    "        verbose=False,\n",
    "        validation_data=(scalerR.transform(X_testR), y_testR),\n",
    "        )\n",
    "    y_predR = model(scalerR.transform(X_testR)).numpy().argmax(axis=1)\n",
    "    print('accuracy_score:', accuracy_score(y_predR, y_testR.argmax(axis=1)))\n",
    "    print('f1_score:', f1_score(y_predR, y_testR.argmax(axis=1), average=None))\n",
    "    model.save(f'{TF_MODEL_FNAMER}.h5')\n",
    "    return model\n",
    "def load_tf_modelR():\n",
    "    return load_model(f'{TF_MODEL_FNAMER}.h5', compile= False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:23:37] WARNING: c:\\windows\\temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\objective\\regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:23:37] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:0.49719\ttrain-rmspe:inf\teval-rmse:0.49723\teval-rmspe:inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christian Karg\\AppData\\Local\\Temp\\ipykernel_19692\\2814622519.py:4: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return np.sqrt(np.mean((yhat/y-1) ** 2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttrain-rmse:0.49285\ttrain-rmspe:inf\teval-rmse:0.49291\teval-rmspe:inf\n",
      "[2]\ttrain-rmse:0.48763\ttrain-rmspe:inf\teval-rmse:0.48774\teval-rmspe:inf\n",
      "[3]\ttrain-rmse:0.48237\ttrain-rmspe:inf\teval-rmse:0.48254\teval-rmspe:inf\n",
      "[4]\ttrain-rmse:0.47723\ttrain-rmspe:inf\teval-rmse:0.47744\teval-rmspe:inf\n",
      "[5]\ttrain-rmse:0.47199\ttrain-rmspe:inf\teval-rmse:0.47225\teval-rmspe:inf\n",
      "[6]\ttrain-rmse:0.46695\ttrain-rmspe:inf\teval-rmse:0.46726\teval-rmspe:inf\n",
      "[7]\ttrain-rmse:0.46233\ttrain-rmspe:inf\teval-rmse:0.46269\teval-rmspe:inf\n",
      "[8]\ttrain-rmse:0.45826\ttrain-rmspe:inf\teval-rmse:0.45865\teval-rmspe:inf\n",
      "[9]\ttrain-rmse:0.45494\ttrain-rmspe:inf\teval-rmse:0.45538\teval-rmspe:inf\n",
      "[10]\ttrain-rmse:0.45130\ttrain-rmspe:inf\teval-rmse:0.45178\teval-rmspe:inf\n",
      "[11]\ttrain-rmse:0.44779\ttrain-rmspe:inf\teval-rmse:0.44831\teval-rmspe:inf\n",
      "[12]\ttrain-rmse:0.44406\ttrain-rmspe:inf\teval-rmse:0.44463\teval-rmspe:inf\n",
      "[13]\ttrain-rmse:0.44213\ttrain-rmspe:inf\teval-rmse:0.44273\teval-rmspe:inf\n",
      "[14]\ttrain-rmse:0.43839\ttrain-rmspe:inf\teval-rmse:0.43905\teval-rmspe:inf\n",
      "[15]\ttrain-rmse:0.43441\ttrain-rmspe:inf\teval-rmse:0.43512\teval-rmspe:inf\n",
      "[16]\ttrain-rmse:0.43083\ttrain-rmspe:inf\teval-rmse:0.43159\teval-rmspe:inf\n",
      "[17]\ttrain-rmse:0.42731\ttrain-rmspe:inf\teval-rmse:0.42811\teval-rmspe:inf\n",
      "[18]\ttrain-rmse:0.42397\ttrain-rmspe:inf\teval-rmse:0.42481\teval-rmspe:inf\n",
      "[19]\ttrain-rmse:0.42158\ttrain-rmspe:inf\teval-rmse:0.42246\teval-rmspe:inf\n",
      "[20]\ttrain-rmse:0.41862\ttrain-rmspe:inf\teval-rmse:0.41953\teval-rmspe:inf\n",
      "[21]\ttrain-rmse:0.41549\ttrain-rmspe:inf\teval-rmse:0.41644\teval-rmspe:inf\n",
      "[22]\ttrain-rmse:0.41345\ttrain-rmspe:inf\teval-rmse:0.41442\teval-rmspe:inf\n",
      "[23]\ttrain-rmse:0.41098\ttrain-rmspe:inf\teval-rmse:0.41200\teval-rmspe:inf\n",
      "[24]\ttrain-rmse:0.40862\ttrain-rmspe:inf\teval-rmse:0.40968\teval-rmspe:inf\n",
      "[25]\ttrain-rmse:0.40640\ttrain-rmspe:inf\teval-rmse:0.40750\teval-rmspe:inf\n",
      "[26]\ttrain-rmse:0.40450\ttrain-rmspe:inf\teval-rmse:0.40563\teval-rmspe:inf\n",
      "[27]\ttrain-rmse:0.40230\ttrain-rmspe:inf\teval-rmse:0.40346\teval-rmspe:inf\n",
      "[28]\ttrain-rmse:0.40023\ttrain-rmspe:inf\teval-rmse:0.40141\teval-rmspe:inf\n",
      "[29]\ttrain-rmse:0.39808\ttrain-rmspe:inf\teval-rmse:0.39930\teval-rmspe:inf\n",
      "[30]\ttrain-rmse:0.39614\ttrain-rmspe:inf\teval-rmse:0.39740\teval-rmspe:inf\n",
      "[31]\ttrain-rmse:0.39389\ttrain-rmspe:inf\teval-rmse:0.39519\teval-rmspe:inf\n",
      "[32]\ttrain-rmse:0.39204\ttrain-rmspe:inf\teval-rmse:0.39338\teval-rmspe:inf\n",
      "[33]\ttrain-rmse:0.39067\ttrain-rmspe:inf\teval-rmse:0.39204\teval-rmspe:inf\n",
      "[34]\ttrain-rmse:0.38907\ttrain-rmspe:inf\teval-rmse:0.39048\teval-rmspe:inf\n",
      "[35]\ttrain-rmse:0.38732\ttrain-rmspe:inf\teval-rmse:0.38876\teval-rmspe:inf\n",
      "[36]\ttrain-rmse:0.38575\ttrain-rmspe:inf\teval-rmse:0.38721\teval-rmspe:inf\n",
      "[37]\ttrain-rmse:0.38435\ttrain-rmspe:inf\teval-rmse:0.38583\teval-rmspe:inf\n",
      "[38]\ttrain-rmse:0.38294\ttrain-rmspe:inf\teval-rmse:0.38444\teval-rmspe:inf\n",
      "[39]\ttrain-rmse:0.38171\ttrain-rmspe:inf\teval-rmse:0.38324\teval-rmspe:inf\n",
      "[40]\ttrain-rmse:0.38033\ttrain-rmspe:inf\teval-rmse:0.38189\teval-rmspe:inf\n",
      "[41]\ttrain-rmse:0.37955\ttrain-rmspe:inf\teval-rmse:0.38114\teval-rmspe:inf\n",
      "[42]\ttrain-rmse:0.37830\ttrain-rmspe:inf\teval-rmse:0.37992\teval-rmspe:inf\n",
      "[43]\ttrain-rmse:0.37737\ttrain-rmspe:inf\teval-rmse:0.37901\teval-rmspe:inf\n",
      "[44]\ttrain-rmse:0.37632\ttrain-rmspe:inf\teval-rmse:0.37800\teval-rmspe:inf\n",
      "[45]\ttrain-rmse:0.37522\ttrain-rmspe:inf\teval-rmse:0.37691\teval-rmspe:inf\n",
      "[46]\ttrain-rmse:0.37344\ttrain-rmspe:inf\teval-rmse:0.37515\teval-rmspe:inf\n",
      "[47]\ttrain-rmse:0.37230\ttrain-rmspe:inf\teval-rmse:0.37404\teval-rmspe:inf\n",
      "[48]\ttrain-rmse:0.37158\ttrain-rmspe:inf\teval-rmse:0.37334\teval-rmspe:inf\n",
      "[49]\ttrain-rmse:0.37091\ttrain-rmspe:inf\teval-rmse:0.37270\teval-rmspe:inf\n",
      "[50]\ttrain-rmse:0.37007\ttrain-rmspe:inf\teval-rmse:0.37189\teval-rmspe:inf\n",
      "[51]\ttrain-rmse:0.36861\ttrain-rmspe:inf\teval-rmse:0.37044\teval-rmspe:inf\n",
      "[52]\ttrain-rmse:0.36797\ttrain-rmspe:inf\teval-rmse:0.36981\teval-rmspe:inf\n",
      "[53]\ttrain-rmse:0.36660\ttrain-rmspe:inf\teval-rmse:0.36847\teval-rmspe:inf\n",
      "[54]\ttrain-rmse:0.36611\ttrain-rmspe:inf\teval-rmse:0.36801\teval-rmspe:inf\n",
      "[55]\ttrain-rmse:0.36516\ttrain-rmspe:inf\teval-rmse:0.36710\teval-rmspe:inf\n",
      "[56]\ttrain-rmse:0.36422\ttrain-rmspe:inf\teval-rmse:0.36619\teval-rmspe:inf\n",
      "[57]\ttrain-rmse:0.36326\ttrain-rmspe:inf\teval-rmse:0.36527\teval-rmspe:inf\n",
      "[58]\ttrain-rmse:0.36219\ttrain-rmspe:inf\teval-rmse:0.36422\teval-rmspe:inf\n",
      "[59]\ttrain-rmse:0.36170\ttrain-rmspe:inf\teval-rmse:0.36375\teval-rmspe:inf\n",
      "[60]\ttrain-rmse:0.36118\ttrain-rmspe:inf\teval-rmse:0.36325\teval-rmspe:inf\n",
      "[61]\ttrain-rmse:0.36035\ttrain-rmspe:inf\teval-rmse:0.36245\teval-rmspe:inf\n",
      "[62]\ttrain-rmse:0.35957\ttrain-rmspe:inf\teval-rmse:0.36170\teval-rmspe:inf\n",
      "[63]\ttrain-rmse:0.35916\ttrain-rmspe:inf\teval-rmse:0.36131\teval-rmspe:inf\n",
      "[64]\ttrain-rmse:0.35855\ttrain-rmspe:inf\teval-rmse:0.36072\teval-rmspe:inf\n",
      "[65]\ttrain-rmse:0.35822\ttrain-rmspe:inf\teval-rmse:0.36041\teval-rmspe:inf\n",
      "[66]\ttrain-rmse:0.35698\ttrain-rmspe:inf\teval-rmse:0.35919\teval-rmspe:inf\n",
      "[67]\ttrain-rmse:0.35597\ttrain-rmspe:inf\teval-rmse:0.35821\teval-rmspe:inf\n",
      "[68]\ttrain-rmse:0.35542\ttrain-rmspe:inf\teval-rmse:0.35767\teval-rmspe:inf\n",
      "[69]\ttrain-rmse:0.35440\ttrain-rmspe:inf\teval-rmse:0.35668\teval-rmspe:inf\n",
      "[70]\ttrain-rmse:0.35407\ttrain-rmspe:inf\teval-rmse:0.35638\teval-rmspe:inf\n",
      "[71]\ttrain-rmse:0.35322\ttrain-rmspe:inf\teval-rmse:0.35554\teval-rmspe:inf\n",
      "[72]\ttrain-rmse:0.35274\ttrain-rmspe:inf\teval-rmse:0.35510\teval-rmspe:inf\n",
      "[73]\ttrain-rmse:0.35183\ttrain-rmspe:inf\teval-rmse:0.35422\teval-rmspe:inf\n",
      "[74]\ttrain-rmse:0.35080\ttrain-rmspe:inf\teval-rmse:0.35320\teval-rmspe:inf\n",
      "[75]\ttrain-rmse:0.34995\ttrain-rmspe:inf\teval-rmse:0.35236\teval-rmspe:inf\n",
      "[76]\ttrain-rmse:0.34928\ttrain-rmspe:inf\teval-rmse:0.35171\teval-rmspe:inf\n",
      "[77]\ttrain-rmse:0.34864\ttrain-rmspe:inf\teval-rmse:0.35109\teval-rmspe:inf\n",
      "[78]\ttrain-rmse:0.34798\ttrain-rmspe:inf\teval-rmse:0.35045\teval-rmspe:inf\n",
      "[79]\ttrain-rmse:0.34768\ttrain-rmspe:inf\teval-rmse:0.35017\teval-rmspe:inf\n",
      "[80]\ttrain-rmse:0.34672\ttrain-rmspe:inf\teval-rmse:0.34922\teval-rmspe:inf\n",
      "[81]\ttrain-rmse:0.34616\ttrain-rmspe:inf\teval-rmse:0.34869\teval-rmspe:inf\n",
      "[82]\ttrain-rmse:0.34569\ttrain-rmspe:inf\teval-rmse:0.34825\teval-rmspe:inf\n",
      "[83]\ttrain-rmse:0.34524\ttrain-rmspe:inf\teval-rmse:0.34782\teval-rmspe:inf\n",
      "[84]\ttrain-rmse:0.34481\ttrain-rmspe:inf\teval-rmse:0.34742\teval-rmspe:inf\n",
      "[85]\ttrain-rmse:0.34416\ttrain-rmspe:inf\teval-rmse:0.34678\teval-rmspe:inf\n",
      "[86]\ttrain-rmse:0.34379\ttrain-rmspe:inf\teval-rmse:0.34644\teval-rmspe:inf\n",
      "[87]\ttrain-rmse:0.34322\ttrain-rmspe:inf\teval-rmse:0.34589\teval-rmspe:inf\n",
      "[88]\ttrain-rmse:0.34269\ttrain-rmspe:inf\teval-rmse:0.34538\teval-rmspe:inf\n",
      "[89]\ttrain-rmse:0.34204\ttrain-rmspe:inf\teval-rmse:0.34476\teval-rmspe:inf\n",
      "[90]\ttrain-rmse:0.34184\ttrain-rmspe:inf\teval-rmse:0.34456\teval-rmspe:inf\n",
      "[91]\ttrain-rmse:0.34146\ttrain-rmspe:inf\teval-rmse:0.34420\teval-rmspe:inf\n",
      "[92]\ttrain-rmse:0.34126\ttrain-rmspe:inf\teval-rmse:0.34402\teval-rmspe:inf\n",
      "[93]\ttrain-rmse:0.34067\ttrain-rmspe:inf\teval-rmse:0.34345\teval-rmspe:inf\n",
      "[94]\ttrain-rmse:0.34007\ttrain-rmspe:inf\teval-rmse:0.34287\teval-rmspe:inf\n",
      "[95]\ttrain-rmse:0.33980\ttrain-rmspe:inf\teval-rmse:0.34262\teval-rmspe:inf\n",
      "[96]\ttrain-rmse:0.33962\ttrain-rmspe:inf\teval-rmse:0.34245\teval-rmspe:inf\n",
      "[97]\ttrain-rmse:0.33899\ttrain-rmspe:inf\teval-rmse:0.34184\teval-rmspe:inf\n",
      "[98]\ttrain-rmse:0.33837\ttrain-rmspe:inf\teval-rmse:0.34125\teval-rmspe:inf\n",
      "[99]\ttrain-rmse:0.33759\ttrain-rmspe:inf\teval-rmse:0.34049\teval-rmspe:inf\n",
      "[100]\ttrain-rmse:0.33658\ttrain-rmspe:inf\teval-rmse:0.33948\teval-rmspe:inf\n",
      "[101]\ttrain-rmse:0.33629\ttrain-rmspe:inf\teval-rmse:0.33923\teval-rmspe:inf\n",
      "[102]\ttrain-rmse:0.33578\ttrain-rmspe:inf\teval-rmse:0.33874\teval-rmspe:inf\n",
      "[103]\ttrain-rmse:0.33540\ttrain-rmspe:inf\teval-rmse:0.33837\teval-rmspe:inf\n",
      "[104]\ttrain-rmse:0.33475\ttrain-rmspe:inf\teval-rmse:0.33774\teval-rmspe:inf\n",
      "[105]\ttrain-rmse:0.33437\ttrain-rmspe:inf\teval-rmse:0.33738\teval-rmspe:inf\n",
      "[106]\ttrain-rmse:0.33414\ttrain-rmspe:inf\teval-rmse:0.33719\teval-rmspe:inf\n",
      "[107]\ttrain-rmse:0.33339\ttrain-rmspe:inf\teval-rmse:0.33646\teval-rmspe:inf\n",
      "[108]\ttrain-rmse:0.33278\ttrain-rmspe:inf\teval-rmse:0.33587\teval-rmspe:inf\n",
      "[109]\ttrain-rmse:0.33210\ttrain-rmspe:inf\teval-rmse:0.33520\teval-rmspe:inf\n",
      "[110]\ttrain-rmse:0.33181\ttrain-rmspe:inf\teval-rmse:0.33493\teval-rmspe:inf\n",
      "[111]\ttrain-rmse:0.33122\ttrain-rmspe:inf\teval-rmse:0.33436\teval-rmspe:inf\n",
      "[112]\ttrain-rmse:0.33093\ttrain-rmspe:inf\teval-rmse:0.33409\teval-rmspe:inf\n",
      "[113]\ttrain-rmse:0.33031\ttrain-rmspe:inf\teval-rmse:0.33349\teval-rmspe:inf\n",
      "[114]\ttrain-rmse:0.33003\ttrain-rmspe:inf\teval-rmse:0.33324\teval-rmspe:inf\n",
      "[115]\ttrain-rmse:0.32960\ttrain-rmspe:inf\teval-rmse:0.33283\teval-rmspe:inf\n",
      "[116]\ttrain-rmse:0.32917\ttrain-rmspe:inf\teval-rmse:0.33241\teval-rmspe:inf\n",
      "[117]\ttrain-rmse:0.32882\ttrain-rmspe:inf\teval-rmse:0.33208\teval-rmspe:inf\n",
      "[118]\ttrain-rmse:0.32869\ttrain-rmspe:inf\teval-rmse:0.33196\teval-rmspe:inf\n",
      "[119]\ttrain-rmse:0.32830\ttrain-rmspe:inf\teval-rmse:0.33157\teval-rmspe:inf\n",
      "[120]\ttrain-rmse:0.32791\ttrain-rmspe:inf\teval-rmse:0.33120\teval-rmspe:inf\n",
      "[121]\ttrain-rmse:0.32741\ttrain-rmspe:inf\teval-rmse:0.33072\teval-rmspe:inf\n",
      "[122]\ttrain-rmse:0.32686\ttrain-rmspe:inf\teval-rmse:0.33019\teval-rmspe:inf\n",
      "[123]\ttrain-rmse:0.32657\ttrain-rmspe:inf\teval-rmse:0.32992\teval-rmspe:inf\n",
      "[124]\ttrain-rmse:0.32591\ttrain-rmspe:inf\teval-rmse:0.32928\teval-rmspe:inf\n",
      "[125]\ttrain-rmse:0.32518\ttrain-rmspe:inf\teval-rmse:0.32856\teval-rmspe:inf\n",
      "[126]\ttrain-rmse:0.32492\ttrain-rmspe:inf\teval-rmse:0.32829\teval-rmspe:inf\n",
      "[127]\ttrain-rmse:0.32479\ttrain-rmspe:inf\teval-rmse:0.32819\teval-rmspe:inf\n",
      "[128]\ttrain-rmse:0.32450\ttrain-rmspe:inf\teval-rmse:0.32791\teval-rmspe:inf\n",
      "[129]\ttrain-rmse:0.32405\ttrain-rmspe:inf\teval-rmse:0.32750\teval-rmspe:inf\n",
      "[130]\ttrain-rmse:0.32382\ttrain-rmspe:inf\teval-rmse:0.32729\teval-rmspe:inf\n",
      "[131]\ttrain-rmse:0.32352\ttrain-rmspe:inf\teval-rmse:0.32701\teval-rmspe:inf\n",
      "[132]\ttrain-rmse:0.32265\ttrain-rmspe:inf\teval-rmse:0.32615\teval-rmspe:inf\n",
      "[133]\ttrain-rmse:0.32238\ttrain-rmspe:inf\teval-rmse:0.32591\teval-rmspe:inf\n",
      "[134]\ttrain-rmse:0.32212\ttrain-rmspe:inf\teval-rmse:0.32566\teval-rmspe:inf\n",
      "[135]\ttrain-rmse:0.32170\ttrain-rmspe:inf\teval-rmse:0.32525\teval-rmspe:inf\n",
      "[136]\ttrain-rmse:0.32123\ttrain-rmspe:inf\teval-rmse:0.32479\teval-rmspe:inf\n",
      "[137]\ttrain-rmse:0.32084\ttrain-rmspe:inf\teval-rmse:0.32440\teval-rmspe:inf\n",
      "[138]\ttrain-rmse:0.32060\ttrain-rmspe:inf\teval-rmse:0.32418\teval-rmspe:inf\n",
      "[139]\ttrain-rmse:0.32024\ttrain-rmspe:inf\teval-rmse:0.32383\teval-rmspe:inf\n",
      "[140]\ttrain-rmse:0.32006\ttrain-rmspe:inf\teval-rmse:0.32366\teval-rmspe:inf\n",
      "[141]\ttrain-rmse:0.31934\ttrain-rmspe:inf\teval-rmse:0.32296\teval-rmspe:inf\n",
      "[142]\ttrain-rmse:0.31913\ttrain-rmspe:inf\teval-rmse:0.32278\teval-rmspe:inf\n",
      "[143]\ttrain-rmse:0.31881\ttrain-rmspe:inf\teval-rmse:0.32248\teval-rmspe:inf\n",
      "[144]\ttrain-rmse:0.31826\ttrain-rmspe:inf\teval-rmse:0.32193\teval-rmspe:inf\n",
      "[145]\ttrain-rmse:0.31764\ttrain-rmspe:inf\teval-rmse:0.32134\teval-rmspe:inf\n",
      "[146]\ttrain-rmse:0.31677\ttrain-rmspe:inf\teval-rmse:0.32049\teval-rmspe:inf\n",
      "[147]\ttrain-rmse:0.31665\ttrain-rmspe:inf\teval-rmse:0.32039\teval-rmspe:inf\n",
      "[148]\ttrain-rmse:0.31636\ttrain-rmspe:inf\teval-rmse:0.32012\teval-rmspe:inf\n",
      "[149]\ttrain-rmse:0.31617\ttrain-rmspe:inf\teval-rmse:0.31995\teval-rmspe:inf\n",
      "[150]\ttrain-rmse:0.31578\ttrain-rmspe:inf\teval-rmse:0.31958\teval-rmspe:inf\n",
      "[151]\ttrain-rmse:0.31562\ttrain-rmspe:inf\teval-rmse:0.31944\teval-rmspe:inf\n",
      "[152]\ttrain-rmse:0.31509\ttrain-rmspe:inf\teval-rmse:0.31893\teval-rmspe:inf\n",
      "[153]\ttrain-rmse:0.31455\ttrain-rmspe:inf\teval-rmse:0.31841\teval-rmspe:inf\n",
      "[154]\ttrain-rmse:0.31399\ttrain-rmspe:inf\teval-rmse:0.31786\teval-rmspe:inf\n",
      "[155]\ttrain-rmse:0.31375\ttrain-rmspe:inf\teval-rmse:0.31763\teval-rmspe:inf\n",
      "[156]\ttrain-rmse:0.31349\ttrain-rmspe:inf\teval-rmse:0.31738\teval-rmspe:inf\n",
      "[157]\ttrain-rmse:0.31332\ttrain-rmspe:inf\teval-rmse:0.31724\teval-rmspe:inf\n",
      "[158]\ttrain-rmse:0.31286\ttrain-rmspe:inf\teval-rmse:0.31679\teval-rmspe:inf\n",
      "[159]\ttrain-rmse:0.31265\ttrain-rmspe:inf\teval-rmse:0.31659\teval-rmspe:inf\n",
      "[160]\ttrain-rmse:0.31184\ttrain-rmspe:inf\teval-rmse:0.31581\teval-rmspe:inf\n",
      "[161]\ttrain-rmse:0.31123\ttrain-rmspe:inf\teval-rmse:0.31523\teval-rmspe:inf\n",
      "[162]\ttrain-rmse:0.31084\ttrain-rmspe:inf\teval-rmse:0.31484\teval-rmspe:inf\n",
      "[163]\ttrain-rmse:0.31065\ttrain-rmspe:inf\teval-rmse:0.31467\teval-rmspe:inf\n",
      "[164]\ttrain-rmse:0.31043\ttrain-rmspe:inf\teval-rmse:0.31448\teval-rmspe:inf\n",
      "[165]\ttrain-rmse:0.31016\ttrain-rmspe:inf\teval-rmse:0.31423\teval-rmspe:inf\n",
      "[166]\ttrain-rmse:0.30969\ttrain-rmspe:inf\teval-rmse:0.31377\teval-rmspe:inf\n",
      "[167]\ttrain-rmse:0.30950\ttrain-rmspe:inf\teval-rmse:0.31359\teval-rmspe:inf\n",
      "[168]\ttrain-rmse:0.30910\ttrain-rmspe:inf\teval-rmse:0.31321\teval-rmspe:inf\n",
      "[169]\ttrain-rmse:0.30898\ttrain-rmspe:inf\teval-rmse:0.31312\teval-rmspe:inf\n",
      "[170]\ttrain-rmse:0.30876\ttrain-rmspe:inf\teval-rmse:0.31290\teval-rmspe:inf\n",
      "[171]\ttrain-rmse:0.30856\ttrain-rmspe:inf\teval-rmse:0.31272\teval-rmspe:inf\n",
      "[172]\ttrain-rmse:0.30830\ttrain-rmspe:inf\teval-rmse:0.31247\teval-rmspe:inf\n",
      "[173]\ttrain-rmse:0.30819\ttrain-rmspe:inf\teval-rmse:0.31238\teval-rmspe:inf\n",
      "[174]\ttrain-rmse:0.30781\ttrain-rmspe:inf\teval-rmse:0.31202\teval-rmspe:inf\n",
      "[175]\ttrain-rmse:0.30753\ttrain-rmspe:inf\teval-rmse:0.31174\teval-rmspe:inf\n",
      "[176]\ttrain-rmse:0.30734\ttrain-rmspe:inf\teval-rmse:0.31158\teval-rmspe:inf\n",
      "[177]\ttrain-rmse:0.30694\ttrain-rmspe:inf\teval-rmse:0.31121\teval-rmspe:inf\n",
      "[178]\ttrain-rmse:0.30684\ttrain-rmspe:inf\teval-rmse:0.31112\teval-rmspe:inf\n",
      "[179]\ttrain-rmse:0.30669\ttrain-rmspe:inf\teval-rmse:0.31099\teval-rmspe:inf\n",
      "[180]\ttrain-rmse:0.30649\ttrain-rmspe:inf\teval-rmse:0.31080\teval-rmspe:inf\n",
      "[181]\ttrain-rmse:0.30589\ttrain-rmspe:inf\teval-rmse:0.31021\teval-rmspe:inf\n",
      "[182]\ttrain-rmse:0.30512\ttrain-rmspe:inf\teval-rmse:0.30944\teval-rmspe:inf\n",
      "[183]\ttrain-rmse:0.30474\ttrain-rmspe:inf\teval-rmse:0.30908\teval-rmspe:inf\n",
      "[184]\ttrain-rmse:0.30451\ttrain-rmspe:inf\teval-rmse:0.30885\teval-rmspe:inf\n",
      "[185]\ttrain-rmse:0.30406\ttrain-rmspe:inf\teval-rmse:0.30841\teval-rmspe:inf\n",
      "[186]\ttrain-rmse:0.30372\ttrain-rmspe:inf\teval-rmse:0.30808\teval-rmspe:inf\n",
      "[187]\ttrain-rmse:0.30355\ttrain-rmspe:inf\teval-rmse:0.30792\teval-rmspe:inf\n",
      "[188]\ttrain-rmse:0.30276\ttrain-rmspe:inf\teval-rmse:0.30715\teval-rmspe:inf\n",
      "[189]\ttrain-rmse:0.30263\ttrain-rmspe:inf\teval-rmse:0.30704\teval-rmspe:inf\n",
      "[190]\ttrain-rmse:0.30254\ttrain-rmspe:inf\teval-rmse:0.30696\teval-rmspe:inf\n",
      "[191]\ttrain-rmse:0.30204\ttrain-rmspe:inf\teval-rmse:0.30647\teval-rmspe:inf\n",
      "[192]\ttrain-rmse:0.30145\ttrain-rmspe:inf\teval-rmse:0.30589\teval-rmspe:inf\n",
      "[193]\ttrain-rmse:0.30124\ttrain-rmspe:inf\teval-rmse:0.30571\teval-rmspe:inf\n",
      "[194]\ttrain-rmse:0.30074\ttrain-rmspe:inf\teval-rmse:0.30522\teval-rmspe:inf\n",
      "[195]\ttrain-rmse:0.30056\ttrain-rmspe:inf\teval-rmse:0.30504\teval-rmspe:inf\n",
      "[196]\ttrain-rmse:0.30027\ttrain-rmspe:inf\teval-rmse:0.30476\teval-rmspe:inf\n",
      "[197]\ttrain-rmse:0.30017\ttrain-rmspe:inf\teval-rmse:0.30467\teval-rmspe:inf\n",
      "[198]\ttrain-rmse:0.29973\ttrain-rmspe:inf\teval-rmse:0.30424\teval-rmspe:inf\n",
      "[199]\ttrain-rmse:0.29895\ttrain-rmspe:inf\teval-rmse:0.30347\teval-rmspe:inf\n",
      "[200]\ttrain-rmse:0.29815\ttrain-rmspe:inf\teval-rmse:0.30269\teval-rmspe:inf\n",
      "[201]\ttrain-rmse:0.29781\ttrain-rmspe:inf\teval-rmse:0.30235\teval-rmspe:inf\n",
      "[202]\ttrain-rmse:0.29727\ttrain-rmspe:inf\teval-rmse:0.30183\teval-rmspe:inf\n",
      "[203]\ttrain-rmse:0.29685\ttrain-rmspe:inf\teval-rmse:0.30143\teval-rmspe:inf\n",
      "[204]\ttrain-rmse:0.29627\ttrain-rmspe:inf\teval-rmse:0.30087\teval-rmspe:inf\n",
      "[205]\ttrain-rmse:0.29592\ttrain-rmspe:inf\teval-rmse:0.30053\teval-rmspe:inf\n",
      "[206]\ttrain-rmse:0.29586\ttrain-rmspe:inf\teval-rmse:0.30049\teval-rmspe:inf\n",
      "[207]\ttrain-rmse:0.29559\ttrain-rmspe:inf\teval-rmse:0.30021\teval-rmspe:inf\n",
      "[208]\ttrain-rmse:0.29547\ttrain-rmspe:inf\teval-rmse:0.30012\teval-rmspe:inf\n",
      "[209]\ttrain-rmse:0.29537\ttrain-rmspe:inf\teval-rmse:0.30003\teval-rmspe:inf\n",
      "[210]\ttrain-rmse:0.29499\ttrain-rmspe:inf\teval-rmse:0.29967\teval-rmspe:inf\n",
      "[211]\ttrain-rmse:0.29485\ttrain-rmspe:inf\teval-rmse:0.29954\teval-rmspe:inf\n",
      "[212]\ttrain-rmse:0.29473\ttrain-rmspe:inf\teval-rmse:0.29945\teval-rmspe:inf\n",
      "[213]\ttrain-rmse:0.29412\ttrain-rmspe:inf\teval-rmse:0.29885\teval-rmspe:inf\n",
      "[214]\ttrain-rmse:0.29400\ttrain-rmspe:inf\teval-rmse:0.29874\teval-rmspe:inf\n",
      "[215]\ttrain-rmse:0.29391\ttrain-rmspe:inf\teval-rmse:0.29867\teval-rmspe:inf\n",
      "[216]\ttrain-rmse:0.29364\ttrain-rmspe:inf\teval-rmse:0.29841\teval-rmspe:inf\n",
      "[217]\ttrain-rmse:0.29325\ttrain-rmspe:inf\teval-rmse:0.29802\teval-rmspe:inf\n",
      "[218]\ttrain-rmse:0.29304\ttrain-rmspe:inf\teval-rmse:0.29782\teval-rmspe:inf\n",
      "[219]\ttrain-rmse:0.29284\ttrain-rmspe:inf\teval-rmse:0.29764\teval-rmspe:inf\n",
      "[220]\ttrain-rmse:0.29246\ttrain-rmspe:inf\teval-rmse:0.29728\teval-rmspe:inf\n",
      "[221]\ttrain-rmse:0.29235\ttrain-rmspe:inf\teval-rmse:0.29718\teval-rmspe:inf\n",
      "[222]\ttrain-rmse:0.29165\ttrain-rmspe:inf\teval-rmse:0.29648\teval-rmspe:inf\n",
      "[223]\ttrain-rmse:0.29151\ttrain-rmspe:inf\teval-rmse:0.29636\teval-rmspe:inf\n",
      "[224]\ttrain-rmse:0.29139\ttrain-rmspe:inf\teval-rmse:0.29627\teval-rmspe:inf\n",
      "[225]\ttrain-rmse:0.29075\ttrain-rmspe:inf\teval-rmse:0.29563\teval-rmspe:inf\n",
      "[226]\ttrain-rmse:0.29032\ttrain-rmspe:inf\teval-rmse:0.29522\teval-rmspe:inf\n",
      "[227]\ttrain-rmse:0.29028\ttrain-rmspe:inf\teval-rmse:0.29519\teval-rmspe:inf\n",
      "[228]\ttrain-rmse:0.29000\ttrain-rmspe:inf\teval-rmse:0.29492\teval-rmspe:inf\n",
      "[229]\ttrain-rmse:0.28996\ttrain-rmspe:inf\teval-rmse:0.29489\teval-rmspe:inf\n",
      "[230]\ttrain-rmse:0.28936\ttrain-rmspe:inf\teval-rmse:0.29432\teval-rmspe:inf\n",
      "[231]\ttrain-rmse:0.28919\ttrain-rmspe:inf\teval-rmse:0.29416\teval-rmspe:inf\n",
      "[232]\ttrain-rmse:0.28869\ttrain-rmspe:inf\teval-rmse:0.29369\teval-rmspe:inf\n",
      "[233]\ttrain-rmse:0.28821\ttrain-rmspe:inf\teval-rmse:0.29322\teval-rmspe:inf\n",
      "[234]\ttrain-rmse:0.28768\ttrain-rmspe:inf\teval-rmse:0.29270\teval-rmspe:inf\n",
      "[235]\ttrain-rmse:0.28764\ttrain-rmspe:inf\teval-rmse:0.29267\teval-rmspe:inf\n",
      "[236]\ttrain-rmse:0.28722\ttrain-rmspe:inf\teval-rmse:0.29228\teval-rmspe:inf\n",
      "[237]\ttrain-rmse:0.28707\ttrain-rmspe:inf\teval-rmse:0.29214\teval-rmspe:inf\n",
      "[238]\ttrain-rmse:0.28682\ttrain-rmspe:inf\teval-rmse:0.29191\teval-rmspe:inf\n",
      "[239]\ttrain-rmse:0.28667\ttrain-rmspe:inf\teval-rmse:0.29178\teval-rmspe:inf\n",
      "[240]\ttrain-rmse:0.28639\ttrain-rmspe:inf\teval-rmse:0.29152\teval-rmspe:inf\n",
      "[241]\ttrain-rmse:0.28612\ttrain-rmspe:inf\teval-rmse:0.29126\teval-rmspe:inf\n",
      "[242]\ttrain-rmse:0.28589\ttrain-rmspe:inf\teval-rmse:0.29105\teval-rmspe:inf\n",
      "[243]\ttrain-rmse:0.28569\ttrain-rmspe:inf\teval-rmse:0.29086\teval-rmspe:inf\n",
      "[244]\ttrain-rmse:0.28554\ttrain-rmspe:inf\teval-rmse:0.29074\teval-rmspe:inf\n",
      "[245]\ttrain-rmse:0.28524\ttrain-rmspe:inf\teval-rmse:0.29044\teval-rmspe:inf\n",
      "[246]\ttrain-rmse:0.28500\ttrain-rmspe:inf\teval-rmse:0.29021\teval-rmspe:inf\n",
      "[247]\ttrain-rmse:0.28491\ttrain-rmspe:inf\teval-rmse:0.29014\teval-rmspe:inf\n",
      "[248]\ttrain-rmse:0.28476\ttrain-rmspe:inf\teval-rmse:0.29001\teval-rmspe:inf\n",
      "[249]\ttrain-rmse:0.28457\ttrain-rmspe:inf\teval-rmse:0.28984\teval-rmspe:inf\n",
      "[250]\ttrain-rmse:0.28425\ttrain-rmspe:inf\teval-rmse:0.28954\teval-rmspe:inf\n",
      "[251]\ttrain-rmse:0.28414\ttrain-rmspe:inf\teval-rmse:0.28943\teval-rmspe:inf\n",
      "[252]\ttrain-rmse:0.28351\ttrain-rmspe:inf\teval-rmse:0.28882\teval-rmspe:inf\n",
      "[253]\ttrain-rmse:0.28304\ttrain-rmspe:inf\teval-rmse:0.28837\teval-rmspe:inf\n",
      "[254]\ttrain-rmse:0.28300\ttrain-rmspe:inf\teval-rmse:0.28833\teval-rmspe:inf\n",
      "[255]\ttrain-rmse:0.28278\ttrain-rmspe:inf\teval-rmse:0.28814\teval-rmspe:inf\n",
      "[256]\ttrain-rmse:0.28275\ttrain-rmspe:inf\teval-rmse:0.28811\teval-rmspe:inf\n",
      "[257]\ttrain-rmse:0.28259\ttrain-rmspe:inf\teval-rmse:0.28799\teval-rmspe:inf\n",
      "[258]\ttrain-rmse:0.28236\ttrain-rmspe:inf\teval-rmse:0.28779\teval-rmspe:inf\n",
      "[259]\ttrain-rmse:0.28195\ttrain-rmspe:inf\teval-rmse:0.28739\teval-rmspe:inf\n",
      "[260]\ttrain-rmse:0.28162\ttrain-rmspe:inf\teval-rmse:0.28707\teval-rmspe:inf\n",
      "[261]\ttrain-rmse:0.28120\ttrain-rmspe:inf\teval-rmse:0.28667\teval-rmspe:inf\n",
      "[262]\ttrain-rmse:0.28088\ttrain-rmspe:inf\teval-rmse:0.28636\teval-rmspe:inf\n",
      "[263]\ttrain-rmse:0.28055\ttrain-rmspe:inf\teval-rmse:0.28602\teval-rmspe:inf\n",
      "[264]\ttrain-rmse:0.28052\ttrain-rmspe:inf\teval-rmse:0.28601\teval-rmspe:inf\n",
      "[265]\ttrain-rmse:0.28010\ttrain-rmspe:inf\teval-rmse:0.28559\teval-rmspe:inf\n",
      "[266]\ttrain-rmse:0.27987\ttrain-rmspe:inf\teval-rmse:0.28538\teval-rmspe:inf\n",
      "[267]\ttrain-rmse:0.27964\ttrain-rmspe:inf\teval-rmse:0.28517\teval-rmspe:inf\n",
      "[268]\ttrain-rmse:0.27938\ttrain-rmspe:inf\teval-rmse:0.28492\teval-rmspe:inf\n",
      "[269]\ttrain-rmse:0.27899\ttrain-rmspe:inf\teval-rmse:0.28456\teval-rmspe:inf\n",
      "[270]\ttrain-rmse:0.27883\ttrain-rmspe:inf\teval-rmse:0.28443\teval-rmspe:inf\n",
      "[271]\ttrain-rmse:0.27866\ttrain-rmspe:inf\teval-rmse:0.28430\teval-rmspe:inf\n",
      "[272]\ttrain-rmse:0.27843\ttrain-rmspe:inf\teval-rmse:0.28408\teval-rmspe:inf\n",
      "[273]\ttrain-rmse:0.27810\ttrain-rmspe:inf\teval-rmse:0.28376\teval-rmspe:inf\n",
      "[274]\ttrain-rmse:0.27794\ttrain-rmspe:inf\teval-rmse:0.28363\teval-rmspe:inf\n",
      "[275]\ttrain-rmse:0.27758\ttrain-rmspe:inf\teval-rmse:0.28327\teval-rmspe:inf\n",
      "[276]\ttrain-rmse:0.27747\ttrain-rmspe:inf\teval-rmse:0.28318\teval-rmspe:inf\n",
      "[277]\ttrain-rmse:0.27724\ttrain-rmspe:inf\teval-rmse:0.28295\teval-rmspe:inf\n",
      "[278]\ttrain-rmse:0.27704\ttrain-rmspe:inf\teval-rmse:0.28277\teval-rmspe:inf\n",
      "[279]\ttrain-rmse:0.27687\ttrain-rmspe:inf\teval-rmse:0.28261\teval-rmspe:inf\n",
      "[280]\ttrain-rmse:0.27676\ttrain-rmspe:inf\teval-rmse:0.28253\teval-rmspe:inf\n",
      "[281]\ttrain-rmse:0.27668\ttrain-rmspe:inf\teval-rmse:0.28247\teval-rmspe:inf\n",
      "[282]\ttrain-rmse:0.27634\ttrain-rmspe:inf\teval-rmse:0.28214\teval-rmspe:inf\n",
      "[283]\ttrain-rmse:0.27601\ttrain-rmspe:inf\teval-rmse:0.28182\teval-rmspe:inf\n",
      "[284]\ttrain-rmse:0.27574\ttrain-rmspe:inf\teval-rmse:0.28157\teval-rmspe:inf\n",
      "[285]\ttrain-rmse:0.27564\ttrain-rmspe:inf\teval-rmse:0.28150\teval-rmspe:inf\n",
      "[286]\ttrain-rmse:0.27548\ttrain-rmspe:inf\teval-rmse:0.28135\teval-rmspe:inf\n",
      "[287]\ttrain-rmse:0.27544\ttrain-rmspe:inf\teval-rmse:0.28131\teval-rmspe:inf\n",
      "[288]\ttrain-rmse:0.27525\ttrain-rmspe:inf\teval-rmse:0.28114\teval-rmspe:inf\n",
      "[289]\ttrain-rmse:0.27511\ttrain-rmspe:inf\teval-rmse:0.28100\teval-rmspe:inf\n",
      "[290]\ttrain-rmse:0.27495\ttrain-rmspe:inf\teval-rmse:0.28085\teval-rmspe:inf\n",
      "[291]\ttrain-rmse:0.27478\ttrain-rmspe:inf\teval-rmse:0.28069\teval-rmspe:inf\n",
      "[292]\ttrain-rmse:0.27453\ttrain-rmspe:inf\teval-rmse:0.28047\teval-rmspe:inf\n",
      "[293]\ttrain-rmse:0.27442\ttrain-rmspe:inf\teval-rmse:0.28040\teval-rmspe:inf\n",
      "[294]\ttrain-rmse:0.27407\ttrain-rmspe:inf\teval-rmse:0.28007\teval-rmspe:inf\n",
      "[295]\ttrain-rmse:0.27385\ttrain-rmspe:inf\teval-rmse:0.27986\teval-rmspe:inf\n",
      "[296]\ttrain-rmse:0.27382\ttrain-rmspe:inf\teval-rmse:0.27984\teval-rmspe:inf\n",
      "[297]\ttrain-rmse:0.27374\ttrain-rmspe:inf\teval-rmse:0.27978\teval-rmspe:inf\n",
      "[298]\ttrain-rmse:0.27358\ttrain-rmspe:inf\teval-rmse:0.27964\teval-rmspe:inf\n",
      "[299]\ttrain-rmse:0.27339\ttrain-rmspe:inf\teval-rmse:0.27946\teval-rmspe:inf\n",
      "[300]\ttrain-rmse:0.27327\ttrain-rmspe:inf\teval-rmse:0.27935\teval-rmspe:inf\n",
      "[301]\ttrain-rmse:0.27302\ttrain-rmspe:inf\teval-rmse:0.27913\teval-rmspe:inf\n",
      "[302]\ttrain-rmse:0.27289\ttrain-rmspe:inf\teval-rmse:0.27903\teval-rmspe:inf\n",
      "[303]\ttrain-rmse:0.27258\ttrain-rmspe:inf\teval-rmse:0.27873\teval-rmspe:inf\n",
      "[304]\ttrain-rmse:0.27244\ttrain-rmspe:inf\teval-rmse:0.27860\teval-rmspe:inf\n",
      "[305]\ttrain-rmse:0.27223\ttrain-rmspe:inf\teval-rmse:0.27840\teval-rmspe:inf\n",
      "[306]\ttrain-rmse:0.27216\ttrain-rmspe:inf\teval-rmse:0.27833\teval-rmspe:inf\n",
      "[307]\ttrain-rmse:0.27193\ttrain-rmspe:inf\teval-rmse:0.27812\teval-rmspe:inf\n",
      "[308]\ttrain-rmse:0.27179\ttrain-rmspe:inf\teval-rmse:0.27800\teval-rmspe:inf\n",
      "[309]\ttrain-rmse:0.27165\ttrain-rmspe:inf\teval-rmse:0.27787\teval-rmspe:inf\n",
      "[310]\ttrain-rmse:0.27141\ttrain-rmspe:inf\teval-rmse:0.27764\teval-rmspe:inf\n",
      "[311]\ttrain-rmse:0.27132\ttrain-rmspe:inf\teval-rmse:0.27759\teval-rmspe:inf\n",
      "[312]\ttrain-rmse:0.27109\ttrain-rmspe:inf\teval-rmse:0.27739\teval-rmspe:inf\n",
      "[313]\ttrain-rmse:0.27080\ttrain-rmspe:inf\teval-rmse:0.27710\teval-rmspe:inf\n",
      "[314]\ttrain-rmse:0.27077\ttrain-rmspe:inf\teval-rmse:0.27709\teval-rmspe:inf\n",
      "[315]\ttrain-rmse:0.27062\ttrain-rmspe:inf\teval-rmse:0.27696\teval-rmspe:inf\n",
      "[316]\ttrain-rmse:0.27047\ttrain-rmspe:inf\teval-rmse:0.27683\teval-rmspe:inf\n",
      "[317]\ttrain-rmse:0.27040\ttrain-rmspe:inf\teval-rmse:0.27676\teval-rmspe:inf\n",
      "[318]\ttrain-rmse:0.27021\ttrain-rmspe:inf\teval-rmse:0.27658\teval-rmspe:inf\n",
      "[319]\ttrain-rmse:0.26996\ttrain-rmspe:inf\teval-rmse:0.27635\teval-rmspe:inf\n",
      "[320]\ttrain-rmse:0.26957\ttrain-rmspe:inf\teval-rmse:0.27598\teval-rmspe:inf\n",
      "[321]\ttrain-rmse:0.26942\ttrain-rmspe:inf\teval-rmse:0.27583\teval-rmspe:inf\n",
      "[322]\ttrain-rmse:0.26924\ttrain-rmspe:inf\teval-rmse:0.27567\teval-rmspe:inf\n",
      "[323]\ttrain-rmse:0.26915\ttrain-rmspe:inf\teval-rmse:0.27559\teval-rmspe:inf\n",
      "[324]\ttrain-rmse:0.26895\ttrain-rmspe:inf\teval-rmse:0.27541\teval-rmspe:inf\n",
      "[325]\ttrain-rmse:0.26876\ttrain-rmspe:inf\teval-rmse:0.27524\teval-rmspe:inf\n",
      "[326]\ttrain-rmse:0.26872\ttrain-rmspe:inf\teval-rmse:0.27521\teval-rmspe:inf\n",
      "[327]\ttrain-rmse:0.26827\ttrain-rmspe:inf\teval-rmse:0.27479\teval-rmspe:inf\n",
      "[328]\ttrain-rmse:0.26817\ttrain-rmspe:inf\teval-rmse:0.27470\teval-rmspe:inf\n",
      "[329]\ttrain-rmse:0.26798\ttrain-rmspe:inf\teval-rmse:0.27452\teval-rmspe:inf\n",
      "[330]\ttrain-rmse:0.26765\ttrain-rmspe:inf\teval-rmse:0.27421\teval-rmspe:inf\n",
      "[331]\ttrain-rmse:0.26757\ttrain-rmspe:inf\teval-rmse:0.27414\teval-rmspe:inf\n",
      "[332]\ttrain-rmse:0.26741\ttrain-rmspe:inf\teval-rmse:0.27399\teval-rmspe:inf\n",
      "[333]\ttrain-rmse:0.26726\ttrain-rmspe:inf\teval-rmse:0.27386\teval-rmspe:inf\n",
      "[334]\ttrain-rmse:0.26703\ttrain-rmspe:inf\teval-rmse:0.27364\teval-rmspe:inf\n",
      "[335]\ttrain-rmse:0.26694\ttrain-rmspe:inf\teval-rmse:0.27356\teval-rmspe:inf\n",
      "[336]\ttrain-rmse:0.26679\ttrain-rmspe:inf\teval-rmse:0.27343\teval-rmspe:inf\n",
      "[337]\ttrain-rmse:0.26664\ttrain-rmspe:inf\teval-rmse:0.27330\teval-rmspe:inf\n",
      "[338]\ttrain-rmse:0.26648\ttrain-rmspe:inf\teval-rmse:0.27316\teval-rmspe:inf\n",
      "[339]\ttrain-rmse:0.26616\ttrain-rmspe:inf\teval-rmse:0.27286\teval-rmspe:inf\n",
      "[340]\ttrain-rmse:0.26606\ttrain-rmspe:inf\teval-rmse:0.27277\teval-rmspe:inf\n",
      "[341]\ttrain-rmse:0.26586\ttrain-rmspe:inf\teval-rmse:0.27259\teval-rmspe:inf\n",
      "[342]\ttrain-rmse:0.26577\ttrain-rmspe:inf\teval-rmse:0.27253\teval-rmspe:inf\n",
      "[343]\ttrain-rmse:0.26556\ttrain-rmspe:inf\teval-rmse:0.27235\teval-rmspe:inf\n",
      "[344]\ttrain-rmse:0.26554\ttrain-rmspe:inf\teval-rmse:0.27233\teval-rmspe:inf\n",
      "[345]\ttrain-rmse:0.26541\ttrain-rmspe:inf\teval-rmse:0.27222\teval-rmspe:inf\n",
      "[346]\ttrain-rmse:0.26528\ttrain-rmspe:inf\teval-rmse:0.27210\teval-rmspe:inf\n",
      "[347]\ttrain-rmse:0.26523\ttrain-rmspe:inf\teval-rmse:0.27206\teval-rmspe:inf\n",
      "[348]\ttrain-rmse:0.26511\ttrain-rmspe:inf\teval-rmse:0.27196\teval-rmspe:inf\n",
      "[349]\ttrain-rmse:0.26486\ttrain-rmspe:inf\teval-rmse:0.27173\teval-rmspe:inf\n",
      "[350]\ttrain-rmse:0.26466\ttrain-rmspe:inf\teval-rmse:0.27154\teval-rmspe:inf\n",
      "[351]\ttrain-rmse:0.26444\ttrain-rmspe:inf\teval-rmse:0.27135\teval-rmspe:inf\n",
      "[352]\ttrain-rmse:0.26435\ttrain-rmspe:inf\teval-rmse:0.27128\teval-rmspe:inf\n",
      "[353]\ttrain-rmse:0.26429\ttrain-rmspe:inf\teval-rmse:0.27125\teval-rmspe:inf\n",
      "[354]\ttrain-rmse:0.26411\ttrain-rmspe:inf\teval-rmse:0.27108\teval-rmspe:inf\n",
      "[355]\ttrain-rmse:0.26398\ttrain-rmspe:inf\teval-rmse:0.27096\teval-rmspe:inf\n",
      "[356]\ttrain-rmse:0.26388\ttrain-rmspe:inf\teval-rmse:0.27087\teval-rmspe:inf\n",
      "[357]\ttrain-rmse:0.26374\ttrain-rmspe:inf\teval-rmse:0.27074\teval-rmspe:inf\n",
      "[358]\ttrain-rmse:0.26359\ttrain-rmspe:inf\teval-rmse:0.27061\teval-rmspe:inf\n",
      "[359]\ttrain-rmse:0.26337\ttrain-rmspe:inf\teval-rmse:0.27040\teval-rmspe:inf\n",
      "[360]\ttrain-rmse:0.26330\ttrain-rmspe:inf\teval-rmse:0.27035\teval-rmspe:inf\n",
      "[361]\ttrain-rmse:0.26315\ttrain-rmspe:inf\teval-rmse:0.27024\teval-rmspe:inf\n",
      "[362]\ttrain-rmse:0.26302\ttrain-rmspe:inf\teval-rmse:0.27012\teval-rmspe:inf\n",
      "[363]\ttrain-rmse:0.26282\ttrain-rmspe:inf\teval-rmse:0.26993\teval-rmspe:inf\n",
      "[364]\ttrain-rmse:0.26274\ttrain-rmspe:inf\teval-rmse:0.26988\teval-rmspe:inf\n",
      "[365]\ttrain-rmse:0.26266\ttrain-rmspe:inf\teval-rmse:0.26980\teval-rmspe:inf\n",
      "[366]\ttrain-rmse:0.26248\ttrain-rmspe:inf\teval-rmse:0.26963\teval-rmspe:inf\n",
      "[367]\ttrain-rmse:0.26242\ttrain-rmspe:inf\teval-rmse:0.26959\teval-rmspe:inf\n",
      "[368]\ttrain-rmse:0.26221\ttrain-rmspe:inf\teval-rmse:0.26939\teval-rmspe:inf\n",
      "[369]\ttrain-rmse:0.26212\ttrain-rmspe:inf\teval-rmse:0.26932\teval-rmspe:inf\n",
      "[370]\ttrain-rmse:0.26205\ttrain-rmspe:inf\teval-rmse:0.26926\teval-rmspe:inf\n",
      "[371]\ttrain-rmse:0.26193\ttrain-rmspe:inf\teval-rmse:0.26916\teval-rmspe:inf\n",
      "[372]\ttrain-rmse:0.26171\ttrain-rmspe:inf\teval-rmse:0.26895\teval-rmspe:inf\n",
      "[373]\ttrain-rmse:0.26168\ttrain-rmspe:inf\teval-rmse:0.26893\teval-rmspe:inf\n",
      "[374]\ttrain-rmse:0.26152\ttrain-rmspe:inf\teval-rmse:0.26880\teval-rmspe:inf\n",
      "[375]\ttrain-rmse:0.26138\ttrain-rmspe:inf\teval-rmse:0.26869\teval-rmspe:inf\n",
      "[376]\ttrain-rmse:0.26119\ttrain-rmspe:inf\teval-rmse:0.26851\teval-rmspe:inf\n",
      "[377]\ttrain-rmse:0.26094\ttrain-rmspe:inf\teval-rmse:0.26827\teval-rmspe:inf\n",
      "[378]\ttrain-rmse:0.26078\ttrain-rmspe:inf\teval-rmse:0.26814\teval-rmspe:inf\n",
      "[379]\ttrain-rmse:0.26070\ttrain-rmspe:inf\teval-rmse:0.26808\teval-rmspe:inf\n",
      "[380]\ttrain-rmse:0.26063\ttrain-rmspe:inf\teval-rmse:0.26802\teval-rmspe:inf\n",
      "[381]\ttrain-rmse:0.26040\ttrain-rmspe:inf\teval-rmse:0.26780\teval-rmspe:inf\n",
      "[382]\ttrain-rmse:0.26017\ttrain-rmspe:inf\teval-rmse:0.26760\teval-rmspe:inf\n",
      "[383]\ttrain-rmse:0.26007\ttrain-rmspe:inf\teval-rmse:0.26751\teval-rmspe:inf\n",
      "[384]\ttrain-rmse:0.25998\ttrain-rmspe:inf\teval-rmse:0.26743\teval-rmspe:inf\n",
      "[385]\ttrain-rmse:0.25979\ttrain-rmspe:inf\teval-rmse:0.26724\teval-rmspe:inf\n",
      "[386]\ttrain-rmse:0.25972\ttrain-rmspe:inf\teval-rmse:0.26720\teval-rmspe:inf\n",
      "[387]\ttrain-rmse:0.25951\ttrain-rmspe:inf\teval-rmse:0.26700\teval-rmspe:inf\n",
      "[388]\ttrain-rmse:0.25943\ttrain-rmspe:inf\teval-rmse:0.26695\teval-rmspe:inf\n",
      "[389]\ttrain-rmse:0.25913\ttrain-rmspe:inf\teval-rmse:0.26666\teval-rmspe:inf\n",
      "[390]\ttrain-rmse:0.25909\ttrain-rmspe:inf\teval-rmse:0.26665\teval-rmspe:inf\n",
      "[391]\ttrain-rmse:0.25892\ttrain-rmspe:inf\teval-rmse:0.26649\teval-rmspe:inf\n",
      "[392]\ttrain-rmse:0.25882\ttrain-rmspe:inf\teval-rmse:0.26641\teval-rmspe:inf\n",
      "[393]\ttrain-rmse:0.25873\ttrain-rmspe:inf\teval-rmse:0.26634\teval-rmspe:inf\n",
      "[394]\ttrain-rmse:0.25868\ttrain-rmspe:inf\teval-rmse:0.26631\teval-rmspe:inf\n",
      "[395]\ttrain-rmse:0.25857\ttrain-rmspe:inf\teval-rmse:0.26620\teval-rmspe:inf\n",
      "[396]\ttrain-rmse:0.25851\ttrain-rmspe:inf\teval-rmse:0.26616\teval-rmspe:inf\n",
      "[397]\ttrain-rmse:0.25850\ttrain-rmspe:inf\teval-rmse:0.26615\teval-rmspe:inf\n",
      "[398]\ttrain-rmse:0.25839\ttrain-rmspe:inf\teval-rmse:0.26606\teval-rmspe:inf\n",
      "[399]\ttrain-rmse:0.25828\ttrain-rmspe:inf\teval-rmse:0.26598\teval-rmspe:inf\n",
      "[400]\ttrain-rmse:0.25816\ttrain-rmspe:inf\teval-rmse:0.26587\teval-rmspe:inf\n",
      "[401]\ttrain-rmse:0.25801\ttrain-rmspe:inf\teval-rmse:0.26573\teval-rmspe:inf\n",
      "[402]\ttrain-rmse:0.25789\ttrain-rmspe:inf\teval-rmse:0.26561\teval-rmspe:inf\n",
      "[403]\ttrain-rmse:0.25785\ttrain-rmspe:inf\teval-rmse:0.26558\teval-rmspe:inf\n",
      "[404]\ttrain-rmse:0.25766\ttrain-rmspe:inf\teval-rmse:0.26540\teval-rmspe:inf\n",
      "[405]\ttrain-rmse:0.25753\ttrain-rmspe:inf\teval-rmse:0.26528\teval-rmspe:inf\n",
      "[406]\ttrain-rmse:0.25742\ttrain-rmspe:inf\teval-rmse:0.26520\teval-rmspe:inf\n",
      "[407]\ttrain-rmse:0.25729\ttrain-rmspe:inf\teval-rmse:0.26508\teval-rmspe:inf\n",
      "[408]\ttrain-rmse:0.25720\ttrain-rmspe:inf\teval-rmse:0.26502\teval-rmspe:inf\n",
      "[409]\ttrain-rmse:0.25705\ttrain-rmspe:inf\teval-rmse:0.26488\teval-rmspe:inf\n",
      "[410]\ttrain-rmse:0.25699\ttrain-rmspe:inf\teval-rmse:0.26483\teval-rmspe:inf\n",
      "[411]\ttrain-rmse:0.25682\ttrain-rmspe:inf\teval-rmse:0.26468\teval-rmspe:inf\n",
      "[412]\ttrain-rmse:0.25661\ttrain-rmspe:inf\teval-rmse:0.26448\teval-rmspe:inf\n",
      "[413]\ttrain-rmse:0.25649\ttrain-rmspe:inf\teval-rmse:0.26437\teval-rmspe:inf\n",
      "[414]\ttrain-rmse:0.25648\ttrain-rmspe:inf\teval-rmse:0.26436\teval-rmspe:inf\n",
      "[415]\ttrain-rmse:0.25630\ttrain-rmspe:inf\teval-rmse:0.26419\teval-rmspe:inf\n",
      "[416]\ttrain-rmse:0.25625\ttrain-rmspe:inf\teval-rmse:0.26415\teval-rmspe:inf\n",
      "[417]\ttrain-rmse:0.25619\ttrain-rmspe:inf\teval-rmse:0.26411\teval-rmspe:inf\n",
      "[418]\ttrain-rmse:0.25608\ttrain-rmspe:inf\teval-rmse:0.26401\teval-rmspe:inf\n",
      "[419]\ttrain-rmse:0.25600\ttrain-rmspe:inf\teval-rmse:0.26396\teval-rmspe:inf\n",
      "[420]\ttrain-rmse:0.25591\ttrain-rmspe:inf\teval-rmse:0.26388\teval-rmspe:inf\n",
      "[421]\ttrain-rmse:0.25589\ttrain-rmspe:inf\teval-rmse:0.26386\teval-rmspe:inf\n",
      "[422]\ttrain-rmse:0.25576\ttrain-rmspe:inf\teval-rmse:0.26375\teval-rmspe:inf\n",
      "[423]\ttrain-rmse:0.25562\ttrain-rmspe:inf\teval-rmse:0.26362\teval-rmspe:inf\n",
      "[424]\ttrain-rmse:0.25528\ttrain-rmspe:inf\teval-rmse:0.26330\teval-rmspe:inf\n",
      "[425]\ttrain-rmse:0.25523\ttrain-rmspe:inf\teval-rmse:0.26327\teval-rmspe:inf\n",
      "[426]\ttrain-rmse:0.25506\ttrain-rmspe:inf\teval-rmse:0.26313\teval-rmspe:inf\n",
      "[427]\ttrain-rmse:0.25497\ttrain-rmspe:inf\teval-rmse:0.26307\teval-rmspe:inf\n",
      "[428]\ttrain-rmse:0.25476\ttrain-rmspe:inf\teval-rmse:0.26289\teval-rmspe:inf\n",
      "[429]\ttrain-rmse:0.25467\ttrain-rmspe:inf\teval-rmse:0.26281\teval-rmspe:inf\n",
      "[430]\ttrain-rmse:0.25452\ttrain-rmspe:inf\teval-rmse:0.26267\teval-rmspe:inf\n",
      "[431]\ttrain-rmse:0.25445\ttrain-rmspe:inf\teval-rmse:0.26261\teval-rmspe:inf\n",
      "[432]\ttrain-rmse:0.25435\ttrain-rmspe:inf\teval-rmse:0.26254\teval-rmspe:inf\n",
      "[433]\ttrain-rmse:0.25427\ttrain-rmspe:inf\teval-rmse:0.26247\teval-rmspe:inf\n",
      "[434]\ttrain-rmse:0.25411\ttrain-rmspe:inf\teval-rmse:0.26234\teval-rmspe:inf\n",
      "[435]\ttrain-rmse:0.25395\ttrain-rmspe:inf\teval-rmse:0.26218\teval-rmspe:inf\n",
      "[436]\ttrain-rmse:0.25388\ttrain-rmspe:inf\teval-rmse:0.26213\teval-rmspe:inf\n",
      "[437]\ttrain-rmse:0.25383\ttrain-rmspe:inf\teval-rmse:0.26209\teval-rmspe:inf\n",
      "[438]\ttrain-rmse:0.25379\ttrain-rmspe:inf\teval-rmse:0.26207\teval-rmspe:inf\n",
      "[439]\ttrain-rmse:0.25369\ttrain-rmspe:inf\teval-rmse:0.26199\teval-rmspe:inf\n",
      "[440]\ttrain-rmse:0.25366\ttrain-rmspe:inf\teval-rmse:0.26196\teval-rmspe:inf\n",
      "[441]\ttrain-rmse:0.25361\ttrain-rmspe:inf\teval-rmse:0.26192\teval-rmspe:inf\n",
      "[442]\ttrain-rmse:0.25347\ttrain-rmspe:inf\teval-rmse:0.26180\teval-rmspe:inf\n",
      "[443]\ttrain-rmse:0.25340\ttrain-rmspe:inf\teval-rmse:0.26174\teval-rmspe:inf\n",
      "[444]\ttrain-rmse:0.25333\ttrain-rmspe:inf\teval-rmse:0.26170\teval-rmspe:inf\n",
      "[445]\ttrain-rmse:0.25330\ttrain-rmspe:inf\teval-rmse:0.26168\teval-rmspe:inf\n",
      "[446]\ttrain-rmse:0.25311\ttrain-rmspe:inf\teval-rmse:0.26150\teval-rmspe:inf\n",
      "[447]\ttrain-rmse:0.25305\ttrain-rmspe:inf\teval-rmse:0.26145\teval-rmspe:inf\n",
      "[448]\ttrain-rmse:0.25299\ttrain-rmspe:inf\teval-rmse:0.26141\teval-rmspe:inf\n",
      "[449]\ttrain-rmse:0.25292\ttrain-rmspe:inf\teval-rmse:0.26134\teval-rmspe:inf\n",
      "[450]\ttrain-rmse:0.25278\ttrain-rmspe:inf\teval-rmse:0.26123\teval-rmspe:inf\n",
      "[451]\ttrain-rmse:0.25268\ttrain-rmspe:inf\teval-rmse:0.26115\teval-rmspe:inf\n",
      "[452]\ttrain-rmse:0.25254\ttrain-rmspe:inf\teval-rmse:0.26102\teval-rmspe:inf\n",
      "[453]\ttrain-rmse:0.25248\ttrain-rmspe:inf\teval-rmse:0.26098\teval-rmspe:inf\n",
      "[454]\ttrain-rmse:0.25240\ttrain-rmspe:inf\teval-rmse:0.26092\teval-rmspe:inf\n",
      "[455]\ttrain-rmse:0.25233\ttrain-rmspe:inf\teval-rmse:0.26087\teval-rmspe:inf\n",
      "[456]\ttrain-rmse:0.25225\ttrain-rmspe:inf\teval-rmse:0.26081\teval-rmspe:inf\n",
      "[457]\ttrain-rmse:0.25218\ttrain-rmspe:inf\teval-rmse:0.26076\teval-rmspe:inf\n",
      "[458]\ttrain-rmse:0.25212\ttrain-rmspe:inf\teval-rmse:0.26072\teval-rmspe:inf\n",
      "[459]\ttrain-rmse:0.25200\ttrain-rmspe:inf\teval-rmse:0.26060\teval-rmspe:inf\n",
      "[460]\ttrain-rmse:0.25182\ttrain-rmspe:inf\teval-rmse:0.26043\teval-rmspe:inf\n",
      "[461]\ttrain-rmse:0.25163\ttrain-rmspe:inf\teval-rmse:0.26026\teval-rmspe:inf\n",
      "[462]\ttrain-rmse:0.25159\ttrain-rmspe:inf\teval-rmse:0.26022\teval-rmspe:inf\n",
      "[463]\ttrain-rmse:0.25144\ttrain-rmspe:inf\teval-rmse:0.26009\teval-rmspe:inf\n",
      "[464]\ttrain-rmse:0.25136\ttrain-rmspe:inf\teval-rmse:0.26002\teval-rmspe:inf\n",
      "[465]\ttrain-rmse:0.25131\ttrain-rmspe:inf\teval-rmse:0.25999\teval-rmspe:inf\n",
      "[466]\ttrain-rmse:0.25124\ttrain-rmspe:inf\teval-rmse:0.25995\teval-rmspe:inf\n",
      "[467]\ttrain-rmse:0.25117\ttrain-rmspe:inf\teval-rmse:0.25990\teval-rmspe:inf\n",
      "[468]\ttrain-rmse:0.25111\ttrain-rmspe:inf\teval-rmse:0.25987\teval-rmspe:inf\n",
      "[469]\ttrain-rmse:0.25096\ttrain-rmspe:inf\teval-rmse:0.25973\teval-rmspe:inf\n",
      "[470]\ttrain-rmse:0.25080\ttrain-rmspe:inf\teval-rmse:0.25960\teval-rmspe:inf\n",
      "[471]\ttrain-rmse:0.25073\ttrain-rmspe:inf\teval-rmse:0.25954\teval-rmspe:inf\n",
      "[472]\ttrain-rmse:0.25067\ttrain-rmspe:inf\teval-rmse:0.25950\teval-rmspe:inf\n",
      "[473]\ttrain-rmse:0.25056\ttrain-rmspe:inf\teval-rmse:0.25940\teval-rmspe:inf\n",
      "[474]\ttrain-rmse:0.25049\ttrain-rmspe:inf\teval-rmse:0.25934\teval-rmspe:inf\n",
      "[475]\ttrain-rmse:0.25037\ttrain-rmspe:inf\teval-rmse:0.25924\teval-rmspe:inf\n",
      "[476]\ttrain-rmse:0.25017\ttrain-rmspe:inf\teval-rmse:0.25904\teval-rmspe:inf\n",
      "[477]\ttrain-rmse:0.25008\ttrain-rmspe:inf\teval-rmse:0.25896\teval-rmspe:inf\n",
      "[478]\ttrain-rmse:0.25004\ttrain-rmspe:inf\teval-rmse:0.25893\teval-rmspe:inf\n",
      "[479]\ttrain-rmse:0.24989\ttrain-rmspe:inf\teval-rmse:0.25881\teval-rmspe:inf\n",
      "[480]\ttrain-rmse:0.24979\ttrain-rmspe:inf\teval-rmse:0.25873\teval-rmspe:inf\n",
      "[481]\ttrain-rmse:0.24965\ttrain-rmspe:inf\teval-rmse:0.25860\teval-rmspe:inf\n",
      "[482]\ttrain-rmse:0.24957\ttrain-rmspe:inf\teval-rmse:0.25855\teval-rmspe:inf\n",
      "[483]\ttrain-rmse:0.24943\ttrain-rmspe:inf\teval-rmse:0.25843\teval-rmspe:inf\n",
      "[484]\ttrain-rmse:0.24930\ttrain-rmspe:inf\teval-rmse:0.25833\teval-rmspe:inf\n",
      "[485]\ttrain-rmse:0.24917\ttrain-rmspe:inf\teval-rmse:0.25822\teval-rmspe:inf\n",
      "[486]\ttrain-rmse:0.24913\ttrain-rmspe:inf\teval-rmse:0.25819\teval-rmspe:inf\n",
      "[487]\ttrain-rmse:0.24908\ttrain-rmspe:inf\teval-rmse:0.25816\teval-rmspe:inf\n",
      "[488]\ttrain-rmse:0.24901\ttrain-rmspe:inf\teval-rmse:0.25812\teval-rmspe:inf\n",
      "[489]\ttrain-rmse:0.24897\ttrain-rmspe:inf\teval-rmse:0.25809\teval-rmspe:inf\n",
      "[490]\ttrain-rmse:0.24881\ttrain-rmspe:inf\teval-rmse:0.25794\teval-rmspe:inf\n",
      "[491]\ttrain-rmse:0.24868\ttrain-rmspe:inf\teval-rmse:0.25782\teval-rmspe:inf\n",
      "[492]\ttrain-rmse:0.24861\ttrain-rmspe:inf\teval-rmse:0.25777\teval-rmspe:inf\n",
      "[493]\ttrain-rmse:0.24856\ttrain-rmspe:inf\teval-rmse:0.25775\teval-rmspe:inf\n",
      "[494]\ttrain-rmse:0.24841\ttrain-rmspe:inf\teval-rmse:0.25762\teval-rmspe:inf\n",
      "[495]\ttrain-rmse:0.24838\ttrain-rmspe:inf\teval-rmse:0.25760\teval-rmspe:inf\n",
      "[496]\ttrain-rmse:0.24828\ttrain-rmspe:inf\teval-rmse:0.25751\teval-rmspe:inf\n",
      "[497]\ttrain-rmse:0.24809\ttrain-rmspe:inf\teval-rmse:0.25733\teval-rmspe:inf\n",
      "[498]\ttrain-rmse:0.24805\ttrain-rmspe:inf\teval-rmse:0.25730\teval-rmspe:inf\n",
      "[499]\ttrain-rmse:0.24792\ttrain-rmspe:inf\teval-rmse:0.25720\teval-rmspe:inf\n",
      "[ 0.  0.  0. ...  1.  1. -0.]\n",
      "accuracy_score: 0.9284829514398178\n",
      "f1_score: 0.9150128272826255\n"
     ]
    }
   ],
   "source": [
    "modelXGBR = make_xgb_modelR()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load/Make models\n",
    "We save and load the same models each time to ensure stable results. If they don’t exist we create new ones. If you\n",
    "want to generate new models on each notebook run, then set FROM_SCRATCH=True."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.742264149459503\n",
      "f1_score: [0.68409206 0.7823439 ]\n",
      "accuracy_score: 0.9245231082606182\n",
      "f1_score: [0.93494089 0.91013288]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_2216\\3961335572.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mmodelR\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmake_tf_modelR\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[0mrfcR\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmake_rfcR\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[0maeR\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmake_aeR\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_2216\\225137490.py\u001B[0m in \u001B[0;36mmake_aeR\u001B[1;34m()\u001B[0m\n\u001B[0;32m     16\u001B[0m     \u001B[0mlen_input_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mX_trainR\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     17\u001B[0m     \u001B[0mencoder\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mSequential\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 18\u001B[1;33m     \u001B[0mencoder\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0madd\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mDense\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0munits\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mENCODING_DIM\u001B[0m\u001B[1;33m*\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mactivation\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"relu\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput_shape\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlen_input_output\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     19\u001B[0m     \u001B[0mencoder\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0madd\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mDense\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0munits\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mENCODING_DIM\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mactivation\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"relu\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m     \u001B[0mdecoder\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mSequential\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\keras\\dtensor\\utils.py\u001B[0m in \u001B[0;36m_wrap_function\u001B[1;34m(layer_instance, *args, **kwargs)\u001B[0m\n\u001B[0;32m     94\u001B[0m                     \u001B[0mlayout_args\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mvariable_name\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34m\"_layout\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlayout\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     95\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 96\u001B[1;33m         \u001B[0minit_method\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlayer_instance\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     97\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     98\u001B[0m         \u001B[1;31m# Inject the layout parameter after the invocation of __init__()\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\keras\\layers\\core\\dense.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001B[0m\n\u001B[0;32m    115\u001B[0m         \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    116\u001B[0m     ):\n\u001B[1;32m--> 117\u001B[1;33m         \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mactivity_regularizer\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mactivity_regularizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    118\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    119\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0munits\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0munits\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0munits\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mint\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0munits\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py\u001B[0m in \u001B[0;36m_method_wrapper\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    203\u001B[0m     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_self_setattr_tracking\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mFalse\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    204\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 205\u001B[1;33m       \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmethod\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    206\u001B[0m     \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    207\u001B[0m       \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_self_setattr_tracking\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mprevious_value\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, trainable, name, dtype, dynamic, **kwargs)\u001B[0m\n\u001B[0;32m    450\u001B[0m                 \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    451\u001B[0m                     \u001B[0mbatch_size\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 452\u001B[1;33m                 \u001B[0mbatch_input_shape\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mtuple\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"input_shape\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    453\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_batch_input_shape\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbatch_input_shape\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    454\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "#if FROM_SCRATCH or not os.path.isfile(f'{TF_MODEL_FNAME}.h5'):\n",
    "#    model = make_tf_model()\n",
    "#    rfc = make_rfc()\n",
    "#    ae = make_ae()\n",
    "#else:\n",
    "#    rfc = load_rfc_model()\n",
    "#    model = load_tf_model()\n",
    "#    ae = load_ae_model()\n",
    "modelR = make_tf_modelR()\n",
    "rfcR = make_rfcR()\n",
    "aeR = make_aeR()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "9.1.3 Util functions\n",
    "These are utility functions for exploring results. The first shows two instances of the data side by side and compares\n",
    "the difference. We’ll use this to see how the counterfactuals differ from their original instances. The second function\n",
    "plots the importance of each feature. This will be useful for visualizing the attribution methods"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def compare_instances(x, cf):\n",
    "    \"\"\"\n",
    "    Show the difference in values between two instances.\n",
    "    \"\"\"\n",
    "    x = x.astype('float64')\n",
    "    cf = cf.astype('float64')\n",
    "    for f, v1, v2 in zip(features, x[0], cf[0]):\n",
    "        print(f'{f:<25} instance: {round(v1, 3):^10} counter factual: {round(v2, 3):^10} difference: {round(v2, 7):^5}')\n",
    "\n",
    "def plot_importance(feat_imp, feat_names, class_idx, **kwargs):\n",
    "    \"\"\"\n",
    "    Create a horizontal barchart of feature effects, sorted by their magnitude.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(data=feat_imp, columns=feat_names).sort_values(by=0, axis='columns')\n",
    "    feat_imp, feat_names = df.values[0], df.columns\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    y_pos = np.arange(len(feat_imp))\n",
    "    ax.barh(y_pos, feat_imp)\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(feat_names, fontsize=15)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xlabel(f'Feature effects for class {class_idx}', fontsize=15)\n",
    "    return ax, fig"
   ],
   "metadata": {
    "_uuid": "05e10079a226a714fce092057d40e4ce5a779106",
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 43,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "9.1.5 Local Necessary Features\n",
    "Anchors\n",
    "Anchors tell us what features need to stay the same for a specific instance for the model to give the same classification.\n",
    "In the case of a trained image classification model, an anchor for a given instance would be a minimal subset of the\n",
    "image that the model uses to make its decision.\n",
    "Here we apply Anchors to the tensor flow model trained on the wine-quality dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "ename": "PredictorCallError",
     "evalue": "Predictor failed to be called on <class 'numpy.ndarray'> of shape (1, 20) and dtype float32. Check that the parameter `feature_names` is correctly specified.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\alibi\\explainers\\anchors\\anchor_tabular.py\u001B[0m in \u001B[0;36m_transform_predictor\u001B[1;34m(self, predictor)\u001B[0m\n\u001B[0;32m   1006\u001B[0m             \u001B[1;31m# if needed adjust predictor so it returns the predicted class\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1007\u001B[1;33m             \u001B[0mprediction\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpredictor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1008\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_19692\\1327761483.py\u001B[0m in \u001B[0;36m<lambda>\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0malibi\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexplainers\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mAnchorTabular\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mpredict_fnR\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mmodelR\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mscalerR\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0mexplainerR\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mAnchorTabular\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpredict_fnR\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfeaturesR\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'modelR' is not defined",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mPredictorCallError\u001B[0m                        Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_19692\\1327761483.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0malibi\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexplainers\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mAnchorTabular\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[0mpredict_fnR\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mmodelR\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mscalerR\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[0mexplainerR\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mAnchorTabular\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpredict_fnR\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfeaturesR\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m \u001B[0mexplainerR\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX_trainR\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdisc_perc\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m25\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m50\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m75\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mresultR\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mexplainerR\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexplain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mxR\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mthreshold\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.95\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\alibi\\explainers\\anchors\\anchor_tabular.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, predictor, feature_names, categorical_names, dtype, ohe, seed)\u001B[0m\n\u001B[0;32m    640\u001B[0m         \u001B[1;31m# defines self._predictor which expect label categorical features, and if ohe == True,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    641\u001B[0m         \u001B[1;31m# it defines self._ohe_predictor which expects one-hot encoded categorical features\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 642\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpredictor\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpredictor\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    643\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    644\u001B[0m         \u001B[1;31m# define column indices of categorical and numerical (aka continuous) features\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\alibi\\explainers\\anchors\\anchor_tabular.py\u001B[0m in \u001B[0;36mpredictor\u001B[1;34m(self, predictor)\u001B[0m\n\u001B[0;32m    996\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    997\u001B[0m                 \u001B[1;31m# set the predictor\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 998\u001B[1;33m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_predictor\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_transform_predictor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpredictor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    999\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1000\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_transform_predictor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpredictor\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mCallable\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mCallable\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\alibi\\explainers\\anchors\\anchor_tabular.py\u001B[0m in \u001B[0;36m_transform_predictor\u001B[1;34m(self, predictor)\u001B[0m\n\u001B[0;32m   1009\u001B[0m             \u001B[0mmsg\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34mf\"Predictor failed to be called on {type(x)} of shape {x.shape} and dtype {x.dtype}. \"\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1010\u001B[0m                   \u001B[1;34mf\"Check that the parameter `feature_names` is correctly specified.\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1011\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0mPredictorCallError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmsg\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1012\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1013\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mprediction\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mndarray\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mPredictorCallError\u001B[0m: Predictor failed to be called on <class 'numpy.ndarray'> of shape (1, 20) and dtype float32. Check that the parameter `feature_names` is correctly specified."
     ]
    }
   ],
   "source": [
    "from alibi.explainers import AnchorTabular\n",
    "predict_fnR = lambda x: modelR.predict(scalerR.transform(x))\n",
    "explainerR = AnchorTabular(predict_fnR, featuresR)\n",
    "explainerR.fit(X_trainR, disc_perc=(25, 50, 75))\n",
    "resultR = explainerR.explain(xR, threshold=0.95)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "predict_xgb = lambda x: np.rint(modelXGBR.predict(xgb.DMatrix(x)))\n",
    "explainerXGB = AnchorTabular(predict_xgb, featuresR)\n",
    "explainerXGB.fit(X_trainR, disc_perc=(25, 50, 75))\n",
    "resultXGB = explainerXGB.explain(xR, threshold=0.95)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor = ['Promo > 0.00', 'SchoolHoliday > 0.00', 'DayOfWeek <= 6.00', 'StateHoliday <= 0.00', 'PromoOpen > 24171.75']\n",
      "Precision =  0.9955947136563876\n",
      "Coverage =  0.023005455455849082\n"
     ]
    }
   ],
   "source": [
    "print('Anchor =', resultR.data['anchor'])\n",
    "print('Precision = ', resultR.data['precision'])\n",
    "print('Coverage = ', resultR.data['coverage'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor = ['Promo > 0.00', 'PromoOpen > 24171.75', 'Assortment > 1.00', 'StateHoliday <= 0.00', 'CompetitionOpen > 29.00', 'CompetitionDistance > 6880.00', 'Year > 2013.00', 'StoreType > 1.00', 'Promo2SinceYear <= 2009.00']\n",
      "Precision =  0.9803921568627451\n",
      "Coverage =  0.006392661743386472\n"
     ]
    }
   ],
   "source": [
    "print('Anchor =', resultXGB.data['anchor'])\n",
    "print('Precision = ', resultXGB.data['precision'])\n",
    "print('Coverage = ', resultXGB.data['coverage'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "1\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1000us/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "Anchor: Promo <= 0.00 AND WeekOfYear <= 22.00 AND SchoolHoliday <= 0.00 AND StoreType > 1.00\n",
      "Precision: 0.97\n",
      "Coverage: 0.13\n"
     ]
    }
   ],
   "source": [
    "print(explainerR.predictor(X_testR[idx].reshape(1, -1))[0])\n",
    "explanation = explainerR.explain(X_testR[idx], threshold=0.95)\n",
    "print('Anchor: %s' % (' AND '.join(explanation.anchor)))\n",
    "print('Precision: %.2f' % explanation.precision)\n",
    "print('Coverage: %.2f' % explanation.coverage)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "Anchor: Promo <= 0.00 AND CompetitionDistance > 2320.00 AND Month <= 8.00 AND Store > 838.00 AND WeekOfYear <= 11.00 AND Day > 8.00 AND DayOfWeek <= 2.00 AND StoreType > 1.00\n",
      "Precision: 0.98\n",
      "Coverage: 0.00\n"
     ]
    }
   ],
   "source": [
    "idx = 11\n",
    "print(explainerXGB.predictor(X_testR[idx].reshape(1, -1))[0])\n",
    "explanation = explainerXGB.explain(X_testR[idx], threshold=0.95)\n",
    "print('Anchor: %s' % (' AND '.join(explanation.anchor)))\n",
    "print('Precision: %.2f' % explanation.precision)\n",
    "print('Coverage: %.2f' % explanation.coverage)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}